{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/raytaylorism/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"source/imgs/alipay-rewardcode.jpg","path":"imgs/alipay-rewardcode.jpg","modified":1,"renderable":0},{"_id":"source/imgs/avatar.jpg","path":"imgs/avatar.jpg","modified":1,"renderable":0},{"_id":"source/imgs/conv-kernel.png","path":"imgs/conv-kernel.png","modified":1,"renderable":0},{"_id":"source/imgs/conv_3d.jpg","path":"imgs/conv_3d.jpg","modified":1,"renderable":0},{"_id":"source/imgs/conv_no_padding.gif","path":"imgs/conv_no_padding.gif","modified":1,"renderable":0},{"_id":"source/imgs/conv_padding.gif","path":"imgs/conv_padding.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.1.gif","path":"imgs/figure2.1.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.1_2.png","path":"imgs/figure2.1_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.2.gif","path":"imgs/figure2.2.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.2_2.png","path":"imgs/figure2.2_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.3.gif","path":"imgs/figure2.3.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.3_2.png","path":"imgs/figure2.3_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.4.gif","path":"imgs/figure2.4.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.4_2.png","path":"imgs/figure2.4_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.5.gif","path":"imgs/figure2.5.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.5_5.png","path":"imgs/figure2.5_5.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.6.gif","path":"imgs/figure2.6.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.6_2.png","path":"imgs/figure2.6_2.png","modified":1,"renderable":0},{"_id":"source/imgs/machine-api.jpg","path":"imgs/machine-api.jpg","modified":1,"renderable":0},{"_id":"source/imgs/LeNet-5.png","path":"imgs/LeNet-5.png","modified":1,"renderable":0},{"_id":"source/imgs/fc.png","path":"imgs/fc.png","modified":1,"renderable":0},{"_id":"source/imgs/figure1.5.gif","path":"imgs/figure1.5.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure1.6.gif","path":"imgs/figure1.6.gif","modified":1,"renderable":0},{"_id":"source/imgs/flower.jpg","path":"imgs/flower.jpg","modified":1,"renderable":0},{"_id":"source/imgs/iclass.png","path":"imgs/iclass.png","modified":1,"renderable":0},{"_id":"source/imgs/mnist_data.png","path":"imgs/mnist_data.png","modified":1,"renderable":0},{"_id":"source/imgs/mnist_resultpng.png","path":"imgs/mnist_resultpng.png","modified":1,"renderable":0},{"_id":"source/imgs/pythoner.png","path":"imgs/pythoner.png","modified":1,"renderable":0},{"_id":"source/imgs/sky.jpg","path":"imgs/sky.jpg","modified":1,"renderable":0},{"_id":"source/imgs/surf_roughness_128.png","path":"imgs/surf_roughness_128.png","modified":1,"renderable":0},{"_id":"source/imgs/surf_3dmax.png","path":"imgs/surf_3dmax.png","modified":1,"renderable":0},{"_id":"source/imgs/wetchat-rewardcode.jpg","path":"imgs/wetchat-rewardcode.jpg","modified":1,"renderable":0},{"_id":"source/imgs/water.jpg","path":"imgs/water.jpg","modified":1,"renderable":0},{"_id":"source/imgs/conv_as_matrix.png","path":"imgs/conv_as_matrix.png","modified":1,"renderable":0},{"_id":"source/imgs/figure1.5.png","path":"imgs/figure1.5.png","modified":1,"renderable":0},{"_id":"source/imgs/figure1.6.png","path":"imgs/figure1.6.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.1.png","path":"imgs/figure2.1.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.3.png","path":"imgs/figure2.3.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.4.png","path":"imgs/figure2.4.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.5.png","path":"imgs/figure2.5.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.6.png","path":"imgs/figure2.6.png","modified":1,"renderable":0},{"_id":"source/imgs/surf_roughness_256.png","path":"imgs/surf_roughness_256.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/js/jquery.min.js","path":"js/jquery.min.js","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/prettify.js","path":"js/prettify.js","modified":1,"renderable":1},{"_id":"source/imgs/figure2.2.png","path":"imgs/figure2.2.png","modified":1,"renderable":0},{"_id":"source/imgs/histequa.png","path":"imgs/histequa.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","path":"css/lib/prettify-tomorrow-night-eighties.css","modified":1,"renderable":1},{"_id":"source/imgs/conv_as_matrix_2.png","path":"imgs/conv_as_matrix_2.png","modified":1,"renderable":0},{"_id":"source/imgs/orange.jpg","path":"imgs/orange.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/js/materialize.min.js","path":"js/materialize.min.js","modified":1,"renderable":1},{"_id":"source/imgs/algorithm/string/rota_str2.png","path":"imgs/algorithm/string/rota_str2.png","modified":1,"renderable":0},{"_id":"source/imgs/light.jpg","path":"imgs/light.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","path":"css/images/side-user-cover.jpg","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","path":"css/lib/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","path":"css/font/roboto/Roboto-Light.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","path":"css/font/roboto/Roboto-Bold.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","path":"css/font/roboto/Roboto-Bold.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","path":"css/font/roboto/Roboto-Bold.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","path":"css/font/roboto/Roboto-Medium.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","path":"css/font/roboto/Roboto-Medium.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","path":"css/font/roboto/Roboto-Medium.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","path":"css/font/roboto/Roboto-Regular.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","path":"css/font/roboto/Roboto-Light.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","path":"css/font/roboto/Roboto-Light.woff2","modified":1,"renderable":1},{"_id":"source/imgs/50mm-106-68.png","path":"imgs/50mm-106-68.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str10.png","path":"imgs/algorithm/string/rota_str10.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str4.png","path":"imgs/algorithm/string/rota_str4.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/tree/level_tree.png","path":"imgs/algorithm/tree/level_tree.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/tree/level_tree2.png","path":"imgs/algorithm/tree/level_tree2.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/tree/level_tree3.png","path":"imgs/algorithm/tree/level_tree3.png","modified":1,"renderable":0},{"_id":"source/imgs/water-circle.jpg","path":"imgs/water-circle.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","path":"css/font/font-awesome/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","path":"css/font/font-awesome/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","path":"css/font/font-awesome/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","path":"css/font/font-awesome/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","path":"css/font/roboto/Roboto-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","path":"css/lib/materialize.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","path":"css/font/roboto/Roboto-Medium.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","path":"css/font/roboto/Roboto-Light.ttf","modified":1,"renderable":1},{"_id":"source/imgs/50mm-106-68-ans.png","path":"imgs/50mm-106-68-ans.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str3.png","path":"imgs/algorithm/string/rota_str3.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str6.png","path":"imgs/algorithm/string/rota_str6.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str7.png","path":"imgs/algorithm/string/rota_str7.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","path":"css/font/font-awesome/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"source/imgs/all_day.jpg","path":"imgs/all_day.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","path":"css/font/roboto/Roboto-Regular.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","path":"css/font/roboto/Roboto-Regular.woff2","modified":1,"renderable":1},{"_id":"source/imgs/algorithm/string/rota_str5.png","path":"imgs/algorithm/string/rota_str5.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str8.png","path":"imgs/algorithm/string/rota_str8.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str9.png","path":"imgs/algorithm/string/rota_str9.png","modified":1,"renderable":0},{"_id":"source/imgs/game.png","path":"imgs/game.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","path":"css/font/roboto/Roboto-Regular.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","path":"css/font/font-awesome/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"source/imgs/sky-night.jpg","path":"imgs/sky-night.jpg","modified":1,"renderable":0},{"_id":"source/imgs/img-cov.png","path":"imgs/img-cov.png","modified":1,"renderable":0},{"_id":"source/imgs/wall.png","path":"imgs/wall.png","modified":1,"renderable":0},{"_id":"source/imgs/winter.jpg","path":"imgs/winter.jpg","modified":1,"renderable":0},{"_id":"source/imgs/coloreggs.jpg","path":"imgs/coloreggs.jpg","modified":1,"renderable":0},{"_id":"source/imgs/rain.png","path":"imgs/rain.png","modified":1,"renderable":0}],"Cache":[{"_id":"themes/raytaylorism/.gitignore","hash":"cda50c55bb8864e0d96101140b62f880f690da5e","modified":1521800509193},{"_id":"themes/raytaylorism/Gruntfile.js","hash":"f69b2e716f955c9d5a23ca1b75394098c1494858","modified":1521800509193},{"_id":"themes/raytaylorism/LICENSE","hash":"115cd028ae511ac9e3d30eb4933da38136a68513","modified":1521800509193},{"_id":"themes/raytaylorism/README.md","hash":"a195db5b7c40e99d4da1fdf252e78c496f51f48e","modified":1521800509194},{"_id":"themes/raytaylorism/_config.yml","hash":"c24f24a0b72476b40b6addf0bdb2893fcb8facb9","modified":1521800509194},{"_id":"themes/raytaylorism/log.md","hash":"99d57a50f8f328d1a313b47bb636d0dc5656d813","modified":1521800509211},{"_id":"themes/raytaylorism/_data/about.json","hash":"c74a6298b37f190c5cf940abf3accc463cb771a6","modified":1529586095805},{"_id":"themes/raytaylorism/_data/link.json","hash":"fb7c0deefc9952cf7819234fadb2ea751777e609","modified":1521800509195},{"_id":"themes/raytaylorism/_data/hint.json","hash":"7433c56bdcc76fab584670a80442200d9b605f5e","modified":1521800509195},{"_id":"themes/raytaylorism/_data/slider.json","hash":"2f350790b3d2a42dcd6b7adb12c730633a2964c4","modified":1529586095806},{"_id":"themes/raytaylorism/_data/reading.json","hash":"4b91b2b0d7d06d3cd3bf2fbe59c51884d6304b13","modified":1521800509196},{"_id":"themes/raytaylorism/layout/archive.ejs","hash":"0a21af8903e95c6d8bb7554b089ac219e8708ad7","modified":1521800509209},{"_id":"themes/raytaylorism/layout/index.ejs","hash":"50c1e7dab5a065fd10dd3a28fdffa5e3d342de82","modified":1521800509209},{"_id":"themes/raytaylorism/layout/layout.ejs","hash":"43beb54ac81519cf5e88a3a1494649beeb856066","modified":1521800509209},{"_id":"themes/raytaylorism/layout/page.ejs","hash":"90441f114859ce63ef7c7d93d668dbe5939995c2","modified":1521800509210},{"_id":"themes/raytaylorism/layout/tag.ejs","hash":"42ecab14917abd40c0a38e6ab629f089352a24b1","modified":1521800509211},{"_id":"themes/raytaylorism/layout/post.ejs","hash":"8e550fd95ef761909294ed3a4aa428ff0509fbf0","modified":1521800509210},{"_id":"themes/raytaylorism/layout/about.ejs","hash":"599b3bb334b3f88b918e67f7a709287b8effee6d","modified":1521800509209},{"_id":"themes/raytaylorism/layout/reading.ejs","hash":"3b2f77f0a154d2f6966b684eee69f26709968936","modified":1521800509211},{"_id":"themes/raytaylorism/source/favicon.png","hash":"f28180f9a5026132b36b4a786c0577e68ea1fe55","modified":1521800509300},{"_id":"source/_data/about.json","hash":"c74a6298b37f190c5cf940abf3accc463cb771a6","modified":1529586095683},{"_id":"source/_data/hint.json","hash":"7433c56bdcc76fab584670a80442200d9b605f5e","modified":1521800509156},{"_id":"source/_data/link.json","hash":"fb7c0deefc9952cf7819234fadb2ea751777e609","modified":1521800509156},{"_id":"themes/raytaylorism/languages/en.yml","hash":"ac672903f9c45f244db56e9408b4546d026fee8f","modified":1521800509197},{"_id":"source/_data/slider.json","hash":"2f350790b3d2a42dcd6b7adb12c730633a2964c4","modified":1529586095684},{"_id":"source/_posts/Hexo多电脑同步写作.md","hash":"03307813d58aa842e930a62b5e8557bfbbe856d9","modified":1521800509157},{"_id":"themes/raytaylorism/languages/zh-CN.yml","hash":"b2211c4d88a3f319316f6ecbad748a0ae4b4b91b","modified":1521800509198},{"_id":"source/_posts/图像与常用算子进行卷积运算.md","hash":"9c09f52c640cf82e5afbbf4fc5667152963d5969","modified":1529586095685},{"_id":"themes/raytaylorism/languages/zh-TW.yml","hash":"ae281c898cea81f4c897c0a69c45e2ce6a4314a6","modified":1521800509198},{"_id":"source/_posts/深度学习中的卷积和池化.md","hash":"03e76171d0f651a16e0aef20f1905529e3c7e8a6","modified":1529590745945},{"_id":"source/_posts/粗糙表面计算机模拟-matlab-3dsMax仿真.md","hash":"acc52e1ece0fbb35e33875bb16291871b46f4a32","modified":1529586095687},{"_id":"source/_posts/直方图均衡化图片.md","hash":"d568f92365f0964a7a3673b276a19dc6373943b2","modified":1529586095686},{"_id":"source/imgs/alipay-rewardcode.jpg","hash":"7093a60c54438b347cb13d79b7a34c99b0d6a4e3","modified":1529586095724},{"_id":"source/imgs/avatar.jpg","hash":"ad4c9872fa0b5c143a8778d95437452e2186a2e6","modified":1529586095728},{"_id":"source/imgs/conv-kernel.png","hash":"49385ef7cb512818b398d0a7b0f05ef87eb13815","modified":1529586956114},{"_id":"source/imgs/conv_3d.jpg","hash":"685e9b2737808127c0dc5dd31f3974429ec79ba8","modified":1529587112925},{"_id":"source/imgs/conv_no_padding.gif","hash":"b7e6480ca8666308bb6d682fcfb3a1de52541800","modified":1529587008570},{"_id":"source/imgs/conv_padding.gif","hash":"766855e42a62f80f33e2e91eacd2c81de0cc4a71","modified":1529587016143},{"_id":"source/imgs/figure2.1.gif","hash":"65d8d06d3b173689605608cc0873d9f8ec54e704","modified":1529586831779},{"_id":"source/imgs/figure2.1_2.png","hash":"33324d6a1f36eebba5e4ced6c545046e1dcf34a5","modified":1529588743599},{"_id":"source/imgs/figure2.2.gif","hash":"0475289e221e9f6fab182681ea7dc2b5a5d86b35","modified":1529586849157},{"_id":"source/imgs/figure2.2_2.png","hash":"30de37808d2e242d9673db8bfce4cf61a6783c3c","modified":1529588839912},{"_id":"source/imgs/figure2.3.gif","hash":"f0c73fc225bd7ce39516671adcc152fb242173f3","modified":1529586866289},{"_id":"source/imgs/figure2.3_2.png","hash":"f5a66b81e556847aec8ddea0b91c598bf27a8cce","modified":1529588942231},{"_id":"source/imgs/figure2.4.gif","hash":"83e9c625f85dedfae3250cc70a5cca2e97d9d912","modified":1529586881522},{"_id":"source/imgs/figure2.4_2.png","hash":"51cc4ee92b896f32f3755d08ad2f80be85c2b889","modified":1529589099013},{"_id":"source/imgs/figure2.5.gif","hash":"30f768150fdd4ac706c4276b5875aa491d9c2f0d","modified":1529586898644},{"_id":"source/imgs/figure2.5_5.png","hash":"2549be25e02bb9da01ec076b2d590b488217d139","modified":1529589373285},{"_id":"source/imgs/figure2.6.gif","hash":"6e983967bb3807df17c97047fe7fc4c63c3bd0c0","modified":1529586913759},{"_id":"source/imgs/figure2.6_2.png","hash":"c134b016f110977a0329f10ecd99700db685917b","modified":1529589504884},{"_id":"source/imgs/machine-api.jpg","hash":"cdb314781288a551f97244988f29cff0d4bc0db9","modified":1529586095760},{"_id":"source/about/index.md","hash":"fa416d307e7d2e4f0162c58a0d6ffe8a40e28ee8","modified":1521800509160},{"_id":"source/reading/index.md","hash":"8f8179d7dac09b88bfc085465350a753e9eebded","modified":1521800509160},{"_id":"source/imgs/LeNet-5.png","hash":"e0b98c516f321c5a7d3462894216bb29c41c46e1","modified":1529587120037},{"_id":"source/imgs/fc.png","hash":"d0c8bf012b0155c31f7f7fb1d4b0a73ea6fdc666","modified":1529587129013},{"_id":"source/imgs/figure1.5.gif","hash":"8a0d1d60844e6df01cee4ebe6c5b78a973242804","modified":1529587086733},{"_id":"source/imgs/figure1.6.gif","hash":"dff4a86a663d6dd26902ec7b74d6d2a6d07d84a1","modified":1529587106309},{"_id":"source/imgs/flower.jpg","hash":"c182da7dd384e63ffc854e083bc2b7a035f6abed","modified":1529586095745},{"_id":"source/imgs/iclass.png","hash":"ee48ed2068acfcd8f3bbc1101134a9a68043ff4b","modified":1529586095751},{"_id":"source/imgs/mnist_data.png","hash":"8ab2f5139eaea8e13f9560ddcf03e1480d80e166","modified":1529587154517},{"_id":"source/imgs/mnist_resultpng.png","hash":"836352838b8e8d5736b0f1d15371d326e55ed6ba","modified":1529587162751},{"_id":"source/imgs/pythoner.png","hash":"926f3e215b9b527227303e2a6ec13f3cf0612d5c","modified":1529586095762},{"_id":"source/imgs/sky.jpg","hash":"9e6292ef9a088fe81b8becc64683e66839561bf0","modified":1529586095779},{"_id":"source/imgs/surf_roughness_128.png","hash":"435a40961079504345286e199c450dd6caf151ac","modified":1529586095781},{"_id":"source/imgs/surf_3dmax.png","hash":"e29f0ff7249d89d6a7c62fa6249ab36cb0f4205e","modified":1529586095780},{"_id":"source/imgs/wetchat-rewardcode.jpg","hash":"37f65c6d09fca7a09dd10da6987268daa42203b4","modified":1529586095796},{"_id":"source/imgs/water.jpg","hash":"d70d3a35840bd6f4a785ceb68bc97459836b9886","modified":1529586095795},{"_id":"source/_data/reading.json","hash":"4b91b2b0d7d06d3cd3bf2fbe59c51884d6304b13","modified":1521800509157},{"_id":"themes/raytaylorism/layout/_widget/blogroll.ejs","hash":"1a6808fa62906e7fb1fac3e16208fa6b1fc8d0ea","modified":1521800509208},{"_id":"themes/raytaylorism/layout/_widget/tag.ejs","hash":"90e0ba4412285903420ee3b43125a56743edf0c6","modified":1521800509208},{"_id":"themes/raytaylorism/layout/_widget/recent_posts.ejs","hash":"935bfacce10a726eed6cd82fe39d2c6f9cce9e2a","modified":1521800509208},{"_id":"themes/raytaylorism/layout/_widget/tagcloud.ejs","hash":"f256f028c247bdcb7927351df89f2284c64b7b6c","modified":1521800509209},{"_id":"themes/raytaylorism/layout/_partial/after_footer.ejs","hash":"9fafc2cb14cbca89e48335d64ab058b5f256a36e","modified":1521800509198},{"_id":"themes/raytaylorism/layout/_widget/category.ejs","hash":"95292eb643be63d98f08e28f759c9b01bbfcb9b8","modified":1521800509208},{"_id":"themes/raytaylorism/layout/_partial/archive.ejs","hash":"68c7db951ffb5323d49d4de74e3b0de7f70fb4c3","modified":1521800509199},{"_id":"themes/raytaylorism/layout/_partial/archive_title.ejs","hash":"dfc6c670702e64abce5fd87e3e2ea43c966ace32","modified":1521800509199},{"_id":"themes/raytaylorism/layout/_partial/construction.ejs","hash":"21190b5a0d567ed4ea5d5289459690b72c1452f0","modified":1521800509200},{"_id":"themes/raytaylorism/layout/_partial/article.ejs","hash":"8269f333b405412510454a2a2dd4ef75a19d7465","modified":1521800509199},{"_id":"themes/raytaylorism/layout/_partial/float.ejs","hash":"a5594e23bff2047156b647fbdd0ef8247ee4ec65","modified":1521800509200},{"_id":"themes/raytaylorism/layout/_partial/feature_guide.ejs","hash":"7aefb6bdc65d1e6113cb83190fcd2f29af2c9125","modified":1521800509200},{"_id":"themes/raytaylorism/layout/_partial/footer.ejs","hash":"7d8ade0e17012bf0006d234a8e8efd633d2658f2","modified":1521800509200},{"_id":"source/_posts/Mnist手写数字体识别-tensorflow.md","hash":"a191f3b253a4f1faa9ea1dfb1a4241f5211387aa","modified":1521800509158},{"_id":"themes/raytaylorism/layout/_partial/menu_drawer.ejs","hash":"028ecbf59089cc4d1907a2d91d8da937f92d321c","modified":1521800509201},{"_id":"themes/raytaylorism/layout/_partial/pagenav.ejs","hash":"e7ada8faaee878ea4dde267d1b420bb45421670d","modified":1521800509201},{"_id":"source/_posts/关于k阶矩的理解.md","hash":"5381d5d5739318fd2c871d308d16a050ba37aae5","modified":1521800509158},{"_id":"themes/raytaylorism/layout/_partial/search.ejs","hash":"0eca40de0d39c1ae52040fcb8c9d7f79afce35dc","modified":1521800509207},{"_id":"themes/raytaylorism/layout/_partial/header.ejs","hash":"0616dd744262dd4cc98cd1cabe959643c845141f","modified":1521800509201},{"_id":"themes/raytaylorism/layout/_partial/head.ejs","hash":"7ceea72401426588cd7778f92585ab9487b463da","modified":1521800509201},{"_id":"themes/raytaylorism/layout/_partial/slider.ejs","hash":"bb7b53f6ca9c852808d955fb074f88112e51ea59","modified":1521800509207},{"_id":"themes/raytaylorism/layout/_partial/pagination.ejs","hash":"00de7746cf4ef8c4b67a72e825e5ff236f9d5814","modified":1521800509202},{"_id":"themes/raytaylorism/_md/about/index.md","hash":"fa416d307e7d2e4f0162c58a0d6ffe8a40e28ee8","modified":1521800509197},{"_id":"themes/raytaylorism/_md/reading/index.md","hash":"ab4ae4fad36f371f60b49973797a115423a784d4","modified":1521800509197},{"_id":"themes/raytaylorism/layout/_partial/side_nav.ejs","hash":"c69c45de069c348bf3906f1bd941920887a85c98","modified":1521800509207},{"_id":"themes/raytaylorism/layout/_partial/simple_article.ejs","hash":"6480e101b2f29dddd661410c56516c767d88b79f","modified":1521800509207},{"_id":"source/imgs/conv_as_matrix.png","hash":"983cde6950719d5167edb8b1c647a63f897df7d5","modified":1529587035118},{"_id":"source/imgs/figure1.5.png","hash":"dfc7b73284a3bc732295eb915b557f0a8e7df92a","modified":1529587076231},{"_id":"source/imgs/figure1.6.png","hash":"082f7688f09bca65b53b77e8dcb73492e3f6ef82","modified":1529587100172},{"_id":"source/imgs/figure2.1.png","hash":"cd375642745e34ffde4aaa4031ad0819b9d64d43","modified":1529586820498},{"_id":"source/imgs/figure2.3.png","hash":"8a21087ecf5badbcc1897715495e89a3d204a3ac","modified":1529586859307},{"_id":"source/imgs/figure2.4.png","hash":"0ff077e4226522b003ae45d9399f8505e2000156","modified":1529586874397},{"_id":"source/imgs/figure2.5.png","hash":"37752f80f83e750295334fc4cc72deace76c60d3","modified":1529586890225},{"_id":"source/imgs/figure2.6.png","hash":"2b6a7a9394a9ede424487a3de96b8ff2febe7844","modified":1529586905890},{"_id":"source/imgs/surf_roughness_256.png","hash":"f450d97d71a44c51dc4af9d2b7c73b9125240b28","modified":1529586095783},{"_id":"themes/raytaylorism/source/js/jquery.min.js","hash":"f694238d616f579a0690001f37984af430c19963","modified":1521800509301},{"_id":"themes/raytaylorism/source/js/prettify.js","hash":"d592e6f771c2955cea3764d819221b91bc343961","modified":1521800509303},{"_id":"source/imgs/figure2.2.png","hash":"e02638ef5be764c4f8be1168bbb555db1694f075","modified":1529586841104},{"_id":"source/imgs/histequa.png","hash":"89eb9d30577401536f20ab07ae8e5647ce79e867","modified":1529586095750},{"_id":"themes/raytaylorism/layout/_partial/plugin/google_code_prettify.ejs","hash":"336f01048440f0c9f7b75f24aafcc3a1ffefd9a0","modified":1521800509202},{"_id":"themes/raytaylorism/layout/_partial/plugin/analytics.ejs","hash":"b7dbd8342866929e683e9b013caa7324547ff704","modified":1521800509202},{"_id":"themes/raytaylorism/layout/_partial/plugin/noscript.ejs","hash":"182650c8be93b093997ac4d5fe14af2f835b98d9","modified":1521800509204},{"_id":"themes/raytaylorism/layout/_partial/plugin/mathjax.ejs","hash":"6f6b85a5876ae150d3e5f08e384aff68652c0335","modified":1521800509203},{"_id":"themes/raytaylorism/layout/_partial/plugin/comment.ejs","hash":"9d8e3cda9e11cfcb199da90e79baf11e71c2cfec","modified":1521800509202},{"_id":"themes/raytaylorism/layout/_partial/plugin/page_stat.ejs","hash":"9b667cbe0e8031997da065b667d12b0c944a9dad","modified":1521800509204},{"_id":"themes/raytaylorism/layout/_partial/plugin/main_javascript.ejs","hash":"6629eec982aa789767b83e80af12fa40189ac344","modified":1521800509203},{"_id":"themes/raytaylorism/layout/_partial/post/gallery.ejs","hash":"bd2285802766572736663e61852eb49f6acc744f","modified":1521800509205},{"_id":"themes/raytaylorism/layout/_partial/post/category.ejs","hash":"e17f452079201bd2a5a37bc76b51b132afd04faa","modified":1521800509204},{"_id":"themes/raytaylorism/layout/_partial/plugin/reward.ejs","hash":"284ab1d5cb4f43eb23b6d7a8aba2477b34abdc00","modified":1521800509204},{"_id":"themes/raytaylorism/layout/_partial/post/livere.ejs","hash":"bfad3e53acffd56b950017ed1754c7fdb8ec1486","modified":1521800509205},{"_id":"themes/raytaylorism/layout/_partial/post/readtimes.ejs","hash":"c829d0598f9906f663a8ace1c86f2aa6024d642c","modified":1521800509205},{"_id":"themes/raytaylorism/layout/_partial/post/tablecontents.ejs","hash":"585ea42410648f193184931864a64b41635af956","modified":1521800509206},{"_id":"themes/raytaylorism/layout/_partial/post/prevnext.ejs","hash":"6556eea4fb351639006c16e9831fd72ab46076ba","modified":1521800509205},{"_id":"themes/raytaylorism/layout/_partial/post/tag.ejs","hash":"0f84c1aded9ba1887566d34e7f0d696c015295f0","modified":1521800509206},{"_id":"themes/raytaylorism/layout/_partial/post/title.ejs","hash":"f0733a134b375172a2cec830d7d09bdba33891fe","modified":1521800509206},{"_id":"themes/raytaylorism/layout/_partial/post/time.ejs","hash":"42210d6b5a132f5c18352dcff2983d3fdbe26956","modified":1521800509206},{"_id":"themes/raytaylorism/source/css/_base/layout.styl","hash":"b2f718418de61946504a3f8bf28b75be165913a7","modified":1521800509212},{"_id":"themes/raytaylorism/source/css/_base/variable.styl","hash":"ce4e056d1bbfb80734d98a6898950e7c0136edf4","modified":1521800509213},{"_id":"themes/raytaylorism/source/css/_partial/comment.styl","hash":"590f1386581181ab588be06e4189861f5a209467","modified":1521800509215},{"_id":"themes/raytaylorism/source/css/_base/icons.css","hash":"ab167f1694ffe10c3c51d18a633efd41be121555","modified":1521800509212},{"_id":"themes/raytaylorism/source/css/_base/lib_customize.styl","hash":"5f25b295a3ad99991952f864573c0f1ccc6a1591","modified":1521800509212},{"_id":"themes/raytaylorism/source/css/_partial/archive.styl","hash":"4d48566e9f72b8eac8875b6985885418f56fbafa","modified":1521800509214},{"_id":"themes/raytaylorism/source/css/_partial/footer.styl","hash":"7f2c22ebc3fe551496625e9453017e512d670aea","modified":1521800509215},{"_id":"themes/raytaylorism/source/css/_partial/header.styl","hash":"ebfd0155cda8a0876c36595708f02c294a7c82a0","modified":1521800509215},{"_id":"themes/raytaylorism/source/css/style.styl","hash":"a05bcd2543b7bdcd3f725db6d053cd76ccf154be","modified":1521800509300},{"_id":"themes/raytaylorism/source/css/_partial/about.styl","hash":"def183d6908ebcbd59341b09e9f7e06dc277b9ca","modified":1521800509214},{"_id":"themes/raytaylorism/source/css/_partial/article.styl","hash":"293e38a8ab9aee346cc8e52421f1519c5a46a667","modified":1521800509214},{"_id":"themes/raytaylorism/source/css/_partial/reading.styl","hash":"f81929fa12212465b02456d0bb3b8263355e3281","modified":1521800509216},{"_id":"themes/raytaylorism/source/css/_partial/search.styl","hash":"f9ca6f5626c795ae73ff7412ff58207b62fd64ac","modified":1521800509217},{"_id":"themes/raytaylorism/source/css/_partial/link_context.styl","hash":"5b23db4dee53cbbe9eef257f4a542823100fde72","modified":1521800509216},{"_id":"themes/raytaylorism/source/css/_partial/slider.styl","hash":"ad757e74b3500aa774636ebbe5bdcee7e52e5ad7","modified":1521800509217},{"_id":"themes/raytaylorism/source/css/_partial/other.styl","hash":"32bf499037a45ad2e0007a9ab3054067adc02506","modified":1521800509216},{"_id":"themes/raytaylorism/source/css/_partial/index.styl","hash":"ac83523dd14a1fc1fe55f98c84ed84cb03be864b","modified":1521800509216},{"_id":"themes/raytaylorism/source/css/_partial/side_nav.styl","hash":"b239b6b55e87e86d038d6aa821beeb66a9cbaf39","modified":1521800509217},{"_id":"themes/raytaylorism/source/css/_partial/syntax.styl","hash":"f39e7bb08abcc220f7c57fb413e76f4043ab9c35","modified":1521800509217},{"_id":"themes/raytaylorism/source/css/_partial/tablecontents.styl","hash":"e04fa0e7664065077750a7223ae3390cc84a4c56","modified":1521800509218},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","hash":"e320b2be926124d30998af0e149b7f06303b8f8b","modified":1521800509300},{"_id":"source/imgs/conv_as_matrix_2.png","hash":"e97001c70d7a66fa8df60a93df5d52982de843df","modified":1529587054333},{"_id":"source/imgs/orange.jpg","hash":"0c112866bd377b63b171771422d6b20b997d7e46","modified":1529586095762},{"_id":"themes/raytaylorism/source/js/materialize.min.js","hash":"04fe8bbc9a3165eb7bfb13b7166306ed671268d8","modified":1521800509302},{"_id":"source/imgs/algorithm/string/rota_str2.png","hash":"169ddce44c2ae804501828b906bd9cece5a3d775","modified":1529586095706},{"_id":"source/imgs/light.jpg","hash":"75230c1891009503598822c19d41e4a85e211244","modified":1529586095758},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1521800509292},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","hash":"14be7d7ae1894d2cc7c1a8e847df4db42a310b2f","modified":1521800509298},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1521800509227},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1521800509224},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1521800509226},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1521800509225},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1521800509229},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1521800509231},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1521800509232},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1521800509232},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1521800509229},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1521800509229},{"_id":"source/imgs/50mm-106-68.png","hash":"391095396ddd75705ce82f1a74ee225557860764","modified":1529586095704},{"_id":"source/imgs/algorithm/string/rota_str10.png","hash":"feb768182712b66df3ba47c626ffa8c2ae50dde7","modified":1529586095706},{"_id":"source/imgs/algorithm/string/rota_str4.png","hash":"5437c9889d8cd642d4e31a122efc8ee7abeab6e2","modified":1529586095709},{"_id":"source/imgs/algorithm/tree/level_tree.png","hash":"6fe7f8c02badf03b66d7f0ae1b39271ec2108396","modified":1529586095721},{"_id":"source/imgs/algorithm/tree/level_tree2.png","hash":"bf2a0461ff3501f0f436c332e872afd932322ad7","modified":1529586095722},{"_id":"source/imgs/algorithm/tree/level_tree3.png","hash":"7f3a2d38dd3f4bb79c713d1f88455bc114640108","modified":1529586095723},{"_id":"source/imgs/water-circle.jpg","hash":"f8cc3871a46549b703063c864b764e22ec3d13c2","modified":1529586095793},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1521800509219},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1521800509220},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1521800509223},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1521800509223},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1521800509225},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","hash":"2cdb74e6b61dc8f08352ba61979d3de314fe2af7","modified":1521800509299},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1521800509231},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1521800509228},{"_id":"source/imgs/50mm-106-68-ans.png","hash":"b02f3c530b8275a198f2624444f3c6b2a7740fb4","modified":1529586095699},{"_id":"source/imgs/algorithm/string/rota_str3.png","hash":"cbc60e259bd9b649cce3b93479a620cb66fd84ab","modified":1529586095708},{"_id":"source/imgs/algorithm/string/rota_str6.png","hash":"e64e096d5ce26167180ccc2f3332a75050669def","modified":1529586095713},{"_id":"source/imgs/algorithm/string/rota_str7.png","hash":"c3504ff240f1a5964954c3b39cf5ed3e0a34a775","modified":1529586095715},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1521800509222},{"_id":"source/imgs/all_day.jpg","hash":"e25f7319e8c3a178dffc6d93676bd184e4234926","modified":1529586095727},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1521800509234},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1521800509234},{"_id":"source/imgs/algorithm/string/rota_str5.png","hash":"48ed77eb529a4fa605be982ec03cb9d776ab71bd","modified":1529586095712},{"_id":"source/imgs/algorithm/string/rota_str8.png","hash":"ddd356d7e348611ad9f8bda46e32db52873135bf","modified":1529586095718},{"_id":"source/imgs/algorithm/string/rota_str9.png","hash":"6e19ad313c849c3c692ba46c091fd4c8765aa76a","modified":1529586095719},{"_id":"source/imgs/game.png","hash":"ca05a21991384b3fe0eaf3f529b1fbfb09439e10","modified":1529586095749},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1521800509233},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","hash":"550ef5c1253c8376f2ead32b654eb58d3c106ca3","modified":1521800509221},{"_id":"source/imgs/sky-night.jpg","hash":"1c15377e38819ad6c8bc4feb25d9a98a8b679c8e","modified":1529586095777},{"_id":"source/imgs/img-cov.png","hash":"71f7616a6e54befc3d7019b6e0b85e1bfb7cc784","modified":1529586095757},{"_id":"source/imgs/wall.png","hash":"d47ce4e9c164c9563cd230767bbc29f29a738981","modified":1529586095787},{"_id":"source/imgs/winter.jpg","hash":"d62f8b169f87e09f46884582b5652e78834433c4","modified":1529586095804},{"_id":"source/imgs/coloreggs.jpg","hash":"518341c5e974a98f8bc00b9e5972954071bf645a","modified":1529586095743},{"_id":"source/imgs/rain.png","hash":"5d382d60591561923d178dd24d89cb1189b193f6","modified":1529586095773},{"_id":"public/search.xml","hash":"f988d8aa25b37b8764c0610b50e2261ad86730c2","modified":1529590795655},{"_id":"public/reading/index.html","hash":"9f53134a213dd80d63ec446f730df9b741ea8489","modified":1529590795875},{"_id":"public/about/index.html","hash":"38d8cc8fbb605afef6e7638885b30a5cf7b67fc2","modified":1529590795882},{"_id":"public/2018/06/21/深度学习中的卷积和池化/index.html","hash":"bb6c633381f7fc2f79bd397f3c0f5c2817cf5136","modified":1529590795896},{"_id":"public/2018/03/23/粗糙表面计算机模拟-matlab-3dsMax仿真/index.html","hash":"9d17d013f26cdb97727e696c4d70b153f23f67c3","modified":1529590795896},{"_id":"public/2018/03/22/关于k阶矩的理解/index.html","hash":"4b1403375ad59d3683a0a3a88ab1cfff89449bf8","modified":1529590795896},{"_id":"public/2018/03/22/Hexo多电脑同步写作/index.html","hash":"3e10d590e31e07a3537911673dd4ab4d7fa67e20","modified":1529590795896},{"_id":"public/2018/03/20/直方图均衡化图片/index.html","hash":"67ef832e54d092808f0a72857d3ddd87451425cd","modified":1529590795896},{"_id":"public/2018/03/20/Mnist手写数字体识别-tensorflow/index.html","hash":"b914b93c0abc3b47ec30e7ed5ddb12ac39a6c14f","modified":1529590795896},{"_id":"public/2018/03/19/图像与常用算子进行卷积运算/index.html","hash":"0751d0ea125319eef556888089ed38d58fdf0662","modified":1529590795896},{"_id":"public/categories/技术文章/index.html","hash":"30708e550881d4e8babb1062ffc0e3fb16a43616","modified":1529590795896},{"_id":"public/categories/深度学习/index.html","hash":"bdc58cf17b8251b2487ff05eb032e9c0c8c918b8","modified":1529590795896},{"_id":"public/categories/图像处理/index.html","hash":"2af221396f63059bc0389071f85fdd023c81bc06","modified":1529590795896},{"_id":"public/categories/研磨-粗糙度/index.html","hash":"1e7837aa0d5e81c10c31bf3e297da71670c90022","modified":1529590795896},{"_id":"public/categories/技术文章/博客搭建/index.html","hash":"2e98a8d7cf8d32eba6709501b534dbc2855ad5d3","modified":1529590795896},{"_id":"public/categories/图像处理/图像增强/index.html","hash":"5279618d386744106e1f4f0d3af5bd2e47b5f752","modified":1529590795896},{"_id":"public/categories/研磨-粗糙度/研磨表面仿真/index.html","hash":"4909aa2dff2ccf4a489060b11edc60869aed564c","modified":1529590795896},{"_id":"public/categories/深度学习/深度学习基础/index.html","hash":"2e3f63c502c2bf9e06685ec6a774070afccb7b9b","modified":1529590795896},{"_id":"public/categories/tensorflow学习/index.html","hash":"6d594c39e77cfc3154a88205ba933ae6a833bde1","modified":1529590795896},{"_id":"public/categories/数学基础/index.html","hash":"21b11f9474b6defc9563544b8382b749cf5c140e","modified":1529590795897},{"_id":"public/categories/tensorflow学习/tensorflow-Demo/index.html","hash":"2bc2cf2275f6fefbb4366ed707b539589f3fdd7a","modified":1529590795897},{"_id":"public/archives/index.html","hash":"2715f4c53a2b10e8988a0206a15edd6f442d6f62","modified":1529590795897},{"_id":"public/categories/数学基础/随机过程/index.html","hash":"8c784f9b3ec9b3c3c1cffd874059454c242109b8","modified":1529590795897},{"_id":"public/archives/2018/index.html","hash":"28af6a78c2c25f2138dc622c5faeb8074654cb33","modified":1529590795897},{"_id":"public/archives/2018/03/index.html","hash":"1bb0318c05d98eefe3f2a47de5b895fbd5997d87","modified":1529590795897},{"_id":"public/archives/2018/06/index.html","hash":"d4b5a0cbf189dd0528714b2fab2b1671446b142b","modified":1529590795897},{"_id":"public/tags/hexo/index.html","hash":"9585fb0f2c41dca060edea8c3e689d0d0202311e","modified":1529590795897},{"_id":"public/tags/多电脑同步/index.html","hash":"cf4b7051ae464779bf2d0a0e4786df84d7bb96d2","modified":1529590795897},{"_id":"public/index.html","hash":"7253c36109c27e0d242b750b782a4f40a373af93","modified":1529590795897},{"_id":"public/tags/图像处理/index.html","hash":"614e53fb9259b805ef3e55813b225a2dc45d96ab","modified":1529590795897},{"_id":"public/tags/soble/index.html","hash":"d01de5f081779856fe7e0ed7cc41732e2d7becc8","modified":1529590795897},{"_id":"public/tags/卷积运算/index.html","hash":"cfdcf964437ca6f624a9b8d875beaf6e70fcdbb9","modified":1529590795897},{"_id":"public/tags/guass/index.html","hash":"e57d4297c926a158f18ed8033d80f217d6b0fc24","modified":1529590795897},{"_id":"public/tags/prewitt/index.html","hash":"71e600136980e6ec1c571c709a43117be3c3b656","modified":1529590795897},{"_id":"public/tags/laplacian/index.html","hash":"a4d3db5d7d32b5fc1cd57d122d271d095cdeac0f","modified":1529590795897},{"_id":"public/tags/卷积/index.html","hash":"8ef48be10b4b3b14dca61251212942a8b19f09bb","modified":1529590795897},{"_id":"public/tags/池化/index.html","hash":"5d8678eecfe679a50fec0f5b43a021f0b7c7f8ee","modified":1529590795897},{"_id":"public/tags/tensorflow/index.html","hash":"afa035836c39d9d2f49b85e2838f329669767438","modified":1529590795897},{"_id":"public/tags/LeNet-5/index.html","hash":"2cae1cef7c5e0c46c53e5207a78319aa92a6ad47","modified":1529590795898},{"_id":"public/tags/表面粗糙度/index.html","hash":"4ae303c283b496ee69bb6c49def48fe37dcba872","modified":1529590795898},{"_id":"public/tags/matlab模拟粗糙度/index.html","hash":"2ed9d20e60f5fd9b39af1c813837b55d01a78806","modified":1529590795898},{"_id":"public/tags/3dsMax仿真/index.html","hash":"aebd9e32b58c227faacf0870dfe3dc1a53a59e22","modified":1529590795898},{"_id":"public/tags/python/index.html","hash":"4b325d54925afe7a6924087826afb0f384106d68","modified":1529590795898},{"_id":"public/tags/直方图均衡化/index.html","hash":"b31b9e962299b31f407a9e1e12fb125715db73e9","modified":1529590795898},{"_id":"public/tags/Mnist/index.html","hash":"1fc954045ec655e8187f4ef3798c1636cd412c55","modified":1529590795898},{"_id":"public/tags/k阶矩/index.html","hash":"3462f123e2c847d1bca96e80c2f8019d821493da","modified":1529590795898},{"_id":"public/tags/偏度/index.html","hash":"572a4f6b9ac37221108c26281ba4aa2efd24eb9f","modified":1529590795898},{"_id":"public/tags/随机过程/index.html","hash":"049c0a5ce663e6bb1e955f93ab2776962b6d88cb","modified":1529590795898},{"_id":"public/tags/峰度/index.html","hash":"ceb74a49743957a8cc22c3a2aaad08975ef9aaa2","modified":1529590795898},{"_id":"public/favicon.png","hash":"f28180f9a5026132b36b4a786c0577e68ea1fe55","modified":1529590795898},{"_id":"public/imgs/alipay-rewardcode.jpg","hash":"7093a60c54438b347cb13d79b7a34c99b0d6a4e3","modified":1529590795898},{"_id":"public/imgs/avatar.jpg","hash":"ad4c9872fa0b5c143a8778d95437452e2186a2e6","modified":1529590795898},{"_id":"public/imgs/conv-kernel.png","hash":"49385ef7cb512818b398d0a7b0f05ef87eb13815","modified":1529590795898},{"_id":"public/imgs/conv_3d.jpg","hash":"685e9b2737808127c0dc5dd31f3974429ec79ba8","modified":1529590795898},{"_id":"public/imgs/conv_no_padding.gif","hash":"b7e6480ca8666308bb6d682fcfb3a1de52541800","modified":1529590795898},{"_id":"public/imgs/conv_padding.gif","hash":"766855e42a62f80f33e2e91eacd2c81de0cc4a71","modified":1529590795898},{"_id":"public/imgs/figure2.1.gif","hash":"65d8d06d3b173689605608cc0873d9f8ec54e704","modified":1529590795899},{"_id":"public/imgs/figure2.1_2.png","hash":"33324d6a1f36eebba5e4ced6c545046e1dcf34a5","modified":1529590795899},{"_id":"public/imgs/figure2.2.gif","hash":"0475289e221e9f6fab182681ea7dc2b5a5d86b35","modified":1529590795899},{"_id":"public/imgs/figure2.2_2.png","hash":"30de37808d2e242d9673db8bfce4cf61a6783c3c","modified":1529590795899},{"_id":"public/imgs/figure2.3_2.png","hash":"f5a66b81e556847aec8ddea0b91c598bf27a8cce","modified":1529590795899},{"_id":"public/imgs/figure2.3.gif","hash":"f0c73fc225bd7ce39516671adcc152fb242173f3","modified":1529590795899},{"_id":"public/imgs/figure2.4_2.png","hash":"51cc4ee92b896f32f3755d08ad2f80be85c2b889","modified":1529590795899},{"_id":"public/imgs/figure2.4.gif","hash":"83e9c625f85dedfae3250cc70a5cca2e97d9d912","modified":1529590795899},{"_id":"public/imgs/figure2.5_5.png","hash":"2549be25e02bb9da01ec076b2d590b488217d139","modified":1529590795899},{"_id":"public/imgs/figure2.5.gif","hash":"30f768150fdd4ac706c4276b5875aa491d9c2f0d","modified":1529590795899},{"_id":"public/imgs/figure2.6.gif","hash":"6e983967bb3807df17c97047fe7fc4c63c3bd0c0","modified":1529590795899},{"_id":"public/imgs/figure2.6_2.png","hash":"c134b016f110977a0329f10ecd99700db685917b","modified":1529590795899},{"_id":"public/imgs/machine-api.jpg","hash":"cdb314781288a551f97244988f29cff0d4bc0db9","modified":1529590795899},{"_id":"public/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1529590795899},{"_id":"public/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1529590795899},{"_id":"public/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1529590795899},{"_id":"public/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1529590795899},{"_id":"public/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1529590795900},{"_id":"public/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1529590795900},{"_id":"public/imgs/LeNet-5.png","hash":"e0b98c516f321c5a7d3462894216bb29c41c46e1","modified":1529590796199},{"_id":"public/imgs/fc.png","hash":"d0c8bf012b0155c31f7f7fb1d4b0a73ea6fdc666","modified":1529590796199},{"_id":"public/imgs/figure1.5.gif","hash":"8a0d1d60844e6df01cee4ebe6c5b78a973242804","modified":1529590796201},{"_id":"public/imgs/figure1.6.gif","hash":"dff4a86a663d6dd26902ec7b74d6d2a6d07d84a1","modified":1529590796201},{"_id":"public/imgs/flower.jpg","hash":"c182da7dd384e63ffc854e083bc2b7a035f6abed","modified":1529590796201},{"_id":"public/imgs/iclass.png","hash":"ee48ed2068acfcd8f3bbc1101134a9a68043ff4b","modified":1529590796201},{"_id":"public/imgs/mnist_data.png","hash":"8ab2f5139eaea8e13f9560ddcf03e1480d80e166","modified":1529590796201},{"_id":"public/imgs/mnist_resultpng.png","hash":"836352838b8e8d5736b0f1d15371d326e55ed6ba","modified":1529590796201},{"_id":"public/imgs/pythoner.png","hash":"926f3e215b9b527227303e2a6ec13f3cf0612d5c","modified":1529590796202},{"_id":"public/imgs/sky.jpg","hash":"9e6292ef9a088fe81b8becc64683e66839561bf0","modified":1529590796202},{"_id":"public/imgs/surf_3dmax.png","hash":"e29f0ff7249d89d6a7c62fa6249ab36cb0f4205e","modified":1529590796202},{"_id":"public/imgs/surf_roughness_128.png","hash":"435a40961079504345286e199c450dd6caf151ac","modified":1529590796202},{"_id":"public/imgs/wetchat-rewardcode.jpg","hash":"37f65c6d09fca7a09dd10da6987268daa42203b4","modified":1529590796202},{"_id":"public/imgs/water.jpg","hash":"d70d3a35840bd6f4a785ceb68bc97459836b9886","modified":1529590796202},{"_id":"public/imgs/algorithm/string/rota_str2.png","hash":"169ddce44c2ae804501828b906bd9cece5a3d775","modified":1529590796202},{"_id":"public/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1529590796202},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1529590796202},{"_id":"public/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1529590796202},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1529590796202},{"_id":"public/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1529590796202},{"_id":"public/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1529590796202},{"_id":"public/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1529590796203},{"_id":"public/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1529590796203},{"_id":"public/js/prettify.js","hash":"d24b1da342b5c2d0582f0922118aaf0b2a6840d5","modified":1529590796210},{"_id":"public/css/lib/prettify-tomorrow-night-eighties.css","hash":"35e07bd7a4585363060edd558a0e9939e7e68323","modified":1529590796210},{"_id":"public/js/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1529590796210},{"_id":"public/css/style.css","hash":"55a917def994d7b8bd35785f6b3d147435bc0b88","modified":1529590796210},{"_id":"public/js/materialize.min.js","hash":"c9308fbe808a149aa11061af40a4be5f391cccee","modified":1529590796210},{"_id":"public/css/lib/font-awesome.min.css","hash":"683d12731b7429d32ec7de00a6706602e403013f","modified":1529590796210},{"_id":"public/css/lib/materialize.min.css","hash":"0f3f6e0cc632835eca104efc5c26dfc5f4d7e5f1","modified":1529590796210},{"_id":"public/imgs/conv_as_matrix.png","hash":"983cde6950719d5167edb8b1c647a63f897df7d5","modified":1529590796211},{"_id":"public/imgs/figure1.5.png","hash":"dfc7b73284a3bc732295eb915b557f0a8e7df92a","modified":1529590796211},{"_id":"public/imgs/figure1.6.png","hash":"082f7688f09bca65b53b77e8dcb73492e3f6ef82","modified":1529590796211},{"_id":"public/imgs/figure2.1.png","hash":"cd375642745e34ffde4aaa4031ad0819b9d64d43","modified":1529590796211},{"_id":"public/imgs/figure2.3.png","hash":"8a21087ecf5badbcc1897715495e89a3d204a3ac","modified":1529590796211},{"_id":"public/imgs/figure2.4.png","hash":"0ff077e4226522b003ae45d9399f8505e2000156","modified":1529590796211},{"_id":"public/imgs/figure2.5.png","hash":"37752f80f83e750295334fc4cc72deace76c60d3","modified":1529590796211},{"_id":"public/imgs/surf_roughness_256.png","hash":"f450d97d71a44c51dc4af9d2b7c73b9125240b28","modified":1529590796212},{"_id":"public/imgs/figure2.6.png","hash":"2b6a7a9394a9ede424487a3de96b8ff2febe7844","modified":1529590796212},{"_id":"public/imgs/algorithm/string/rota_str10.png","hash":"feb768182712b66df3ba47c626ffa8c2ae50dde7","modified":1529590796212},{"_id":"public/imgs/algorithm/string/rota_str4.png","hash":"5437c9889d8cd642d4e31a122efc8ee7abeab6e2","modified":1529590796212},{"_id":"public/imgs/algorithm/tree/level_tree.png","hash":"6fe7f8c02badf03b66d7f0ae1b39271ec2108396","modified":1529590796212},{"_id":"public/imgs/algorithm/tree/level_tree2.png","hash":"bf2a0461ff3501f0f436c332e872afd932322ad7","modified":1529590796212},{"_id":"public/imgs/algorithm/tree/level_tree3.png","hash":"7f3a2d38dd3f4bb79c713d1f88455bc114640108","modified":1529590796212},{"_id":"public/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1529590796213},{"_id":"public/imgs/histequa.png","hash":"89eb9d30577401536f20ab07ae8e5647ce79e867","modified":1529590796219},{"_id":"public/imgs/figure2.2.png","hash":"e02638ef5be764c4f8be1168bbb555db1694f075","modified":1529590796219},{"_id":"public/imgs/algorithm/string/rota_str3.png","hash":"cbc60e259bd9b649cce3b93479a620cb66fd84ab","modified":1529590796219},{"_id":"public/imgs/algorithm/string/rota_str6.png","hash":"e64e096d5ce26167180ccc2f3332a75050669def","modified":1529590796220},{"_id":"public/imgs/algorithm/string/rota_str7.png","hash":"c3504ff240f1a5964954c3b39cf5ed3e0a34a775","modified":1529590796220},{"_id":"public/imgs/conv_as_matrix_2.png","hash":"e97001c70d7a66fa8df60a93df5d52982de843df","modified":1529590796223},{"_id":"public/imgs/orange.jpg","hash":"0c112866bd377b63b171771422d6b20b997d7e46","modified":1529590796223},{"_id":"public/imgs/light.jpg","hash":"75230c1891009503598822c19d41e4a85e211244","modified":1529590796227},{"_id":"public/imgs/algorithm/string/rota_str5.png","hash":"48ed77eb529a4fa605be982ec03cb9d776ab71bd","modified":1529590796227},{"_id":"public/imgs/algorithm/string/rota_str8.png","hash":"ddd356d7e348611ad9f8bda46e32db52873135bf","modified":1529590796227},{"_id":"public/imgs/algorithm/string/rota_str9.png","hash":"6e19ad313c849c3c692ba46c091fd4c8765aa76a","modified":1529590796228},{"_id":"public/css/font/font-awesome/fontawesome-webfont.svg","hash":"550ef5c1253c8376f2ead32b654eb58d3c106ca3","modified":1529590796228},{"_id":"public/imgs/50mm-106-68.png","hash":"391095396ddd75705ce82f1a74ee225557860764","modified":1529590796279},{"_id":"public/imgs/water-circle.jpg","hash":"f8cc3871a46549b703063c864b764e22ec3d13c2","modified":1529590796279},{"_id":"public/imgs/50mm-106-68-ans.png","hash":"b02f3c530b8275a198f2624444f3c6b2a7740fb4","modified":1529590796310},{"_id":"public/imgs/all_day.jpg","hash":"e25f7319e8c3a178dffc6d93676bd184e4234926","modified":1529590796316},{"_id":"public/imgs/game.png","hash":"ca05a21991384b3fe0eaf3f529b1fbfb09439e10","modified":1529590796322},{"_id":"public/imgs/sky-night.jpg","hash":"1c15377e38819ad6c8bc4feb25d9a98a8b679c8e","modified":1529590796330},{"_id":"public/imgs/img-cov.png","hash":"71f7616a6e54befc3d7019b6e0b85e1bfb7cc784","modified":1529590796364},{"_id":"public/imgs/wall.png","hash":"d47ce4e9c164c9563cd230767bbc29f29a738981","modified":1529590796366},{"_id":"public/imgs/winter.jpg","hash":"d62f8b169f87e09f46884582b5652e78834433c4","modified":1529590796371},{"_id":"public/imgs/coloreggs.jpg","hash":"518341c5e974a98f8bc00b9e5972954071bf645a","modified":1529590796374},{"_id":"public/imgs/rain.png","hash":"5d382d60591561923d178dd24d89cb1189b193f6","modified":1529590796376}],"Category":[{"name":"技术文章","_id":"cjiomv74r0004kou8mnw47j8j"},{"name":"图像处理","_id":"cjiomv74w0009kou8tdg6vbkc"},{"name":"深度学习","_id":"cjiomv74x000bkou8h5ulvaqh"},{"name":"研磨&粗糙度","_id":"cjiomv74y000fkou86ehsi8go"},{"name":"博客搭建","parent":"cjiomv74r0004kou8mnw47j8j","_id":"cjiomv74z000hkou8z8mk7gz8"},{"name":"图像增强","parent":"cjiomv74w0009kou8tdg6vbkc","_id":"cjiomv752000lkou8a7umx1wy"},{"name":"深度学习基础","parent":"cjiomv74x000bkou8h5ulvaqh","_id":"cjiomv754000pkou85zt768gf"},{"name":"研磨表面仿真","parent":"cjiomv74y000fkou86ehsi8go","_id":"cjiomv755000skou8qvm8wy9c"},{"name":"tensorflow学习","_id":"cjiomv77a001tkou8zcwljtf1"},{"name":"数学基础","_id":"cjiomv77b001vkou8f8tp3jwf"},{"name":"tensorflow-Demo","parent":"cjiomv77a001tkou8zcwljtf1","_id":"cjiomv77d001zkou8ovchy63b"},{"name":"随机过程","parent":"cjiomv77b001vkou8f8tp3jwf","_id":"cjiomv77e0021kou8fj3c346o"}],"Data":[{"_id":"hint","data":{"new":{"selector":[".menu-reading"]}}},{"_id":"about","data":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]}},{"_id":"link","data":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}}},{"_id":"slider","data":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}]},{"_id":"reading","data":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}],"Page":[{"title":"关于","layout":"about","_content":"大家好，我是tech.radish。欢迎来到我的个人技术博客。\n","source":"about/index.md","raw":"title: 关于\nlayout: about\n---\n大家好，我是tech.radish。欢迎来到我的个人技术博客。\n","date":"2018-03-23T10:21:49.160Z","updated":"2018-03-23T10:21:49.160Z","path":"about/index.html","comments":1,"_id":"cjiomv74n0001kou8nt3x0tv0","content":"<p>大家好，我是tech.radish。欢迎来到我的个人技术博客。</p>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"","more":"<p>大家好，我是tech.radish。欢迎来到我的个人技术博客。</p>\n"},{"title":"读书","layout":"reading","_content":"# 我想读 #\n\n书籍1\n\n----------\n\n书籍2\n","source":"reading/index.md","raw":"title: 读书\nlayout: reading\n---\n# 我想读 #\n\n书籍1\n\n----------\n\n书籍2\n","date":"2018-03-23T10:21:49.160Z","updated":"2018-03-23T10:21:49.160Z","path":"reading/index.html","comments":1,"_id":"cjiomv74q0003kou8xof7stjp","content":"<h1 id=\"我想读\"><a href=\"#我想读\" class=\"headerlink\" title=\"我想读\"></a>我想读</h1><p>书籍1</p>\n<hr>\n<p>书籍2</p>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"","more":"<h1 id=\"我想读\"><a href=\"#我想读\" class=\"headerlink\" title=\"我想读\"></a>我想读</h1><p>书籍1</p>\n<hr>\n<p>书籍2</p>\n"}],"Post":[{"title":"Hexo多电脑同步写作","date":"2018-03-22T13:01:20.000Z","_content":"利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明\n\n# 1.上传文件到仓库 #\n前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。\n首先你要明白，你创建的博客通过`hexo d`命令部署到github之后,和你的本地博客根目录下的`.deploy_git`文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。\n\n<!-- more -->\n\n1. 首先在你Github账户上新建一个仓库，例如名为`hexo-blog`\n2. 将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。\n\t1. scaffolds\n\t2. source\n\t3. themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，`ls -a && rm .git`）\n\t4. _config.yml\n\t5. package.json\n3.  在hexo-blog目录中执行\n\t1.  `git init`\n\t2.  `git add .`\n\t3.  `git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`（使用你新建的仓库的SSH地址）\n\t4.  `git commit -m 'blog source bakcup'`(commit之后才能创建分支)\n\t5.  `git branch hexo`创建一个hexo分支\n\t6.  `git checkout hexo`切换到hexo分支\n\t5.  `git push origin hexo`\n4. 在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。\n\n# 2. 下载文件 #\n上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。\n\n1. 进入到你放置博客的目录中，然后执行`git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`\n2. `cd hexo-blog`进入此仓库目录中\n3. 执行`npm install`安装所需组件\n4. 使用`hexo g && hexo s -p 8080` 在本地打开浏览器输入`localhost:8080` 查看与在线的博客是否一致。\n5. 使用`hexo new \"page name\"`新建一片博客，写完一篇博客，然后部署`hexo clean && hexo g && hexo d`,再执行以下命令来完成同步\n\t1. `git add .`\n\t2. `git commit -m 'add a new page'`\n\t3. `git push origin hexo`\n6. 此时就可以在你原先电脑上执行`git pull origin hexo`来完成同步了。 \n\n# 留言 #\n如果还有不懂请在下面留言，我会及时回复。","source":"_posts/Hexo多电脑同步写作.md","raw":"---\ntitle: Hexo多电脑同步写作\ndate: 2018-03-22 21:01:20\ncategories:\n- 技术文章\n- 博客搭建\ntags:\n- hexo\n- 多电脑同步\n---\n利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明\n\n# 1.上传文件到仓库 #\n前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。\n首先你要明白，你创建的博客通过`hexo d`命令部署到github之后,和你的本地博客根目录下的`.deploy_git`文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。\n\n<!-- more -->\n\n1. 首先在你Github账户上新建一个仓库，例如名为`hexo-blog`\n2. 将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。\n\t1. scaffolds\n\t2. source\n\t3. themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，`ls -a && rm .git`）\n\t4. _config.yml\n\t5. package.json\n3.  在hexo-blog目录中执行\n\t1.  `git init`\n\t2.  `git add .`\n\t3.  `git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`（使用你新建的仓库的SSH地址）\n\t4.  `git commit -m 'blog source bakcup'`(commit之后才能创建分支)\n\t5.  `git branch hexo`创建一个hexo分支\n\t6.  `git checkout hexo`切换到hexo分支\n\t5.  `git push origin hexo`\n4. 在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。\n\n# 2. 下载文件 #\n上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。\n\n1. 进入到你放置博客的目录中，然后执行`git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`\n2. `cd hexo-blog`进入此仓库目录中\n3. 执行`npm install`安装所需组件\n4. 使用`hexo g && hexo s -p 8080` 在本地打开浏览器输入`localhost:8080` 查看与在线的博客是否一致。\n5. 使用`hexo new \"page name\"`新建一片博客，写完一篇博客，然后部署`hexo clean && hexo g && hexo d`,再执行以下命令来完成同步\n\t1. `git add .`\n\t2. `git commit -m 'add a new page'`\n\t3. `git push origin hexo`\n6. 此时就可以在你原先电脑上执行`git pull origin hexo`来完成同步了。 \n\n# 留言 #\n如果还有不懂请在下面留言，我会及时回复。","slug":"Hexo多电脑同步写作","published":1,"updated":"2018-03-23T10:21:49.157Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv74k0000kou8vutuiah6","content":"<p>利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明</p>\n<h1 id=\"1-上传文件到仓库\"><a href=\"#1-上传文件到仓库\" class=\"headerlink\" title=\"1.上传文件到仓库\"></a>1.上传文件到仓库</h1><p>前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。<br>首先你要明白，你创建的博客通过<code>hexo d</code>命令部署到github之后,和你的本地博客根目录下的<code>.deploy_git</code>文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。</p>\n<a id=\"more\"></a>\n<ol>\n<li>首先在你Github账户上新建一个仓库，例如名为<code>hexo-blog</code></li>\n<li>将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。<ol>\n<li>scaffolds</li>\n<li>source</li>\n<li>themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，<code>ls -a &amp;&amp; rm .git</code>）</li>\n<li>_config.yml</li>\n<li>package.json</li>\n</ol>\n</li>\n<li>在hexo-blog目录中执行<ol>\n<li><code>git init</code></li>\n<li><code>git add .</code></li>\n<li><code>git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code>（使用你新建的仓库的SSH地址）</li>\n<li><code>git commit -m &#39;blog source bakcup&#39;</code>(commit之后才能创建分支)</li>\n<li><code>git branch hexo</code>创建一个hexo分支</li>\n<li><code>git checkout hexo</code>切换到hexo分支</li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。</li>\n</ol>\n<h1 id=\"2-下载文件\"><a href=\"#2-下载文件\" class=\"headerlink\" title=\"2. 下载文件\"></a>2. 下载文件</h1><p>上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。</p>\n<ol>\n<li>进入到你放置博客的目录中，然后执行<code>git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code></li>\n<li><code>cd hexo-blog</code>进入此仓库目录中</li>\n<li>执行<code>npm install</code>安装所需组件</li>\n<li>使用<code>hexo g &amp;&amp; hexo s -p 8080</code> 在本地打开浏览器输入<code>localhost:8080</code> 查看与在线的博客是否一致。</li>\n<li>使用<code>hexo new &quot;page name&quot;</code>新建一片博客，写完一篇博客，然后部署<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code>,再执行以下命令来完成同步<ol>\n<li><code>git add .</code></li>\n<li><code>git commit -m &#39;add a new page&#39;</code></li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>此时就可以在你原先电脑上执行<code>git pull origin hexo</code>来完成同步了。 </li>\n</ol>\n<h1 id=\"留言\"><a href=\"#留言\" class=\"headerlink\" title=\"留言\"></a>留言</h1><p>如果还有不懂请在下面留言，我会及时回复。</p>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"<p>利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明</p>\n<h1 id=\"1-上传文件到仓库\"><a href=\"#1-上传文件到仓库\" class=\"headerlink\" title=\"1.上传文件到仓库\"></a>1.上传文件到仓库</h1><p>前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。<br>首先你要明白，你创建的博客通过<code>hexo d</code>命令部署到github之后,和你的本地博客根目录下的<code>.deploy_git</code>文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。</p>","more":"<ol>\n<li>首先在你Github账户上新建一个仓库，例如名为<code>hexo-blog</code></li>\n<li>将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。<ol>\n<li>scaffolds</li>\n<li>source</li>\n<li>themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，<code>ls -a &amp;&amp; rm .git</code>）</li>\n<li>_config.yml</li>\n<li>package.json</li>\n</ol>\n</li>\n<li>在hexo-blog目录中执行<ol>\n<li><code>git init</code></li>\n<li><code>git add .</code></li>\n<li><code>git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code>（使用你新建的仓库的SSH地址）</li>\n<li><code>git commit -m &#39;blog source bakcup&#39;</code>(commit之后才能创建分支)</li>\n<li><code>git branch hexo</code>创建一个hexo分支</li>\n<li><code>git checkout hexo</code>切换到hexo分支</li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。</li>\n</ol>\n<h1 id=\"2-下载文件\"><a href=\"#2-下载文件\" class=\"headerlink\" title=\"2. 下载文件\"></a>2. 下载文件</h1><p>上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。</p>\n<ol>\n<li>进入到你放置博客的目录中，然后执行<code>git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code></li>\n<li><code>cd hexo-blog</code>进入此仓库目录中</li>\n<li>执行<code>npm install</code>安装所需组件</li>\n<li>使用<code>hexo g &amp;&amp; hexo s -p 8080</code> 在本地打开浏览器输入<code>localhost:8080</code> 查看与在线的博客是否一致。</li>\n<li>使用<code>hexo new &quot;page name&quot;</code>新建一片博客，写完一篇博客，然后部署<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code>,再执行以下命令来完成同步<ol>\n<li><code>git add .</code></li>\n<li><code>git commit -m &#39;add a new page&#39;</code></li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>此时就可以在你原先电脑上执行<code>git pull origin hexo</code>来完成同步了。 </li>\n</ol>\n<h1 id=\"留言\"><a href=\"#留言\" class=\"headerlink\" title=\"留言\"></a>留言</h1><p>如果还有不懂请在下面留言，我会及时回复。</p>"},{"title":"图像与常用算子进行卷积运算","date":"2018-03-19T11:11:57.000Z","_content":"> 图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。\n\n<!-- more -->\n\n# 实现代码 #\n\n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/9/18 16:57\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : image_convolve.py\n\t# @ToDo    :  图像卷积\n\t\n\timport numpy as np\n\timport os\n\tfrom PIL import Image\n\t\n\t\n\tdef convolve(image, weight):\n\t\theight, width = image.shape\n\t\th, w = weight.shape\n\t\theight_new = height - h + 1\n\t\twidth_new = width - w + 1\n\t\tprint image.shape\n\t\timage_new = np.zeros((height_new, width_new), dtype=np.float)\n\t\tfor i in range(height_new):\n\t\t\tfor j in range(width_new):\n\t\t\t\timage_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n\t\timage_new = image_new.clip(0, 255)\n\t\timage_new = np.rint(image_new).astype(\"uint8\")\n\t\tprint image_new.shape\n\t\treturn image_new\n\t\n\t\n\t# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\t\n\tif __name__ == '__main__':\n\t\timage = Image.open(\"son.png\", \"r\")\n\t\toutput_path = \".\\\\ImageConvolve\\\\\"\n\t\tif not os.path.exists(output_path):\n\t\t\tos.mkdir(output_path)\n\t\ta = np.array(image)\n\t\tavg3 = np.ones((3, 3))\n\t\tavg3 /= avg3.sum()\n\t\tavg5 = np.ones((5, 5))\n\t\tavg5 /= avg5.sum()\n\t\n\t\tgauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.022, 0.097, 0.159, 0.097, 0.022],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.003, 0.013, 0.022, 0.013, 0.003]))\n\t\n\t\tsoble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n\t\tsoble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n\t\tsoble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\t\n\t\tprewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n\t\tprewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n\t\tprewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\t\n\t\tlaplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n\t\tlaplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n\t\tweight_list = (\n\t\t\t'avg3', 'avg5', 'gauss', 'soble_x', 'soble_y', 'soble', 'prewitt_x', 'prewitt_y', 'prewitt', 'laplacian4',\n\t\t\t'laplacian8')\n\t\n\t\tprint \"梯度检测\"\n\t\tfor weight in weight_list:\n\t\t\tprint weight, \"R\",\n\t\t\tR = convolve(a[:, :, 0], eval(weight))\n\t\t\tprint \"G\",\n\t\t\tG = convolve(a[:, :, 1], eval(weight))\n\t\t\tprint \"B\"\n\t\t\tB = convolve(a[:, :, 2], eval(weight))\n\t\t\tI = np.stack((R, G, B), 2)\n\t\t# Image.fromarray(I).save(output_path + weight + \".png\")\n\n# 实验结果 #\n![图像卷积运算实验结果](https://mic-jasontang.github.io/imgs/img-cov.png)","source":"_posts/图像与常用算子进行卷积运算.md","raw":"---\ntitle: 图像与常用算子进行卷积运算\ndate: 2018-03-19 19:11:57\ncategories:\n- 图像处理\n- 图像增强\ntags:\n- 图像处理 \n- 卷积运算 \n- guass \n- soble \n- prewitt \n- laplacian\n---\n> 图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。\n\n<!-- more -->\n\n# 实现代码 #\n\n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/9/18 16:57\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : image_convolve.py\n\t# @ToDo    :  图像卷积\n\t\n\timport numpy as np\n\timport os\n\tfrom PIL import Image\n\t\n\t\n\tdef convolve(image, weight):\n\t\theight, width = image.shape\n\t\th, w = weight.shape\n\t\theight_new = height - h + 1\n\t\twidth_new = width - w + 1\n\t\tprint image.shape\n\t\timage_new = np.zeros((height_new, width_new), dtype=np.float)\n\t\tfor i in range(height_new):\n\t\t\tfor j in range(width_new):\n\t\t\t\timage_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n\t\timage_new = image_new.clip(0, 255)\n\t\timage_new = np.rint(image_new).astype(\"uint8\")\n\t\tprint image_new.shape\n\t\treturn image_new\n\t\n\t\n\t# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\t\n\tif __name__ == '__main__':\n\t\timage = Image.open(\"son.png\", \"r\")\n\t\toutput_path = \".\\\\ImageConvolve\\\\\"\n\t\tif not os.path.exists(output_path):\n\t\t\tos.mkdir(output_path)\n\t\ta = np.array(image)\n\t\tavg3 = np.ones((3, 3))\n\t\tavg3 /= avg3.sum()\n\t\tavg5 = np.ones((5, 5))\n\t\tavg5 /= avg5.sum()\n\t\n\t\tgauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.022, 0.097, 0.159, 0.097, 0.022],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.003, 0.013, 0.022, 0.013, 0.003]))\n\t\n\t\tsoble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n\t\tsoble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n\t\tsoble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\t\n\t\tprewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n\t\tprewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n\t\tprewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\t\n\t\tlaplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n\t\tlaplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n\t\tweight_list = (\n\t\t\t'avg3', 'avg5', 'gauss', 'soble_x', 'soble_y', 'soble', 'prewitt_x', 'prewitt_y', 'prewitt', 'laplacian4',\n\t\t\t'laplacian8')\n\t\n\t\tprint \"梯度检测\"\n\t\tfor weight in weight_list:\n\t\t\tprint weight, \"R\",\n\t\t\tR = convolve(a[:, :, 0], eval(weight))\n\t\t\tprint \"G\",\n\t\t\tG = convolve(a[:, :, 1], eval(weight))\n\t\t\tprint \"B\"\n\t\t\tB = convolve(a[:, :, 2], eval(weight))\n\t\t\tI = np.stack((R, G, B), 2)\n\t\t# Image.fromarray(I).save(output_path + weight + \".png\")\n\n# 实验结果 #\n![图像卷积运算实验结果](https://mic-jasontang.github.io/imgs/img-cov.png)","slug":"图像与常用算子进行卷积运算","published":1,"updated":"2018-06-21T13:01:35.685Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv74o0002kou83svzhk13","content":"<blockquote>\n<p>图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1 id=\"实现代码\"><a href=\"#实现代码\" class=\"headerlink\" title=\"实现代码\"></a>实现代码</h1><pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/9/18 16:57\n# @Author  : Jasontang\n# @Site    : \n# @File    : image_convolve.py\n# @ToDo    :  图像卷积\n\nimport numpy as np\nimport os\nfrom PIL import Image\n\n\ndef convolve(image, weight):\n    height, width = image.shape\n    h, w = weight.shape\n    height_new = height - h + 1\n    width_new = width - w + 1\n    print image.shape\n    image_new = np.zeros((height_new, width_new), dtype=np.float)\n    for i in range(height_new):\n        for j in range(width_new):\n            image_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n    image_new = image_new.clip(0, 255)\n    image_new = np.rint(image_new).astype(&quot;uint8&quot;)\n    print image_new.shape\n    return image_new\n\n\n# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\nif __name__ == &apos;__main__&apos;:\n    image = Image.open(&quot;son.png&quot;, &quot;r&quot;)\n    output_path = &quot;.\\\\ImageConvolve\\\\&quot;\n    if not os.path.exists(output_path):\n        os.mkdir(output_path)\n    a = np.array(image)\n    avg3 = np.ones((3, 3))\n    avg3 /= avg3.sum()\n    avg5 = np.ones((5, 5))\n    avg5 /= avg5.sum()\n\n    gauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.022, 0.097, 0.159, 0.097, 0.022],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.003, 0.013, 0.022, 0.013, 0.003]))\n\n    soble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n    soble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n    soble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\n    prewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n    prewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n    prewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\n    laplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n    laplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n    weight_list = (\n        &apos;avg3&apos;, &apos;avg5&apos;, &apos;gauss&apos;, &apos;soble_x&apos;, &apos;soble_y&apos;, &apos;soble&apos;, &apos;prewitt_x&apos;, &apos;prewitt_y&apos;, &apos;prewitt&apos;, &apos;laplacian4&apos;,\n        &apos;laplacian8&apos;)\n\n    print &quot;梯度检测&quot;\n    for weight in weight_list:\n        print weight, &quot;R&quot;,\n        R = convolve(a[:, :, 0], eval(weight))\n        print &quot;G&quot;,\n        G = convolve(a[:, :, 1], eval(weight))\n        print &quot;B&quot;\n        B = convolve(a[:, :, 2], eval(weight))\n        I = np.stack((R, G, B), 2)\n    # Image.fromarray(I).save(output_path + weight + &quot;.png&quot;)\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p><img src=\"https://mic-jasontang.github.io/imgs/img-cov.png\" alt=\"图像卷积运算实验结果\"></p>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"<blockquote>\n<p>图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。</p>\n</blockquote>","more":"<h1 id=\"实现代码\"><a href=\"#实现代码\" class=\"headerlink\" title=\"实现代码\"></a>实现代码</h1><pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/9/18 16:57\n# @Author  : Jasontang\n# @Site    : \n# @File    : image_convolve.py\n# @ToDo    :  图像卷积\n\nimport numpy as np\nimport os\nfrom PIL import Image\n\n\ndef convolve(image, weight):\n    height, width = image.shape\n    h, w = weight.shape\n    height_new = height - h + 1\n    width_new = width - w + 1\n    print image.shape\n    image_new = np.zeros((height_new, width_new), dtype=np.float)\n    for i in range(height_new):\n        for j in range(width_new):\n            image_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n    image_new = image_new.clip(0, 255)\n    image_new = np.rint(image_new).astype(&quot;uint8&quot;)\n    print image_new.shape\n    return image_new\n\n\n# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\nif __name__ == &apos;__main__&apos;:\n    image = Image.open(&quot;son.png&quot;, &quot;r&quot;)\n    output_path = &quot;.\\\\ImageConvolve\\\\&quot;\n    if not os.path.exists(output_path):\n        os.mkdir(output_path)\n    a = np.array(image)\n    avg3 = np.ones((3, 3))\n    avg3 /= avg3.sum()\n    avg5 = np.ones((5, 5))\n    avg5 /= avg5.sum()\n\n    gauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.022, 0.097, 0.159, 0.097, 0.022],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.003, 0.013, 0.022, 0.013, 0.003]))\n\n    soble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n    soble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n    soble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\n    prewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n    prewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n    prewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\n    laplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n    laplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n    weight_list = (\n        &apos;avg3&apos;, &apos;avg5&apos;, &apos;gauss&apos;, &apos;soble_x&apos;, &apos;soble_y&apos;, &apos;soble&apos;, &apos;prewitt_x&apos;, &apos;prewitt_y&apos;, &apos;prewitt&apos;, &apos;laplacian4&apos;,\n        &apos;laplacian8&apos;)\n\n    print &quot;梯度检测&quot;\n    for weight in weight_list:\n        print weight, &quot;R&quot;,\n        R = convolve(a[:, :, 0], eval(weight))\n        print &quot;G&quot;,\n        G = convolve(a[:, :, 1], eval(weight))\n        print &quot;B&quot;\n        B = convolve(a[:, :, 2], eval(weight))\n        I = np.stack((R, G, B), 2)\n    # Image.fromarray(I).save(output_path + weight + &quot;.png&quot;)\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p><img src=\"https://mic-jasontang.github.io/imgs/img-cov.png\" alt=\"图像卷积运算实验结果\"></p>"},{"title":"深度学习中的卷积和池化","date":"2018-06-21T13:05:19.000Z","_content":"\n# 1. Convolution  #\n> 卷积是什么？\n> \n> 卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。\n\n- 卷积动画演示\n\t- 卷积核\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv-kernel.png)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_no_padding.gif)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_padding.gif)\n> 在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。\n> \n> - 卷积的目的\n> \n> 卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。\n\n----------\n**卷积分为许多种，下面将会一一介绍。**\n\n- 符号约定\n\n> i: 输入大小表示为i*i\n> \n> k: 卷积核大小表示为k*k\n> \n> s: 步长\n> \n> p: 填充\n>\n> o: 输出表示为o*o\n## 1.1 unit strides ##\n卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。\n### 1.1.1  No zero padding, unit strides ###\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.png)\n\n无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1_2.png)\n\n动画演示\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.gif)\n\n### 1.1.2 Zero padding, unit strides ###\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2_2.png)\n\n动画演示\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.gif)\n\n#### 1.1.2.1 Zero padding, unit strides - Half(Same) padding ####\n这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.png)\n\n有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3_2.png)\n\n动画演示\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.gif)\n\n#### 1.1.2.2 Zero padding, unit strides - Full padding ####\n卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4_2.png)\n\n动画演示\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.gif)\n\n## 1.2 Non-unit strides ##\n接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。\n### 1.2.1 No zero padding, non-unit strides ###\n\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.png)\n\n无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5_2.png)\n\n其中向下取整是为了避免(i-k)/s是小数的情况。\n\n动画演示\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.gif)\n\n### 1.2.2 Zero padding, non-unit strides ###\n\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.png)\n\n有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6_2.png)\n\n其中向下取整是为了避免(i+2p-k)/s是小数的情况。\n\n动画演示\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.gif)\n\n## 1.3 Convolution as a matrix operation ##\n卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：\n\n1.1.1中的图被表示为如下形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png)\n\n矩阵表示的形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix.png)\n\n我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。\n# 2. Pooling  #\n> 池化操作是什么？\n>\n> 池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。\n\n池化分为平均值池化和最大值池化，下面会一一介绍。\n## 2.1 Average Pooling ##\n- 平均值池化可以被表示为\n\n![figure1.5](https://mic-jasontang.github.io/imgs/figure1.5.png)\n\n- 平均值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.5.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n\n## 2.2 Max Pooling ##\n- 最大值池化可以被表示为\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.png)\n\n- 最大值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n# 3. 3D-Conv  #\n3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。\n\n![figure1.6](https://mic-jasontang.github.io/imgs/conv_3d.jpg)\n\n输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。\n\n为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/ \"博客原地址\")\n\n\n<iframe \n    width=\"100%\" \n    height=\"100%\" \n    src=\"http://cs231n.github.io/assets/conv-demo/index.html\"\n    frameborder=\"0\" \n    allowfullscreen>\n</iframe>\n\n# 4. LeNet-5  #\n这里介绍下LeNet-5模型，为了理解前面讲述的各种模型\n\n# Bibliography #","source":"_posts/深度学习中的卷积和池化.md","raw":"---\ntitle: 深度学习中的卷积和池化\ndate: 2018-06-21 21:05:19\ncategories:\n- 深度学习\n- 深度学习基础\ntags: \n- 卷积\n- 池化\n- tensorflow\n- LeNet-5\n\n---\n\n# 1. Convolution  #\n> 卷积是什么？\n> \n> 卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。\n\n- 卷积动画演示\n\t- 卷积核\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv-kernel.png)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_no_padding.gif)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_padding.gif)\n> 在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。\n> \n> - 卷积的目的\n> \n> 卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。\n\n----------\n**卷积分为许多种，下面将会一一介绍。**\n\n- 符号约定\n\n> i: 输入大小表示为i*i\n> \n> k: 卷积核大小表示为k*k\n> \n> s: 步长\n> \n> p: 填充\n>\n> o: 输出表示为o*o\n## 1.1 unit strides ##\n卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。\n### 1.1.1  No zero padding, unit strides ###\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.png)\n\n无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1_2.png)\n\n动画演示\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.gif)\n\n### 1.1.2 Zero padding, unit strides ###\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2_2.png)\n\n动画演示\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.gif)\n\n#### 1.1.2.1 Zero padding, unit strides - Half(Same) padding ####\n这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.png)\n\n有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3_2.png)\n\n动画演示\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.gif)\n\n#### 1.1.2.2 Zero padding, unit strides - Full padding ####\n卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4_2.png)\n\n动画演示\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.gif)\n\n## 1.2 Non-unit strides ##\n接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。\n### 1.2.1 No zero padding, non-unit strides ###\n\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.png)\n\n无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5_2.png)\n\n其中向下取整是为了避免(i-k)/s是小数的情况。\n\n动画演示\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.gif)\n\n### 1.2.2 Zero padding, non-unit strides ###\n\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.png)\n\n有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6_2.png)\n\n其中向下取整是为了避免(i+2p-k)/s是小数的情况。\n\n动画演示\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.gif)\n\n## 1.3 Convolution as a matrix operation ##\n卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：\n\n1.1.1中的图被表示为如下形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png)\n\n矩阵表示的形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix.png)\n\n我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。\n# 2. Pooling  #\n> 池化操作是什么？\n>\n> 池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。\n\n池化分为平均值池化和最大值池化，下面会一一介绍。\n## 2.1 Average Pooling ##\n- 平均值池化可以被表示为\n\n![figure1.5](https://mic-jasontang.github.io/imgs/figure1.5.png)\n\n- 平均值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.5.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n\n## 2.2 Max Pooling ##\n- 最大值池化可以被表示为\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.png)\n\n- 最大值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n# 3. 3D-Conv  #\n3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。\n\n![figure1.6](https://mic-jasontang.github.io/imgs/conv_3d.jpg)\n\n输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。\n\n为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/ \"博客原地址\")\n\n\n<iframe \n    width=\"100%\" \n    height=\"100%\" \n    src=\"http://cs231n.github.io/assets/conv-demo/index.html\"\n    frameborder=\"0\" \n    allowfullscreen>\n</iframe>\n\n# 4. LeNet-5  #\n这里介绍下LeNet-5模型，为了理解前面讲述的各种模型\n\n# Bibliography #","slug":"深度学习中的卷积和池化","published":1,"updated":"2018-06-21T14:19:05.945Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv74t0006kou8sic7bdyo","content":"<h1 id=\"1-Convolution\"><a href=\"#1-Convolution\" class=\"headerlink\" title=\"1. Convolution\"></a>1. Convolution</h1><blockquote>\n<p>卷积是什么？</p>\n<p>卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。</p>\n</blockquote>\n<ul>\n<li>卷积动画演示<ul>\n<li>卷积核<br><img src=\"https://mic-jasontang.github.io/imgs/conv-kernel.png\" alt=\"卷积动画演示\"></li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_no_padding.gif\" alt=\"卷积动画演示\"></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_padding.gif\" alt=\"卷积动画演示\"></p>\n<blockquote>\n<p>在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。</p>\n<ul>\n<li>卷积的目的</li>\n</ul>\n<p>卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。</p>\n</blockquote>\n<hr>\n<p><strong>卷积分为许多种，下面将会一一介绍。</strong></p>\n<ul>\n<li>符号约定</li>\n</ul>\n<blockquote>\n<p>i: 输入大小表示为i*i</p>\n<p>k: 卷积核大小表示为k*k</p>\n<p>s: 步长</p>\n<p>p: 填充</p>\n<p>o: 输出表示为o*o</p>\n</blockquote>\n<h2 id=\"1-1-unit-strides\"><a href=\"#1-1-unit-strides\" class=\"headerlink\" title=\"1.1 unit strides\"></a>1.1 unit strides</h2><p>卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。</p>\n<h3 id=\"1-1-1-No-zero-padding-unit-strides\"><a href=\"#1-1-1-No-zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.1  No zero padding, unit strides\"></a>1.1.1  No zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.png\" alt=\"figure2.1\"></p>\n<p>无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1_2.png\" alt=\"figure2.1\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.gif\" alt=\"figure2.1\"></p>\n<h3 id=\"1-1-2-Zero-padding-unit-strides\"><a href=\"#1-1-2-Zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.2 Zero padding, unit strides\"></a>1.1.2 Zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.png\" alt=\"figure2.2\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2_2.png\" alt=\"figure2.2\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.gif\" alt=\"figure2.2\"></p>\n<h4 id=\"1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\"><a href=\"#1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\" class=\"headerlink\" title=\"1.1.2.1 Zero padding, unit strides - Half(Same) padding\"></a>1.1.2.1 Zero padding, unit strides - Half(Same) padding</h4><p>这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.png\" alt=\"figure2.3\"></p>\n<p>有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3_2.png\" alt=\"figure2.3\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.gif\" alt=\"figure2.3\"></p>\n<h4 id=\"1-1-2-2-Zero-padding-unit-strides-Full-padding\"><a href=\"#1-1-2-2-Zero-padding-unit-strides-Full-padding\" class=\"headerlink\" title=\"1.1.2.2 Zero padding, unit strides - Full padding\"></a>1.1.2.2 Zero padding, unit strides - Full padding</h4><p>卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.png\" alt=\"figure2.4\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4_2.png\" alt=\"figure2.4\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.gif\" alt=\"figure2.4\"></p>\n<h2 id=\"1-2-Non-unit-strides\"><a href=\"#1-2-Non-unit-strides\" class=\"headerlink\" title=\"1.2 Non-unit strides\"></a>1.2 Non-unit strides</h2><p>接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。</p>\n<h3 id=\"1-2-1-No-zero-padding-non-unit-strides\"><a href=\"#1-2-1-No-zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.1 No zero padding, non-unit strides\"></a>1.2.1 No zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.png\" alt=\"figure2.5\"></p>\n<p>无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.5_2.png\" alt=\"figure2.5\"></p>\n<p>其中向下取整是为了避免(i-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.gif\" alt=\"figure2.5\"></p>\n<h3 id=\"1-2-2-Zero-padding-non-unit-strides\"><a href=\"#1-2-2-Zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.2 Zero padding, non-unit strides\"></a>1.2.2 Zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.png\" alt=\"figure2.6\"></p>\n<p>有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6_2.png\" alt=\"figure2.6\"></p>\n<p>其中向下取整是为了避免(i+2p-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.gif\" alt=\"figure2.6\"></p>\n<h2 id=\"1-3-Convolution-as-a-matrix-operation\"><a href=\"#1-3-Convolution-as-a-matrix-operation\" class=\"headerlink\" title=\"1.3 Convolution as a matrix operation\"></a>1.3 Convolution as a matrix operation</h2><p>卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：</p>\n<p>1.1.1中的图被表示为如下形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png\" alt=\"figure2.6\"></p>\n<p>矩阵表示的形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix.png\" alt=\"figure2.6\"></p>\n<p>我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。</p>\n<h1 id=\"2-Pooling\"><a href=\"#2-Pooling\" class=\"headerlink\" title=\"2. Pooling\"></a>2. Pooling</h1><blockquote>\n<p>池化操作是什么？</p>\n<p>池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。</p>\n</blockquote>\n<p>池化分为平均值池化和最大值池化，下面会一一介绍。</p>\n<h2 id=\"2-1-Average-Pooling\"><a href=\"#2-1-Average-Pooling\" class=\"headerlink\" title=\"2.1 Average Pooling\"></a>2.1 Average Pooling</h2><ul>\n<li>平均值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.png\" alt=\"figure1.5\"></p>\n<ul>\n<li>平均值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h2 id=\"2-2-Max-Pooling\"><a href=\"#2-2-Max-Pooling\" class=\"headerlink\" title=\"2.2 Max Pooling\"></a>2.2 Max Pooling</h2><ul>\n<li>最大值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.png\" alt=\"figure1.6\"></p>\n<ul>\n<li>最大值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h1 id=\"3-3D-Conv\"><a href=\"#3-3D-Conv\" class=\"headerlink\" title=\"3. 3D-Conv\"></a>3. 3D-Conv</h1><p>3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_3d.jpg\" alt=\"figure1.6\"></p>\n<p>输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。</p>\n<p>为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。<a href=\"http://cs231n.github.io/convolutional-networks/\" title=\"博客原地址\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/convolutional-networks/</a></p>\n<iframe width=\"100%\" height=\"100%\" src=\"http://cs231n.github.io/assets/conv-demo/index.html\" frameborder=\"0\" allowfullscreen><br></iframe>\n\n<h1 id=\"4-LeNet-5\"><a href=\"#4-LeNet-5\" class=\"headerlink\" title=\"4. LeNet-5\"></a>4. LeNet-5</h1><p>这里介绍下LeNet-5模型，为了理解前面讲述的各种模型</p>\n<h1 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h1>","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"","more":"<h1 id=\"1-Convolution\"><a href=\"#1-Convolution\" class=\"headerlink\" title=\"1. Convolution\"></a>1. Convolution</h1><blockquote>\n<p>卷积是什么？</p>\n<p>卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。</p>\n</blockquote>\n<ul>\n<li>卷积动画演示<ul>\n<li>卷积核<br><img src=\"https://mic-jasontang.github.io/imgs/conv-kernel.png\" alt=\"卷积动画演示\"></li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_no_padding.gif\" alt=\"卷积动画演示\"></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_padding.gif\" alt=\"卷积动画演示\"></p>\n<blockquote>\n<p>在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。</p>\n<ul>\n<li>卷积的目的</li>\n</ul>\n<p>卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。</p>\n</blockquote>\n<hr>\n<p><strong>卷积分为许多种，下面将会一一介绍。</strong></p>\n<ul>\n<li>符号约定</li>\n</ul>\n<blockquote>\n<p>i: 输入大小表示为i*i</p>\n<p>k: 卷积核大小表示为k*k</p>\n<p>s: 步长</p>\n<p>p: 填充</p>\n<p>o: 输出表示为o*o</p>\n</blockquote>\n<h2 id=\"1-1-unit-strides\"><a href=\"#1-1-unit-strides\" class=\"headerlink\" title=\"1.1 unit strides\"></a>1.1 unit strides</h2><p>卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。</p>\n<h3 id=\"1-1-1-No-zero-padding-unit-strides\"><a href=\"#1-1-1-No-zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.1  No zero padding, unit strides\"></a>1.1.1  No zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.png\" alt=\"figure2.1\"></p>\n<p>无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1_2.png\" alt=\"figure2.1\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.gif\" alt=\"figure2.1\"></p>\n<h3 id=\"1-1-2-Zero-padding-unit-strides\"><a href=\"#1-1-2-Zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.2 Zero padding, unit strides\"></a>1.1.2 Zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.png\" alt=\"figure2.2\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2_2.png\" alt=\"figure2.2\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.gif\" alt=\"figure2.2\"></p>\n<h4 id=\"1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\"><a href=\"#1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\" class=\"headerlink\" title=\"1.1.2.1 Zero padding, unit strides - Half(Same) padding\"></a>1.1.2.1 Zero padding, unit strides - Half(Same) padding</h4><p>这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.png\" alt=\"figure2.3\"></p>\n<p>有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3_2.png\" alt=\"figure2.3\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.gif\" alt=\"figure2.3\"></p>\n<h4 id=\"1-1-2-2-Zero-padding-unit-strides-Full-padding\"><a href=\"#1-1-2-2-Zero-padding-unit-strides-Full-padding\" class=\"headerlink\" title=\"1.1.2.2 Zero padding, unit strides - Full padding\"></a>1.1.2.2 Zero padding, unit strides - Full padding</h4><p>卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.png\" alt=\"figure2.4\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4_2.png\" alt=\"figure2.4\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.gif\" alt=\"figure2.4\"></p>\n<h2 id=\"1-2-Non-unit-strides\"><a href=\"#1-2-Non-unit-strides\" class=\"headerlink\" title=\"1.2 Non-unit strides\"></a>1.2 Non-unit strides</h2><p>接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。</p>\n<h3 id=\"1-2-1-No-zero-padding-non-unit-strides\"><a href=\"#1-2-1-No-zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.1 No zero padding, non-unit strides\"></a>1.2.1 No zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.png\" alt=\"figure2.5\"></p>\n<p>无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.5_2.png\" alt=\"figure2.5\"></p>\n<p>其中向下取整是为了避免(i-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.gif\" alt=\"figure2.5\"></p>\n<h3 id=\"1-2-2-Zero-padding-non-unit-strides\"><a href=\"#1-2-2-Zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.2 Zero padding, non-unit strides\"></a>1.2.2 Zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.png\" alt=\"figure2.6\"></p>\n<p>有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6_2.png\" alt=\"figure2.6\"></p>\n<p>其中向下取整是为了避免(i+2p-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.gif\" alt=\"figure2.6\"></p>\n<h2 id=\"1-3-Convolution-as-a-matrix-operation\"><a href=\"#1-3-Convolution-as-a-matrix-operation\" class=\"headerlink\" title=\"1.3 Convolution as a matrix operation\"></a>1.3 Convolution as a matrix operation</h2><p>卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：</p>\n<p>1.1.1中的图被表示为如下形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png\" alt=\"figure2.6\"></p>\n<p>矩阵表示的形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix.png\" alt=\"figure2.6\"></p>\n<p>我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。</p>\n<h1 id=\"2-Pooling\"><a href=\"#2-Pooling\" class=\"headerlink\" title=\"2. Pooling\"></a>2. Pooling</h1><blockquote>\n<p>池化操作是什么？</p>\n<p>池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。</p>\n</blockquote>\n<p>池化分为平均值池化和最大值池化，下面会一一介绍。</p>\n<h2 id=\"2-1-Average-Pooling\"><a href=\"#2-1-Average-Pooling\" class=\"headerlink\" title=\"2.1 Average Pooling\"></a>2.1 Average Pooling</h2><ul>\n<li>平均值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.png\" alt=\"figure1.5\"></p>\n<ul>\n<li>平均值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h2 id=\"2-2-Max-Pooling\"><a href=\"#2-2-Max-Pooling\" class=\"headerlink\" title=\"2.2 Max Pooling\"></a>2.2 Max Pooling</h2><ul>\n<li>最大值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.png\" alt=\"figure1.6\"></p>\n<ul>\n<li>最大值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h1 id=\"3-3D-Conv\"><a href=\"#3-3D-Conv\" class=\"headerlink\" title=\"3. 3D-Conv\"></a>3. 3D-Conv</h1><p>3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_3d.jpg\" alt=\"figure1.6\"></p>\n<p>输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。</p>\n<p>为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。<a href=\"http://cs231n.github.io/convolutional-networks/\" title=\"博客原地址\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/convolutional-networks/</a></p>\n<iframe width=\"100%\" height=\"100%\" src=\"http://cs231n.github.io/assets/conv-demo/index.html\" frameborder=\"0\" allowfullscreen><br></iframe>\n\n<h1 id=\"4-LeNet-5\"><a href=\"#4-LeNet-5\" class=\"headerlink\" title=\"4. LeNet-5\"></a>4. LeNet-5</h1><p>这里介绍下LeNet-5模型，为了理解前面讲述的各种模型</p>\n<h1 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h1>"},{"title":"粗糙表面计算机模拟(matlab+3dsMax仿真)","date":"2018-03-23T10:29:30.000Z","_content":"本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.\n采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。\n<!-- more -->\n# 代码 #\n  \n\tclear\n\tclc\n\t\n\tN=128;%生成大小\n\tdelta=0.05;%表面均方根粗糙度\n\tbetax=30;%x方向的相关长度\n\tbetay=30;%y方向的相关长度\n\tC=1;%功率谱密度\n\t\n\tL=0.05;\n\tdx=L/N;dy=dx;\n\tNN=-N/2:N/2-1;\n\t[Nx,Ny]=meshgrid(NN,NN);\n\ttaux=dx.*Nx;tauy=dy.*Ny;\n\t\n\t%%生成具有指定自相关函数的粗糙表面\n\teta=randn(N,N);%高斯分布白噪声\n\tA=fft2(eta);%傅里叶变换\n\tR=zeros(N,N);\n\tR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\n\tGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\n\tH=(Gz/C).^0.5;%传递函数\n\tZ=H.*A;%表面高度的傅里叶变换\n\tz=ifft2(Z);%表面高度分布\n\tz = abs(z) * 1800;\n\tfigure(1);\n\tmesh(z);\n\t% surf2stl('surf_roughness.stl',1,1,z) % 生成模型\n\ttitle('rough surface');\n\taxis square\n\t\n# 实验结果 #\nmatlab仿真结果128*128\n\n![matlab仿真结果128*128](https://mic-jasontang.github.io/imgs/surf_roughness_128.png)\n\nmatlab仿真结果256*256\n\n![matlab仿真结果256*256](https://mic-jasontang.github.io/imgs/surf_roughness_256.png)\n# 仿真结果 #\n采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。\n\n3dMax仿真结果\n\n![3dMax仿真结果](https://mic-jasontang.github.io/imgs/50mm-106-68.png)\n\n3dMax仿真之后进行试验的结果\n\n![3dMax仿真之后进行试验的结果](https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png)\n# 参考文献 #\n[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.\n","source":"_posts/粗糙表面计算机模拟-matlab-3dsMax仿真.md","raw":"---\ntitle: 粗糙表面计算机模拟(matlab+3dsMax仿真)\ndate: 2018-03-23 18:29:30\ncategories:\n- 研磨&粗糙度\n- 研磨表面仿真\ntags: \n- 表面粗糙度\n- matlab模拟粗糙度\n- 3dsMax仿真\n---\n本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.\n采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。\n<!-- more -->\n# 代码 #\n  \n\tclear\n\tclc\n\t\n\tN=128;%生成大小\n\tdelta=0.05;%表面均方根粗糙度\n\tbetax=30;%x方向的相关长度\n\tbetay=30;%y方向的相关长度\n\tC=1;%功率谱密度\n\t\n\tL=0.05;\n\tdx=L/N;dy=dx;\n\tNN=-N/2:N/2-1;\n\t[Nx,Ny]=meshgrid(NN,NN);\n\ttaux=dx.*Nx;tauy=dy.*Ny;\n\t\n\t%%生成具有指定自相关函数的粗糙表面\n\teta=randn(N,N);%高斯分布白噪声\n\tA=fft2(eta);%傅里叶变换\n\tR=zeros(N,N);\n\tR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\n\tGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\n\tH=(Gz/C).^0.5;%传递函数\n\tZ=H.*A;%表面高度的傅里叶变换\n\tz=ifft2(Z);%表面高度分布\n\tz = abs(z) * 1800;\n\tfigure(1);\n\tmesh(z);\n\t% surf2stl('surf_roughness.stl',1,1,z) % 生成模型\n\ttitle('rough surface');\n\taxis square\n\t\n# 实验结果 #\nmatlab仿真结果128*128\n\n![matlab仿真结果128*128](https://mic-jasontang.github.io/imgs/surf_roughness_128.png)\n\nmatlab仿真结果256*256\n\n![matlab仿真结果256*256](https://mic-jasontang.github.io/imgs/surf_roughness_256.png)\n# 仿真结果 #\n采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。\n\n3dMax仿真结果\n\n![3dMax仿真结果](https://mic-jasontang.github.io/imgs/50mm-106-68.png)\n\n3dMax仿真之后进行试验的结果\n\n![3dMax仿真之后进行试验的结果](https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png)\n# 参考文献 #\n[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.\n","slug":"粗糙表面计算机模拟-matlab-3dsMax仿真","published":1,"updated":"2018-06-21T13:01:35.687Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv74u0007kou8ywsfztw1","content":"<p>本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.<br>采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。<br><a id=\"more\"></a></p>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><pre><code>clear\nclc\n\nN=128;%生成大小\ndelta=0.05;%表面均方根粗糙度\nbetax=30;%x方向的相关长度\nbetay=30;%y方向的相关长度\nC=1;%功率谱密度\n\nL=0.05;\ndx=L/N;dy=dx;\nNN=-N/2:N/2-1;\n[Nx,Ny]=meshgrid(NN,NN);\ntaux=dx.*Nx;tauy=dy.*Ny;\n\n%%生成具有指定自相关函数的粗糙表面\neta=randn(N,N);%高斯分布白噪声\nA=fft2(eta);%傅里叶变换\nR=zeros(N,N);\nR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\nGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\nH=(Gz/C).^0.5;%传递函数\nZ=H.*A;%表面高度的傅里叶变换\nz=ifft2(Z);%表面高度分布\nz = abs(z) * 1800;\nfigure(1);\nmesh(z);\n% surf2stl(&apos;surf_roughness.stl&apos;,1,1,z) % 生成模型\ntitle(&apos;rough surface&apos;);\naxis square\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>matlab仿真结果128*128</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_128.png\" alt=\"matlab仿真结果128*128\"></p>\n<p>matlab仿真结果256*256</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_256.png\" alt=\"matlab仿真结果256*256\"></p>\n<h1 id=\"仿真结果\"><a href=\"#仿真结果\" class=\"headerlink\" title=\"仿真结果\"></a>仿真结果</h1><p>采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。</p>\n<p>3dMax仿真结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68.png\" alt=\"3dMax仿真结果\"></p>\n<p>3dMax仿真之后进行试验的结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png\" alt=\"3dMax仿真之后进行试验的结果\"></p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.</p>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"<p>本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.<br>采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。<br>","more":"</p>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><pre><code>clear\nclc\n\nN=128;%生成大小\ndelta=0.05;%表面均方根粗糙度\nbetax=30;%x方向的相关长度\nbetay=30;%y方向的相关长度\nC=1;%功率谱密度\n\nL=0.05;\ndx=L/N;dy=dx;\nNN=-N/2:N/2-1;\n[Nx,Ny]=meshgrid(NN,NN);\ntaux=dx.*Nx;tauy=dy.*Ny;\n\n%%生成具有指定自相关函数的粗糙表面\neta=randn(N,N);%高斯分布白噪声\nA=fft2(eta);%傅里叶变换\nR=zeros(N,N);\nR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\nGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\nH=(Gz/C).^0.5;%传递函数\nZ=H.*A;%表面高度的傅里叶变换\nz=ifft2(Z);%表面高度分布\nz = abs(z) * 1800;\nfigure(1);\nmesh(z);\n% surf2stl(&apos;surf_roughness.stl&apos;,1,1,z) % 生成模型\ntitle(&apos;rough surface&apos;);\naxis square\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>matlab仿真结果128*128</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_128.png\" alt=\"matlab仿真结果128*128\"></p>\n<p>matlab仿真结果256*256</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_256.png\" alt=\"matlab仿真结果256*256\"></p>\n<h1 id=\"仿真结果\"><a href=\"#仿真结果\" class=\"headerlink\" title=\"仿真结果\"></a>仿真结果</h1><p>采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。</p>\n<p>3dMax仿真结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68.png\" alt=\"3dMax仿真结果\"></p>\n<p>3dMax仿真之后进行试验的结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png\" alt=\"3dMax仿真之后进行试验的结果\"></p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.</p>"},{"title":"直方图均衡化图片","date":"2018-03-20T10:31:55.000Z","_content":"\n# 直方图均衡化 #\n\n\n- 1.实验原理\n> 用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。\n \n\n\n\n\n- 2.实验步骤\n>\t1、对图像进行灰度统计，求灰度统计直方图。\n>\t\n>\t2、对灰度统计直方图进行归一化。\n>\t\n>\t3、求累积分布函数，求累积分布直方图。\n>\t\n>\t4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].\n>\t\n>\t5、确定映射对应关系，根据映射关系计算均衡化直方图。\n\n<!-- more -->\n\n- 3.代码\n\n\n> 代码采用python2.0实现   \n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/10/15 18:49\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : histequa.py\n\t# @ToDo    : 直方图均衡化(8bit)\n\t\n\t\n\tfrom PIL import Image\n\timport matplotlib as mpl\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\t\n\tmpl.rcParams['font.sans-serif'] = \"SimHei\"\n\tmpl.rcParams['axes.unicode_minus'] = False\n\t\n\t\n\tdef image2vector():\n\t    return np.array(Image.open(\"images/lena512.bmp\", \"r\").convert(\"L\"))\n\t\n\t\n\tdef equalization(data):\n\t    # 得到图像的高度、宽度\n\t    h = data.shape[0]\n\t    w = data.shape[1]\n\t    # 灰度数组\n\t    grayArr = np.zeros(255)\n\t    # 进行像素灰度统计\n\t    for i in range(h):\n\t        for j in range(w):\n\t            grayArr[data[i][j]] += 1\n\t    print grayArr.shape, grayArr.max()\n\t    # 归一化\n\t    idx = 0\n\t    for item in grayArr:\n\t        grayArr[idx] = item / (h * w)\n\t        idx += 1\n\t    # print grayArr\n\t    cdf = np.zeros(grayArr.shape)\n\t    sum = 0\n\t    # 计算灰度分布密度\n\t    # print cdf.shape\n\t    for i in range(len(grayArr)):\n\t        sum += grayArr[i]\n\t        cdf[i] = sum\n\t    L = 255\n\t    # print cdf\n\t    # 累计分布取整\n\t    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n\t    # print indexArr\n\t    # 对灰度值进行映射（均衡化）\n\t    for i in range(h):\n\t        for j in range(w):\n\t            data[i, j] = indexArr[data[i, j]]\n\t    return grayArr, cdf, data\n\t\n\t\n\tif __name__ == '__main__':\n\t    data = image2vector()\n\t    # print data.shape\n\t    plt.figure(figsize=(7, 9))\n\t    plt.subplot(321)\n\t    plt.title(u\"原始图像\")\n\t    plt.imshow(data, cmap='gray')\n\t    plt.subplot(322)\n\t    plt.title(u\"原始灰度\")\n\t    plt.hist(data.flatten(), normed=True, bins=256)\n\t    srcGray, cdf, equlArr = equalization(data)\n\t    plt.subplot(323)\n\t    plt.title(u\"归一化直方图\")\n\t    plt.hist(srcGray, 255)\n\t    plt.subplot(324)\n\t    plt.title(u\"累积直方图\")\n\t    plt.hist(cdf, 255)\n\t    plt.subplot(325)\n\t    plt.title(u\"均衡化图像\")\n\t    plt.imshow(equlArr, cmap='gray')\n\t    plt.subplot(326)\n\t    plt.title(u\"均衡化的直方图\")\n\t    plt.hist(equlArr.flatten(), normed=True, bins=256)\n\t    # print equlArr\n\t    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n\t    plt.show()\n\n- 4.实验结果\n![实验结果](https://mic-jasontang.github.io/imgs/histequa.png)\n\n\n- 5.实验总结\n> 在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。\n","source":"_posts/直方图均衡化图片.md","raw":"---\ntitle: 直方图均衡化图片\ndate: 2018-03-20 18:31:55\ncategories:\n- 图像处理\n- 图像增强\ntags: \n- python \n- 图像处理 \n- 直方图均衡化\n---\n\n# 直方图均衡化 #\n\n\n- 1.实验原理\n> 用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。\n \n\n\n\n\n- 2.实验步骤\n>\t1、对图像进行灰度统计，求灰度统计直方图。\n>\t\n>\t2、对灰度统计直方图进行归一化。\n>\t\n>\t3、求累积分布函数，求累积分布直方图。\n>\t\n>\t4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].\n>\t\n>\t5、确定映射对应关系，根据映射关系计算均衡化直方图。\n\n<!-- more -->\n\n- 3.代码\n\n\n> 代码采用python2.0实现   \n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/10/15 18:49\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : histequa.py\n\t# @ToDo    : 直方图均衡化(8bit)\n\t\n\t\n\tfrom PIL import Image\n\timport matplotlib as mpl\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\t\n\tmpl.rcParams['font.sans-serif'] = \"SimHei\"\n\tmpl.rcParams['axes.unicode_minus'] = False\n\t\n\t\n\tdef image2vector():\n\t    return np.array(Image.open(\"images/lena512.bmp\", \"r\").convert(\"L\"))\n\t\n\t\n\tdef equalization(data):\n\t    # 得到图像的高度、宽度\n\t    h = data.shape[0]\n\t    w = data.shape[1]\n\t    # 灰度数组\n\t    grayArr = np.zeros(255)\n\t    # 进行像素灰度统计\n\t    for i in range(h):\n\t        for j in range(w):\n\t            grayArr[data[i][j]] += 1\n\t    print grayArr.shape, grayArr.max()\n\t    # 归一化\n\t    idx = 0\n\t    for item in grayArr:\n\t        grayArr[idx] = item / (h * w)\n\t        idx += 1\n\t    # print grayArr\n\t    cdf = np.zeros(grayArr.shape)\n\t    sum = 0\n\t    # 计算灰度分布密度\n\t    # print cdf.shape\n\t    for i in range(len(grayArr)):\n\t        sum += grayArr[i]\n\t        cdf[i] = sum\n\t    L = 255\n\t    # print cdf\n\t    # 累计分布取整\n\t    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n\t    # print indexArr\n\t    # 对灰度值进行映射（均衡化）\n\t    for i in range(h):\n\t        for j in range(w):\n\t            data[i, j] = indexArr[data[i, j]]\n\t    return grayArr, cdf, data\n\t\n\t\n\tif __name__ == '__main__':\n\t    data = image2vector()\n\t    # print data.shape\n\t    plt.figure(figsize=(7, 9))\n\t    plt.subplot(321)\n\t    plt.title(u\"原始图像\")\n\t    plt.imshow(data, cmap='gray')\n\t    plt.subplot(322)\n\t    plt.title(u\"原始灰度\")\n\t    plt.hist(data.flatten(), normed=True, bins=256)\n\t    srcGray, cdf, equlArr = equalization(data)\n\t    plt.subplot(323)\n\t    plt.title(u\"归一化直方图\")\n\t    plt.hist(srcGray, 255)\n\t    plt.subplot(324)\n\t    plt.title(u\"累积直方图\")\n\t    plt.hist(cdf, 255)\n\t    plt.subplot(325)\n\t    plt.title(u\"均衡化图像\")\n\t    plt.imshow(equlArr, cmap='gray')\n\t    plt.subplot(326)\n\t    plt.title(u\"均衡化的直方图\")\n\t    plt.hist(equlArr.flatten(), normed=True, bins=256)\n\t    # print equlArr\n\t    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n\t    plt.show()\n\n- 4.实验结果\n![实验结果](https://mic-jasontang.github.io/imgs/histequa.png)\n\n\n- 5.实验总结\n> 在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。\n","slug":"直方图均衡化图片","published":1,"updated":"2018-06-21T13:01:35.686Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv74v0008kou8akmos414","content":"<h1 id=\"直方图均衡化\"><a href=\"#直方图均衡化\" class=\"headerlink\" title=\"直方图均衡化\"></a>直方图均衡化</h1><ul>\n<li>1.实验原理<blockquote>\n<p>用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。</p>\n</blockquote>\n</li>\n</ul>\n<ul>\n<li>2.实验步骤<blockquote>\n<p>   1、对图像进行灰度统计，求灰度统计直方图。</p>\n<p>   2、对灰度统计直方图进行归一化。</p>\n<p>   3、求累积分布函数，求累积分布直方图。</p>\n<p>   4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].</p>\n<p>   5、确定映射对应关系，根据映射关系计算均衡化直方图。</p>\n</blockquote>\n</li>\n</ul>\n<a id=\"more\"></a>\n<ul>\n<li>3.代码</li>\n</ul>\n<blockquote>\n<p>代码采用python2.0实现   </p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/10/15 18:49\n# @Author  : Jasontang\n# @Site    : \n# @File    : histequa.py\n# @ToDo    : 直方图均衡化(8bit)\n\n\nfrom PIL import Image\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmpl.rcParams[&apos;font.sans-serif&apos;] = &quot;SimHei&quot;\nmpl.rcParams[&apos;axes.unicode_minus&apos;] = False\n\n\ndef image2vector():\n    return np.array(Image.open(&quot;images/lena512.bmp&quot;, &quot;r&quot;).convert(&quot;L&quot;))\n\n\ndef equalization(data):\n    # 得到图像的高度、宽度\n    h = data.shape[0]\n    w = data.shape[1]\n    # 灰度数组\n    grayArr = np.zeros(255)\n    # 进行像素灰度统计\n    for i in range(h):\n        for j in range(w):\n            grayArr[data[i][j]] += 1\n    print grayArr.shape, grayArr.max()\n    # 归一化\n    idx = 0\n    for item in grayArr:\n        grayArr[idx] = item / (h * w)\n        idx += 1\n    # print grayArr\n    cdf = np.zeros(grayArr.shape)\n    sum = 0\n    # 计算灰度分布密度\n    # print cdf.shape\n    for i in range(len(grayArr)):\n        sum += grayArr[i]\n        cdf[i] = sum\n    L = 255\n    # print cdf\n    # 累计分布取整\n    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n    # print indexArr\n    # 对灰度值进行映射（均衡化）\n    for i in range(h):\n        for j in range(w):\n            data[i, j] = indexArr[data[i, j]]\n    return grayArr, cdf, data\n\n\nif __name__ == &apos;__main__&apos;:\n    data = image2vector()\n    # print data.shape\n    plt.figure(figsize=(7, 9))\n    plt.subplot(321)\n    plt.title(u&quot;原始图像&quot;)\n    plt.imshow(data, cmap=&apos;gray&apos;)\n    plt.subplot(322)\n    plt.title(u&quot;原始灰度&quot;)\n    plt.hist(data.flatten(), normed=True, bins=256)\n    srcGray, cdf, equlArr = equalization(data)\n    plt.subplot(323)\n    plt.title(u&quot;归一化直方图&quot;)\n    plt.hist(srcGray, 255)\n    plt.subplot(324)\n    plt.title(u&quot;累积直方图&quot;)\n    plt.hist(cdf, 255)\n    plt.subplot(325)\n    plt.title(u&quot;均衡化图像&quot;)\n    plt.imshow(equlArr, cmap=&apos;gray&apos;)\n    plt.subplot(326)\n    plt.title(u&quot;均衡化的直方图&quot;)\n    plt.hist(equlArr.flatten(), normed=True, bins=256)\n    # print equlArr\n    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n    plt.show()\n</code></pre><ul>\n<li>4.实验结果<br><img src=\"https://mic-jasontang.github.io/imgs/histequa.png\" alt=\"实验结果\"></li>\n</ul>\n<ul>\n<li>5.实验总结<blockquote>\n<p>在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。</p>\n</blockquote>\n</li>\n</ul>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"<h1 id=\"直方图均衡化\"><a href=\"#直方图均衡化\" class=\"headerlink\" title=\"直方图均衡化\"></a>直方图均衡化</h1><ul>\n<li>1.实验原理<blockquote>\n<p>用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。</p>\n</blockquote>\n</li>\n</ul>\n<ul>\n<li>2.实验步骤<blockquote>\n<p>   1、对图像进行灰度统计，求灰度统计直方图。</p>\n<p>   2、对灰度统计直方图进行归一化。</p>\n<p>   3、求累积分布函数，求累积分布直方图。</p>\n<p>   4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].</p>\n<p>   5、确定映射对应关系，根据映射关系计算均衡化直方图。</p>\n</blockquote>\n</li>\n</ul>","more":"<ul>\n<li>3.代码</li>\n</ul>\n<blockquote>\n<p>代码采用python2.0实现   </p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/10/15 18:49\n# @Author  : Jasontang\n# @Site    : \n# @File    : histequa.py\n# @ToDo    : 直方图均衡化(8bit)\n\n\nfrom PIL import Image\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmpl.rcParams[&apos;font.sans-serif&apos;] = &quot;SimHei&quot;\nmpl.rcParams[&apos;axes.unicode_minus&apos;] = False\n\n\ndef image2vector():\n    return np.array(Image.open(&quot;images/lena512.bmp&quot;, &quot;r&quot;).convert(&quot;L&quot;))\n\n\ndef equalization(data):\n    # 得到图像的高度、宽度\n    h = data.shape[0]\n    w = data.shape[1]\n    # 灰度数组\n    grayArr = np.zeros(255)\n    # 进行像素灰度统计\n    for i in range(h):\n        for j in range(w):\n            grayArr[data[i][j]] += 1\n    print grayArr.shape, grayArr.max()\n    # 归一化\n    idx = 0\n    for item in grayArr:\n        grayArr[idx] = item / (h * w)\n        idx += 1\n    # print grayArr\n    cdf = np.zeros(grayArr.shape)\n    sum = 0\n    # 计算灰度分布密度\n    # print cdf.shape\n    for i in range(len(grayArr)):\n        sum += grayArr[i]\n        cdf[i] = sum\n    L = 255\n    # print cdf\n    # 累计分布取整\n    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n    # print indexArr\n    # 对灰度值进行映射（均衡化）\n    for i in range(h):\n        for j in range(w):\n            data[i, j] = indexArr[data[i, j]]\n    return grayArr, cdf, data\n\n\nif __name__ == &apos;__main__&apos;:\n    data = image2vector()\n    # print data.shape\n    plt.figure(figsize=(7, 9))\n    plt.subplot(321)\n    plt.title(u&quot;原始图像&quot;)\n    plt.imshow(data, cmap=&apos;gray&apos;)\n    plt.subplot(322)\n    plt.title(u&quot;原始灰度&quot;)\n    plt.hist(data.flatten(), normed=True, bins=256)\n    srcGray, cdf, equlArr = equalization(data)\n    plt.subplot(323)\n    plt.title(u&quot;归一化直方图&quot;)\n    plt.hist(srcGray, 255)\n    plt.subplot(324)\n    plt.title(u&quot;累积直方图&quot;)\n    plt.hist(cdf, 255)\n    plt.subplot(325)\n    plt.title(u&quot;均衡化图像&quot;)\n    plt.imshow(equlArr, cmap=&apos;gray&apos;)\n    plt.subplot(326)\n    plt.title(u&quot;均衡化的直方图&quot;)\n    plt.hist(equlArr.flatten(), normed=True, bins=256)\n    # print equlArr\n    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n    plt.show()\n</code></pre><ul>\n<li>4.实验结果<br><img src=\"https://mic-jasontang.github.io/imgs/histequa.png\" alt=\"实验结果\"></li>\n</ul>\n<ul>\n<li>5.实验总结<blockquote>\n<p>在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。</p>\n</blockquote>\n</li>\n</ul>"},{"title":"Mnist手写数字体识别(tensorflow)","date":"2018-03-20T13:35:05.000Z","_content":"# Tensorflow #\n\n\n> 首先，简单的说下，tensorflow的基本架构。\n>使用 TensorFlow, 你必须明白 TensorFlow:\n\n- 使用图 (graph) 来表示计算任务.\n- 在被称之为 会话 (Session) 的上下文 (context) 中执行图.\n- 使用 tensor 表示数据.\n- 通过 变量 (Variable) 维护状态.\n- 使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.\n\n<!--more -->\n\n# Tensor #\n\n> TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].\n\n> 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.\n\n> Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。\n\n# Graph #\n\n\n> TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.\n\n\n\n> 例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。\n\n![](https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif)\n> 构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.\n\n\n> TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.\n\n# Session #\n> 当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。\n\n\n\n> tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.\n\n\n# 数据载体 #\n> Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：\n   \n\t# 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\n\tstate = tf.Variable(0, name=\"counter\")\n\n\t# 创建一个常量，其值为1，没有指定数据类型（dtype）\n\tone = tf.constant(1)\n\n\n\n> 针对上面的变量和常量，看看Tensorflow里面的函数定义：\n>\n    class Variable(object):　\n\tdef __init__(self,\n\t\tinitial_value=None,\n\t\ttrainable=True,\n\t\tcollections=None,\n\t\tvalidate_shape=True,\n\t\tcaching_device=None,\n\t\tname=None,\n\t\tvariable_def=None,\n\t\tdtype=None,\n\t\texpected_shape=None,\n\t\timport_scope=None)：\n\n>\n\tdef constant(value, dtype=None, shape=None, name=\"Const\", verify_shape=False)：\n\n> 从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。\n\n> 还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。\n>\n\tinput1 = tf.placeholder(tf.types.float32)\n\tinput2 = tf.placeholder(tf.types.float32)\n\toutput = tf.mul(input1, input2)\n>\t\n\twith tf.Session() as sess:\n\t  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n>\t\n\t# 输出:\n\t# [array([ 14.], dtype=float32)]\n\n> 占位符的定义原型，也是一个函数：\n>\n\tdef placeholder(dtype, shape=None, name=None)：\n\n\n\n> 到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：\n\n\n----------\n\n> main.py驱动程序\n \n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 20:41\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : main.py\n\t# @ToDo    : 驱动程序\n\t\n\timport _thread\n\t\n\tfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\t\n\t\n\tif __name__ == '__main__':\n\t    _thread.start_new_thread(mnist_train.main, (None,))\n\t    _thread.start_new_thread(mnist_eval.main, (None,))\n\t\n\t    # 这个不能删除，当做主线程\n\t    while 1:\n\t        pass\n\n> mnist_inference.py计算前向传播的过程及定义了神经网络的参数\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/20 19:43\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_inference.py\n\t# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\t\n\t\n\timport tensorflow as tf\n\t\n\t# 定义神经网络结构相关的参数\n\tINPUT_NODE = 784\n\tOUTPUT_NODE = 10\n\tLAYER1_NODE = 500\n\t\n\t\n\t# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\n\tdef get_weight_variable(shape, regularizer):\n\t    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t\n\t    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t    # 自定义集合\n\t    if regularizer:\n\t        tf.add_to_collection(\"losses\", regularizer(weights))\n\t    return weights\n\t\n\t\n\t# 前向传播过程\n\tdef inference(input_tensor, regularizer):\n\t    # 声明第一层神经网络的变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer1\"):\n\t        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\t\n\t    # 声明第二层圣经网络变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer2\"):\n\t        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer2 = tf.matmul(layer1, weights) + biases\n\t    # 返回最后前向传播的结果\n\t    return layer2\n\n> mnist_train.py定义了神经网络的训练过程\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:08\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_train.py\n\t# @ToDo    : 定义了神经网络的训练过程\n\t\n\timport os\n\t\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\t\n\t# 配置神经网络的参数\n\tBATCH_SIZE = 100\n\tLEARNING_REATE_BASE = 0.8\n\tLEARNING_RATE_DECAY = 0.99\n\tREGULARAZTION_RATE = 0.0001\n\tTRAING_STEPS = 2000\n\tMOVING_AVERAGE_DECAY = 0.99\n\t# 模型保存的路径和文件名\n\tMODEL_SAVE_PATH = \"./model/\"\n\tMODEL_NAME = \"model.ckpt\"\n\t\n\t\n\tdef train(mnist):\n\t    # 定义输入输出placeholder\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n\t    y = mnist_inference.inference(x, regularizer)\n\t    global_step = tf.Variable(0, trainable=False)\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n\t    variables_average_op = variable_averages.apply(tf.trainable_variables())\n\t    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n\t    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\t    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n\t    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n\t                                              global_step,\n\t                                              mnist.train.num_examples / BATCH_SIZE,\n\t                                              LEARNING_RATE_DECAY)\n\t    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\t\n\t    with tf.control_dependencies([train_step, variables_average_op]):\n\t        train_op = tf.no_op(name=\"train\")\n\t\n\t    # 初始化持久化类\n\t    saver = tf.train.Saver()\n\t    with tf.Session() as sess:\n\t        tf.global_variables_initializer().run()\n\t\n\t        for i in range(TRAING_STEPS):\n\t            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n\t            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\t\n\t            if i % 1000 == 0:\n\t                print(\"After %d training step(s), loss on training batch is %g.\" % (i, loss_value))\n\t\n\t                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    train(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n> mnist_eval.py测试过程\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:32\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_eval.py\n\t# @ToDo    : 测试过程\n\t\n\t\n\timport time\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\timport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\t\n\t# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\n\tEVAL_INTERVAL_SECS = 10\n\t\n\t\n\tdef evaluate(mnist):\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    validate_feed = {x: mnist.validation.images,\n\t                     y_: mnist.validation.labels}\n\t\n\t    y = mnist_inference.inference(x, None)\n\t\n\t    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n\t    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n\t    variables_to_restore = variable_averages.variables_to_restore()\n\t    saver = tf.train.Saver(variables_to_restore)\n\t\n\t    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n\t    stop_count = 0\n\t    while True:\n\t        with tf.Session() as sess:\n\t            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n\t            # 停止条件 #\n\t            stop_count += EVAL_INTERVAL_SECS\n\t            if stop_count == mnist_train.TRAING_STEPS:\n\t                return\n\t            # 停止条件 #\n\t            if ckpt and ckpt.model_checkpoint_path:\n\t                saver.restore(sess, ckpt.model_checkpoint_path)\n\t                # 通过文件名得到模型保存时迭代的轮数\n\t                # 输出./model/model.ckpt-29001\n\t                print(ckpt.model_checkpoint_path)\n\t                global_step = ckpt.model_checkpoint_path.split(\"/\")[-1].split(\"-\")[-1]\n\t                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n\t                print(\"After %s training step(s), validation accuracy is %g\" % (global_step, accuracy_score))\n\t            else:\n\t                print(\"No checkpoint file found\")\n\t                return\n\t        time.sleep(EVAL_INTERVAL_SECS)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    evaluate(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n# 参考文章 #\n[https://www.cnblogs.com/shihuc/p/6648130.html](https://www.cnblogs.com/shihuc/p/6648130.html \"Tensorflow之基于MNIST手写识别的入门介绍\")\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Mnist手写数字体识别-tensorflow.md","raw":"---\ntitle: Mnist手写数字体识别(tensorflow)\ndate: 2018-03-20 21:35:05\ncategories:\n- tensorflow学习\n- tensorflow-Demo\ntags: \n- Mnist\n- tensorflow\n---\n# Tensorflow #\n\n\n> 首先，简单的说下，tensorflow的基本架构。\n>使用 TensorFlow, 你必须明白 TensorFlow:\n\n- 使用图 (graph) 来表示计算任务.\n- 在被称之为 会话 (Session) 的上下文 (context) 中执行图.\n- 使用 tensor 表示数据.\n- 通过 变量 (Variable) 维护状态.\n- 使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.\n\n<!--more -->\n\n# Tensor #\n\n> TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].\n\n> 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.\n\n> Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。\n\n# Graph #\n\n\n> TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.\n\n\n\n> 例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。\n\n![](https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif)\n> 构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.\n\n\n> TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.\n\n# Session #\n> 当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。\n\n\n\n> tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.\n\n\n# 数据载体 #\n> Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：\n   \n\t# 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\n\tstate = tf.Variable(0, name=\"counter\")\n\n\t# 创建一个常量，其值为1，没有指定数据类型（dtype）\n\tone = tf.constant(1)\n\n\n\n> 针对上面的变量和常量，看看Tensorflow里面的函数定义：\n>\n    class Variable(object):　\n\tdef __init__(self,\n\t\tinitial_value=None,\n\t\ttrainable=True,\n\t\tcollections=None,\n\t\tvalidate_shape=True,\n\t\tcaching_device=None,\n\t\tname=None,\n\t\tvariable_def=None,\n\t\tdtype=None,\n\t\texpected_shape=None,\n\t\timport_scope=None)：\n\n>\n\tdef constant(value, dtype=None, shape=None, name=\"Const\", verify_shape=False)：\n\n> 从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。\n\n> 还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。\n>\n\tinput1 = tf.placeholder(tf.types.float32)\n\tinput2 = tf.placeholder(tf.types.float32)\n\toutput = tf.mul(input1, input2)\n>\t\n\twith tf.Session() as sess:\n\t  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n>\t\n\t# 输出:\n\t# [array([ 14.], dtype=float32)]\n\n> 占位符的定义原型，也是一个函数：\n>\n\tdef placeholder(dtype, shape=None, name=None)：\n\n\n\n> 到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：\n\n\n----------\n\n> main.py驱动程序\n \n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 20:41\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : main.py\n\t# @ToDo    : 驱动程序\n\t\n\timport _thread\n\t\n\tfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\t\n\t\n\tif __name__ == '__main__':\n\t    _thread.start_new_thread(mnist_train.main, (None,))\n\t    _thread.start_new_thread(mnist_eval.main, (None,))\n\t\n\t    # 这个不能删除，当做主线程\n\t    while 1:\n\t        pass\n\n> mnist_inference.py计算前向传播的过程及定义了神经网络的参数\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/20 19:43\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_inference.py\n\t# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\t\n\t\n\timport tensorflow as tf\n\t\n\t# 定义神经网络结构相关的参数\n\tINPUT_NODE = 784\n\tOUTPUT_NODE = 10\n\tLAYER1_NODE = 500\n\t\n\t\n\t# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\n\tdef get_weight_variable(shape, regularizer):\n\t    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t\n\t    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t    # 自定义集合\n\t    if regularizer:\n\t        tf.add_to_collection(\"losses\", regularizer(weights))\n\t    return weights\n\t\n\t\n\t# 前向传播过程\n\tdef inference(input_tensor, regularizer):\n\t    # 声明第一层神经网络的变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer1\"):\n\t        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\t\n\t    # 声明第二层圣经网络变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer2\"):\n\t        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer2 = tf.matmul(layer1, weights) + biases\n\t    # 返回最后前向传播的结果\n\t    return layer2\n\n> mnist_train.py定义了神经网络的训练过程\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:08\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_train.py\n\t# @ToDo    : 定义了神经网络的训练过程\n\t\n\timport os\n\t\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\t\n\t# 配置神经网络的参数\n\tBATCH_SIZE = 100\n\tLEARNING_REATE_BASE = 0.8\n\tLEARNING_RATE_DECAY = 0.99\n\tREGULARAZTION_RATE = 0.0001\n\tTRAING_STEPS = 2000\n\tMOVING_AVERAGE_DECAY = 0.99\n\t# 模型保存的路径和文件名\n\tMODEL_SAVE_PATH = \"./model/\"\n\tMODEL_NAME = \"model.ckpt\"\n\t\n\t\n\tdef train(mnist):\n\t    # 定义输入输出placeholder\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n\t    y = mnist_inference.inference(x, regularizer)\n\t    global_step = tf.Variable(0, trainable=False)\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n\t    variables_average_op = variable_averages.apply(tf.trainable_variables())\n\t    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n\t    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\t    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n\t    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n\t                                              global_step,\n\t                                              mnist.train.num_examples / BATCH_SIZE,\n\t                                              LEARNING_RATE_DECAY)\n\t    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\t\n\t    with tf.control_dependencies([train_step, variables_average_op]):\n\t        train_op = tf.no_op(name=\"train\")\n\t\n\t    # 初始化持久化类\n\t    saver = tf.train.Saver()\n\t    with tf.Session() as sess:\n\t        tf.global_variables_initializer().run()\n\t\n\t        for i in range(TRAING_STEPS):\n\t            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n\t            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\t\n\t            if i % 1000 == 0:\n\t                print(\"After %d training step(s), loss on training batch is %g.\" % (i, loss_value))\n\t\n\t                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    train(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n> mnist_eval.py测试过程\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:32\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_eval.py\n\t# @ToDo    : 测试过程\n\t\n\t\n\timport time\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\timport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\t\n\t# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\n\tEVAL_INTERVAL_SECS = 10\n\t\n\t\n\tdef evaluate(mnist):\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    validate_feed = {x: mnist.validation.images,\n\t                     y_: mnist.validation.labels}\n\t\n\t    y = mnist_inference.inference(x, None)\n\t\n\t    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n\t    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n\t    variables_to_restore = variable_averages.variables_to_restore()\n\t    saver = tf.train.Saver(variables_to_restore)\n\t\n\t    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n\t    stop_count = 0\n\t    while True:\n\t        with tf.Session() as sess:\n\t            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n\t            # 停止条件 #\n\t            stop_count += EVAL_INTERVAL_SECS\n\t            if stop_count == mnist_train.TRAING_STEPS:\n\t                return\n\t            # 停止条件 #\n\t            if ckpt and ckpt.model_checkpoint_path:\n\t                saver.restore(sess, ckpt.model_checkpoint_path)\n\t                # 通过文件名得到模型保存时迭代的轮数\n\t                # 输出./model/model.ckpt-29001\n\t                print(ckpt.model_checkpoint_path)\n\t                global_step = ckpt.model_checkpoint_path.split(\"/\")[-1].split(\"-\")[-1]\n\t                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n\t                print(\"After %s training step(s), validation accuracy is %g\" % (global_step, accuracy_score))\n\t            else:\n\t                print(\"No checkpoint file found\")\n\t                return\n\t        time.sleep(EVAL_INTERVAL_SECS)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    evaluate(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n# 参考文章 #\n[https://www.cnblogs.com/shihuc/p/6648130.html](https://www.cnblogs.com/shihuc/p/6648130.html \"Tensorflow之基于MNIST手写识别的入门介绍\")\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Mnist手写数字体识别-tensorflow","published":1,"updated":"2018-03-23T10:21:49.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv778001rkou8gpbfz5gz","content":"<h1 id=\"Tensorflow\"><a href=\"#Tensorflow\" class=\"headerlink\" title=\"Tensorflow\"></a>Tensorflow</h1><blockquote>\n<p>首先，简单的说下，tensorflow的基本架构。<br>使用 TensorFlow, 你必须明白 TensorFlow:</p>\n</blockquote>\n<ul>\n<li>使用图 (graph) 来表示计算任务.</li>\n<li>在被称之为 会话 (Session) 的上下文 (context) 中执行图.</li>\n<li>使用 tensor 表示数据.</li>\n<li>通过 变量 (Variable) 维护状态.</li>\n<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h1><blockquote>\n<p>TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].</p>\n</blockquote>\n<blockquote>\n<p>一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.</p>\n</blockquote>\n<blockquote>\n<p>Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。</p>\n</blockquote>\n<h1 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h1><blockquote>\n<p>TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.</p>\n</blockquote>\n<blockquote>\n<p>例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。</p>\n</blockquote>\n<p><img src=\"https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif\" alt=\"\"></p>\n<blockquote>\n<p>构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.</p>\n</blockquote>\n<blockquote>\n<p>TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.</p>\n</blockquote>\n<h1 id=\"Session\"><a href=\"#Session\" class=\"headerlink\" title=\"Session\"></a>Session</h1><blockquote>\n<p>当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。</p>\n</blockquote>\n<blockquote>\n<p>tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.</p>\n</blockquote>\n<h1 id=\"数据载体\"><a href=\"#数据载体\" class=\"headerlink\" title=\"数据载体\"></a>数据载体</h1><blockquote>\n<p>Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：</p>\n</blockquote>\n<pre><code># 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\nstate = tf.Variable(0, name=&quot;counter&quot;)\n\n# 创建一个常量，其值为1，没有指定数据类型（dtype）\none = tf.constant(1)\n</code></pre><blockquote>\n<p>针对上面的变量和常量，看看Tensorflow里面的函数定义：</p>\n</blockquote>\n<pre><code>class Variable(object):　\ndef __init__(self,\n    initial_value=None,\n    trainable=True,\n    collections=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    expected_shape=None,\n    import_scope=None)：\n</code></pre><blockquote>\n</blockquote>\n<pre><code>def constant(value, dtype=None, shape=None, name=&quot;Const&quot;, verify_shape=False)：\n</code></pre><blockquote>\n<p>从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。</p>\n</blockquote>\n<blockquote>\n<p>还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。</p>\n</blockquote>\n<pre><code>input1 = tf.placeholder(tf.types.float32)\ninput2 = tf.placeholder(tf.types.float32)\noutput = tf.mul(input1, input2)\n</code></pre><blockquote>\n<pre><code>with tf.Session() as sess:\n  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n</code></pre></blockquote>\n<pre><code># 输出:\n# [array([ 14.], dtype=float32)]\n</code></pre><blockquote>\n<p>占位符的定义原型，也是一个函数：</p>\n</blockquote>\n<pre><code>def placeholder(dtype, shape=None, name=None)：\n</code></pre><blockquote>\n<p>到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：</p>\n</blockquote>\n<hr>\n<blockquote>\n<p>main.py驱动程序</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 20:41\n# @Author  : Jasontang\n# @Site    : \n# @File    : main.py\n# @ToDo    : 驱动程序\n\nimport _thread\n\nfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\n\nif __name__ == &apos;__main__&apos;:\n    _thread.start_new_thread(mnist_train.main, (None,))\n    _thread.start_new_thread(mnist_eval.main, (None,))\n\n    # 这个不能删除，当做主线程\n    while 1:\n        pass\n</code></pre><blockquote>\n<p>mnist_inference.py计算前向传播的过程及定义了神经网络的参数</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/20 19:43\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_inference.py\n# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\n\nimport tensorflow as tf\n\n# 定义神经网络结构相关的参数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\n\n# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(&quot;weights&quot;, shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\n    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n    # 自定义集合\n    if regularizer:\n        tf.add_to_collection(&quot;losses&quot;, regularizer(weights))\n    return weights\n\n\n# 前向传播过程\ndef inference(input_tensor, regularizer):\n    # 声明第一层神经网络的变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer1&quot;):\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    # 声明第二层圣经网络变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer2&quot;):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n    # 返回最后前向传播的结果\n    return layer2\n</code></pre><blockquote>\n<p>mnist_train.py定义了神经网络的训练过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:08\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_train.py\n# @ToDo    : 定义了神经网络的训练过程\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\n# 配置神经网络的参数\nBATCH_SIZE = 100\nLEARNING_REATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAING_STEPS = 2000\nMOVING_AVERAGE_DECAY = 0.99\n# 模型保存的路径和文件名\nMODEL_SAVE_PATH = &quot;./model/&quot;\nMODEL_NAME = &quot;model.ckpt&quot;\n\n\ndef train(mnist):\n    # 定义输入输出placeholder\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_average_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(&quot;losses&quot;))\n    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                              global_step,\n                                              mnist.train.num_examples / BATCH_SIZE,\n                                              LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\n    with tf.control_dependencies([train_step, variables_average_op]):\n        train_op = tf.no_op(name=&quot;train&quot;)\n\n    # 初始化持久化类\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\n            if i % 1000 == 0:\n                print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (i, loss_value))\n\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    train(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><blockquote>\n<p>mnist_eval.py测试过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:32\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_eval.py\n# @ToDo    : 测试过程\n\n\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\nimport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\n# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\nEVAL_INTERVAL_SECS = 10\n\n\ndef evaluate(mnist):\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    validate_feed = {x: mnist.validation.images,\n                     y_: mnist.validation.labels}\n\n    y = mnist_inference.inference(x, None)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n    stop_count = 0\n    while True:\n        with tf.Session() as sess:\n            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n            # 停止条件 #\n            stop_count += EVAL_INTERVAL_SECS\n            if stop_count == mnist_train.TRAING_STEPS:\n                return\n            # 停止条件 #\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                # 通过文件名得到模型保存时迭代的轮数\n                # 输出./model/model.ckpt-29001\n                print(ckpt.model_checkpoint_path)\n                global_step = ckpt.model_checkpoint_path.split(&quot;/&quot;)[-1].split(&quot;-&quot;)[-1]\n                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n                print(&quot;After %s training step(s), validation accuracy is %g&quot; % (global_step, accuracy_score))\n            else:\n                print(&quot;No checkpoint file found&quot;)\n                return\n        time.sleep(EVAL_INTERVAL_SECS)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    evaluate(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><h1 id=\"参考文章\"><a href=\"#参考文章\" class=\"headerlink\" title=\"参考文章\"></a>参考文章</h1><p><a href=\"https://www.cnblogs.com/shihuc/p/6648130.html\" title=\"Tensorflow之基于MNIST手写识别的入门介绍\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/shihuc/p/6648130.html</a></p>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"<h1 id=\"Tensorflow\"><a href=\"#Tensorflow\" class=\"headerlink\" title=\"Tensorflow\"></a>Tensorflow</h1><blockquote>\n<p>首先，简单的说下，tensorflow的基本架构。<br>使用 TensorFlow, 你必须明白 TensorFlow:</p>\n</blockquote>\n<ul>\n<li>使用图 (graph) 来表示计算任务.</li>\n<li>在被称之为 会话 (Session) 的上下文 (context) 中执行图.</li>\n<li>使用 tensor 表示数据.</li>\n<li>通过 变量 (Variable) 维护状态.</li>\n<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>\n</ul>","more":"<h1 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h1><blockquote>\n<p>TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].</p>\n</blockquote>\n<blockquote>\n<p>一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.</p>\n</blockquote>\n<blockquote>\n<p>Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。</p>\n</blockquote>\n<h1 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h1><blockquote>\n<p>TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.</p>\n</blockquote>\n<blockquote>\n<p>例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。</p>\n</blockquote>\n<p><img src=\"https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif\" alt=\"\"></p>\n<blockquote>\n<p>构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.</p>\n</blockquote>\n<blockquote>\n<p>TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.</p>\n</blockquote>\n<h1 id=\"Session\"><a href=\"#Session\" class=\"headerlink\" title=\"Session\"></a>Session</h1><blockquote>\n<p>当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。</p>\n</blockquote>\n<blockquote>\n<p>tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.</p>\n</blockquote>\n<h1 id=\"数据载体\"><a href=\"#数据载体\" class=\"headerlink\" title=\"数据载体\"></a>数据载体</h1><blockquote>\n<p>Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：</p>\n</blockquote>\n<pre><code># 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\nstate = tf.Variable(0, name=&quot;counter&quot;)\n\n# 创建一个常量，其值为1，没有指定数据类型（dtype）\none = tf.constant(1)\n</code></pre><blockquote>\n<p>针对上面的变量和常量，看看Tensorflow里面的函数定义：</p>\n</blockquote>\n<pre><code>class Variable(object):　\ndef __init__(self,\n    initial_value=None,\n    trainable=True,\n    collections=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    expected_shape=None,\n    import_scope=None)：\n</code></pre><blockquote>\n</blockquote>\n<pre><code>def constant(value, dtype=None, shape=None, name=&quot;Const&quot;, verify_shape=False)：\n</code></pre><blockquote>\n<p>从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。</p>\n</blockquote>\n<blockquote>\n<p>还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。</p>\n</blockquote>\n<pre><code>input1 = tf.placeholder(tf.types.float32)\ninput2 = tf.placeholder(tf.types.float32)\noutput = tf.mul(input1, input2)\n</code></pre><blockquote>\n<pre><code>with tf.Session() as sess:\n  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n</code></pre></blockquote>\n<pre><code># 输出:\n# [array([ 14.], dtype=float32)]\n</code></pre><blockquote>\n<p>占位符的定义原型，也是一个函数：</p>\n</blockquote>\n<pre><code>def placeholder(dtype, shape=None, name=None)：\n</code></pre><blockquote>\n<p>到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：</p>\n</blockquote>\n<hr>\n<blockquote>\n<p>main.py驱动程序</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 20:41\n# @Author  : Jasontang\n# @Site    : \n# @File    : main.py\n# @ToDo    : 驱动程序\n\nimport _thread\n\nfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\n\nif __name__ == &apos;__main__&apos;:\n    _thread.start_new_thread(mnist_train.main, (None,))\n    _thread.start_new_thread(mnist_eval.main, (None,))\n\n    # 这个不能删除，当做主线程\n    while 1:\n        pass\n</code></pre><blockquote>\n<p>mnist_inference.py计算前向传播的过程及定义了神经网络的参数</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/20 19:43\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_inference.py\n# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\n\nimport tensorflow as tf\n\n# 定义神经网络结构相关的参数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\n\n# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(&quot;weights&quot;, shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\n    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n    # 自定义集合\n    if regularizer:\n        tf.add_to_collection(&quot;losses&quot;, regularizer(weights))\n    return weights\n\n\n# 前向传播过程\ndef inference(input_tensor, regularizer):\n    # 声明第一层神经网络的变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer1&quot;):\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    # 声明第二层圣经网络变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer2&quot;):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n    # 返回最后前向传播的结果\n    return layer2\n</code></pre><blockquote>\n<p>mnist_train.py定义了神经网络的训练过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:08\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_train.py\n# @ToDo    : 定义了神经网络的训练过程\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\n# 配置神经网络的参数\nBATCH_SIZE = 100\nLEARNING_REATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAING_STEPS = 2000\nMOVING_AVERAGE_DECAY = 0.99\n# 模型保存的路径和文件名\nMODEL_SAVE_PATH = &quot;./model/&quot;\nMODEL_NAME = &quot;model.ckpt&quot;\n\n\ndef train(mnist):\n    # 定义输入输出placeholder\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_average_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(&quot;losses&quot;))\n    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                              global_step,\n                                              mnist.train.num_examples / BATCH_SIZE,\n                                              LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\n    with tf.control_dependencies([train_step, variables_average_op]):\n        train_op = tf.no_op(name=&quot;train&quot;)\n\n    # 初始化持久化类\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\n            if i % 1000 == 0:\n                print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (i, loss_value))\n\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    train(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><blockquote>\n<p>mnist_eval.py测试过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:32\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_eval.py\n# @ToDo    : 测试过程\n\n\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\nimport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\n# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\nEVAL_INTERVAL_SECS = 10\n\n\ndef evaluate(mnist):\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    validate_feed = {x: mnist.validation.images,\n                     y_: mnist.validation.labels}\n\n    y = mnist_inference.inference(x, None)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n    stop_count = 0\n    while True:\n        with tf.Session() as sess:\n            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n            # 停止条件 #\n            stop_count += EVAL_INTERVAL_SECS\n            if stop_count == mnist_train.TRAING_STEPS:\n                return\n            # 停止条件 #\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                # 通过文件名得到模型保存时迭代的轮数\n                # 输出./model/model.ckpt-29001\n                print(ckpt.model_checkpoint_path)\n                global_step = ckpt.model_checkpoint_path.split(&quot;/&quot;)[-1].split(&quot;-&quot;)[-1]\n                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n                print(&quot;After %s training step(s), validation accuracy is %g&quot; % (global_step, accuracy_score))\n            else:\n                print(&quot;No checkpoint file found&quot;)\n                return\n        time.sleep(EVAL_INTERVAL_SECS)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    evaluate(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><h1 id=\"参考文章\"><a href=\"#参考文章\" class=\"headerlink\" title=\"参考文章\"></a>参考文章</h1><p><a href=\"https://www.cnblogs.com/shihuc/p/6648130.html\" title=\"Tensorflow之基于MNIST手写识别的入门介绍\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/shihuc/p/6648130.html</a></p>"},{"title":"关于k阶矩的理解","date":"2018-03-22T13:44:18.000Z","_content":"\n# k阶原点矩、2阶矩、3阶矩该怎么理解？ #\n\n下面使用语言描述和代码来讲解。\n<!-- more -->\n\n> 阶矩是用来描述随机变量的概率分布的特性.\n\n> 一阶矩指的是随机变量的平均值,即期望值\n> \n> 二阶矩指的是随机变量的方差\n> \n> 三阶矩指的是随机变量的偏度\n> \n> 四阶矩指的是随机变量的峰度\n \n> 因此通过计算矩,则可以得出随机变量的分布形状\n\n# 代码实现 #\n使用Python2.0实现\n\n    import numpy as np\n\tfrom scipy import stats\n\t\n\t\n\tdef calc_statistics(x):\n\tn = x.shape[0]   #样本个数\n\n\t# 手动计算\n\tm = 0\n\tm2 = 0\n\tm3 = 0\n\tm4 = 0\n\tfor t in x:\n\t\tm += t\n\t\tm2 += t*t\n\t\tm3 += t**3\n\t\tm4 += t**4\n\tm /= n\n\tm2 /= n\n\tm3 /= n\n\tm4 /= n\n\n\tmu = m    # 一阶矩\n\tsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\n\tskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\n\tkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3\t# 四阶矩（峰度）\n\tprint \"手动计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\n\t# 使用系统函数验证\n\tmu = np.mean(x, axis=0)\n\tsigma = np.std(x, axis=0)\n\tskew = stats.skew(x)\n\tkurtosis = stats.kurtosis(x)\n\treturn mu, sigma, skew, kurtosis\n\n\tif __name__ == '__main__':\n\td = np.random.randn(10000)\n\tprint d\n\tprint d.shape\n\tmu, sigma, skew, kurtosis = calc_statistics(d)\n\tprint \"函数库计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\t\n执行结果:\n\t\n\t\n> [-0.42751577  0.36230961  0.37899409 ...,  0.09176115 -1.38955563\n -0.57570736]\n\n\n> (10000L,)\n\n\n> 手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n\n\n> 函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n","source":"_posts/关于k阶矩的理解.md","raw":"---\ntitle: 关于k阶矩的理解\ndate: 2018-03-22 21:44:18\ncategories:\n- 数学基础\n- 随机过程\ntags:\n- k阶矩\n- 随机过程\n- 偏度\n- 峰度\n---\n\n# k阶原点矩、2阶矩、3阶矩该怎么理解？ #\n\n下面使用语言描述和代码来讲解。\n<!-- more -->\n\n> 阶矩是用来描述随机变量的概率分布的特性.\n\n> 一阶矩指的是随机变量的平均值,即期望值\n> \n> 二阶矩指的是随机变量的方差\n> \n> 三阶矩指的是随机变量的偏度\n> \n> 四阶矩指的是随机变量的峰度\n \n> 因此通过计算矩,则可以得出随机变量的分布形状\n\n# 代码实现 #\n使用Python2.0实现\n\n    import numpy as np\n\tfrom scipy import stats\n\t\n\t\n\tdef calc_statistics(x):\n\tn = x.shape[0]   #样本个数\n\n\t# 手动计算\n\tm = 0\n\tm2 = 0\n\tm3 = 0\n\tm4 = 0\n\tfor t in x:\n\t\tm += t\n\t\tm2 += t*t\n\t\tm3 += t**3\n\t\tm4 += t**4\n\tm /= n\n\tm2 /= n\n\tm3 /= n\n\tm4 /= n\n\n\tmu = m    # 一阶矩\n\tsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\n\tskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\n\tkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3\t# 四阶矩（峰度）\n\tprint \"手动计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\n\t# 使用系统函数验证\n\tmu = np.mean(x, axis=0)\n\tsigma = np.std(x, axis=0)\n\tskew = stats.skew(x)\n\tkurtosis = stats.kurtosis(x)\n\treturn mu, sigma, skew, kurtosis\n\n\tif __name__ == '__main__':\n\td = np.random.randn(10000)\n\tprint d\n\tprint d.shape\n\tmu, sigma, skew, kurtosis = calc_statistics(d)\n\tprint \"函数库计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\t\n执行结果:\n\t\n\t\n> [-0.42751577  0.36230961  0.37899409 ...,  0.09176115 -1.38955563\n -0.57570736]\n\n\n> (10000L,)\n\n\n> 手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n\n\n> 函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n","slug":"关于k阶矩的理解","published":1,"updated":"2018-03-23T10:21:49.158Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjiomv779001skou8efbgg7h4","content":"<h1 id=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"><a href=\"#k阶原点矩、2阶矩、3阶矩该怎么理解？\" class=\"headerlink\" title=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"></a>k阶原点矩、2阶矩、3阶矩该怎么理解？</h1><p>下面使用语言描述和代码来讲解。<br><a id=\"more\"></a></p>\n<blockquote>\n<p>阶矩是用来描述随机变量的概率分布的特性.</p>\n</blockquote>\n<blockquote>\n<p>一阶矩指的是随机变量的平均值,即期望值</p>\n<p>二阶矩指的是随机变量的方差</p>\n<p>三阶矩指的是随机变量的偏度</p>\n<p>四阶矩指的是随机变量的峰度</p>\n</blockquote>\n<blockquote>\n<p>因此通过计算矩,则可以得出随机变量的分布形状</p>\n</blockquote>\n<h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h1><p>使用Python2.0实现</p>\n<pre><code>import numpy as np\nfrom scipy import stats\n\n\ndef calc_statistics(x):\nn = x.shape[0]   #样本个数\n\n# 手动计算\nm = 0\nm2 = 0\nm3 = 0\nm4 = 0\nfor t in x:\n    m += t\n    m2 += t*t\n    m3 += t**3\n    m4 += t**4\nm /= n\nm2 /= n\nm3 /= n\nm4 /= n\n\nmu = m    # 一阶矩\nsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\nskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\nkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3    # 四阶矩（峰度）\nprint &quot;手动计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n\n# 使用系统函数验证\nmu = np.mean(x, axis=0)\nsigma = np.std(x, axis=0)\nskew = stats.skew(x)\nkurtosis = stats.kurtosis(x)\nreturn mu, sigma, skew, kurtosis\n\nif __name__ == &apos;__main__&apos;:\nd = np.random.randn(10000)\nprint d\nprint d.shape\nmu, sigma, skew, kurtosis = calc_statistics(d)\nprint &quot;函数库计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n</code></pre><p>执行结果:</p>\n<blockquote>\n<p>[-0.42751577  0.36230961  0.37899409 …,  0.09176115 -1.38955563<br> -0.57570736]</p>\n</blockquote>\n<blockquote>\n<p>(10000L,)</p>\n</blockquote>\n<blockquote>\n<p>手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>\n<blockquote>\n<p>函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>\n","site":{"data":{"hint":{"new":{"selector":[".menu-reading"]}},"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import __helloworld__","subtitle":">>> Hello World","link":null}],"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}}},"excerpt":"<h1 id=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"><a href=\"#k阶原点矩、2阶矩、3阶矩该怎么理解？\" class=\"headerlink\" title=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"></a>k阶原点矩、2阶矩、3阶矩该怎么理解？</h1><p>下面使用语言描述和代码来讲解。<br>","more":"</p>\n<blockquote>\n<p>阶矩是用来描述随机变量的概率分布的特性.</p>\n</blockquote>\n<blockquote>\n<p>一阶矩指的是随机变量的平均值,即期望值</p>\n<p>二阶矩指的是随机变量的方差</p>\n<p>三阶矩指的是随机变量的偏度</p>\n<p>四阶矩指的是随机变量的峰度</p>\n</blockquote>\n<blockquote>\n<p>因此通过计算矩,则可以得出随机变量的分布形状</p>\n</blockquote>\n<h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h1><p>使用Python2.0实现</p>\n<pre><code>import numpy as np\nfrom scipy import stats\n\n\ndef calc_statistics(x):\nn = x.shape[0]   #样本个数\n\n# 手动计算\nm = 0\nm2 = 0\nm3 = 0\nm4 = 0\nfor t in x:\n    m += t\n    m2 += t*t\n    m3 += t**3\n    m4 += t**4\nm /= n\nm2 /= n\nm3 /= n\nm4 /= n\n\nmu = m    # 一阶矩\nsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\nskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\nkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3    # 四阶矩（峰度）\nprint &quot;手动计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n\n# 使用系统函数验证\nmu = np.mean(x, axis=0)\nsigma = np.std(x, axis=0)\nskew = stats.skew(x)\nkurtosis = stats.kurtosis(x)\nreturn mu, sigma, skew, kurtosis\n\nif __name__ == &apos;__main__&apos;:\nd = np.random.randn(10000)\nprint d\nprint d.shape\nmu, sigma, skew, kurtosis = calc_statistics(d)\nprint &quot;函数库计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n</code></pre><p>执行结果:</p>\n<blockquote>\n<p>[-0.42751577  0.36230961  0.37899409 …,  0.09176115 -1.38955563<br> -0.57570736]</p>\n</blockquote>\n<blockquote>\n<p>(10000L,)</p>\n</blockquote>\n<blockquote>\n<p>手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>\n<blockquote>\n<p>函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjiomv74k0000kou8vutuiah6","category_id":"cjiomv74r0004kou8mnw47j8j","_id":"cjiomv753000mkou8wd3h6hrb"},{"post_id":"cjiomv74k0000kou8vutuiah6","category_id":"cjiomv74z000hkou8z8mk7gz8","_id":"cjiomv753000okou8kqn342cp"},{"post_id":"cjiomv74o0002kou83svzhk13","category_id":"cjiomv74w0009kou8tdg6vbkc","_id":"cjiomv754000rkou87kbk8cqm"},{"post_id":"cjiomv74o0002kou83svzhk13","category_id":"cjiomv752000lkou8a7umx1wy","_id":"cjiomv755000ukou8cjdf7s62"},{"post_id":"cjiomv74t0006kou8sic7bdyo","category_id":"cjiomv74x000bkou8h5ulvaqh","_id":"cjiomv756000wkou86lqryrn8"},{"post_id":"cjiomv74t0006kou8sic7bdyo","category_id":"cjiomv754000pkou85zt768gf","_id":"cjiomv7570010kou8u4yjbcgw"},{"post_id":"cjiomv74u0007kou8ywsfztw1","category_id":"cjiomv74y000fkou86ehsi8go","_id":"cjiomv7580013kou8lgbdttwj"},{"post_id":"cjiomv74u0007kou8ywsfztw1","category_id":"cjiomv755000skou8qvm8wy9c","_id":"cjiomv7580016kou8t2olfps9"},{"post_id":"cjiomv74v0008kou8akmos414","category_id":"cjiomv74w0009kou8tdg6vbkc","_id":"cjiomv7590018kou8qga3dn5q"},{"post_id":"cjiomv74v0008kou8akmos414","category_id":"cjiomv752000lkou8a7umx1wy","_id":"cjiomv759001akou8fx4ivn50"},{"post_id":"cjiomv778001rkou8gpbfz5gz","category_id":"cjiomv77a001tkou8zcwljtf1","_id":"cjiomv77f0023kou8qekpw95a"},{"post_id":"cjiomv778001rkou8gpbfz5gz","category_id":"cjiomv77d001zkou8ovchy63b","_id":"cjiomv77f0025kou85koz8bt3"},{"post_id":"cjiomv779001skou8efbgg7h4","category_id":"cjiomv77b001vkou8f8tp3jwf","_id":"cjiomv77g0026kou8otwccgpc"},{"post_id":"cjiomv779001skou8efbgg7h4","category_id":"cjiomv77e0021kou8fj3c346o","_id":"cjiomv77g0027kou89nipw6p5"}],"PostTag":[{"post_id":"cjiomv74k0000kou8vutuiah6","tag_id":"cjiomv74s0005kou8d648bwpt","_id":"cjiomv74y000dkou8vil5tzaq"},{"post_id":"cjiomv74k0000kou8vutuiah6","tag_id":"cjiomv74w000akou8ea69hmdl","_id":"cjiomv74y000ekou8435qabla"},{"post_id":"cjiomv74o0002kou83svzhk13","tag_id":"cjiomv74x000ckou8n98m1auc","_id":"cjiomv756000vkou8j1osg5pj"},{"post_id":"cjiomv74o0002kou83svzhk13","tag_id":"cjiomv74y000gkou84ndc7pr9","_id":"cjiomv756000xkou8tq8g99bl"},{"post_id":"cjiomv74o0002kou83svzhk13","tag_id":"cjiomv74z000ikou8jg4n4r6c","_id":"cjiomv7570011kou8mdq035l1"},{"post_id":"cjiomv74o0002kou83svzhk13","tag_id":"cjiomv750000kkou8ykq2a7gt","_id":"cjiomv7570012kou8spahaj46"},{"post_id":"cjiomv74o0002kou83svzhk13","tag_id":"cjiomv753000nkou8qcd3jce5","_id":"cjiomv7580015kou8oznajijk"},{"post_id":"cjiomv74o0002kou83svzhk13","tag_id":"cjiomv754000qkou8l8qwith4","_id":"cjiomv7590017kou87v386p2s"},{"post_id":"cjiomv74t0006kou8sic7bdyo","tag_id":"cjiomv755000tkou847sqipmd","_id":"cjiomv75a001ckou86eegwjgp"},{"post_id":"cjiomv74t0006kou8sic7bdyo","tag_id":"cjiomv757000zkou8tz44o4l7","_id":"cjiomv75a001dkou8troj3ny1"},{"post_id":"cjiomv74t0006kou8sic7bdyo","tag_id":"cjiomv7580014kou8rfl2kr81","_id":"cjiomv75b001fkou8h3t7zheu"},{"post_id":"cjiomv74t0006kou8sic7bdyo","tag_id":"cjiomv7590019kou8pbirf9nq","_id":"cjiomv75b001gkou85ls3l4np"},{"post_id":"cjiomv74u0007kou8ywsfztw1","tag_id":"cjiomv75a001bkou8v80nt17q","_id":"cjiomv75b001jkou8a8g3a3w0"},{"post_id":"cjiomv74u0007kou8ywsfztw1","tag_id":"cjiomv75a001ekou8nme4pb65","_id":"cjiomv75c001kkou8t5nxxxdu"},{"post_id":"cjiomv74u0007kou8ywsfztw1","tag_id":"cjiomv75b001hkou8p1h6unzc","_id":"cjiomv75c001mkou8isf16xwx"},{"post_id":"cjiomv74v0008kou8akmos414","tag_id":"cjiomv75b001ikou83u0gongu","_id":"cjiomv75d001okou89iiutagq"},{"post_id":"cjiomv74v0008kou8akmos414","tag_id":"cjiomv74x000ckou8n98m1auc","_id":"cjiomv75d001pkou8nz1ger81"},{"post_id":"cjiomv74v0008kou8akmos414","tag_id":"cjiomv75c001nkou8hc8kfil6","_id":"cjiomv75d001qkou8s1q08aly"},{"post_id":"cjiomv778001rkou8gpbfz5gz","tag_id":"cjiomv77b001ukou8ya8e1634","_id":"cjiomv77c001xkou8lnc7do2p"},{"post_id":"cjiomv778001rkou8gpbfz5gz","tag_id":"cjiomv7580014kou8rfl2kr81","_id":"cjiomv77d001ykou85xlu3tuh"},{"post_id":"cjiomv779001skou8efbgg7h4","tag_id":"cjiomv77c001wkou83t0tfe16","_id":"cjiomv77g0028kou8egsmy2rt"},{"post_id":"cjiomv779001skou8efbgg7h4","tag_id":"cjiomv77d0020kou862b9dkge","_id":"cjiomv77h0029kou8948b9xm8"},{"post_id":"cjiomv779001skou8efbgg7h4","tag_id":"cjiomv77e0022kou8yaj5svdv","_id":"cjiomv77h002akou8qhn18d1x"},{"post_id":"cjiomv779001skou8efbgg7h4","tag_id":"cjiomv77f0024kou8c0bpx76q","_id":"cjiomv77h002bkou8befxvzhu"}],"Tag":[{"name":"hexo","_id":"cjiomv74s0005kou8d648bwpt"},{"name":"多电脑同步","_id":"cjiomv74w000akou8ea69hmdl"},{"name":"图像处理","_id":"cjiomv74x000ckou8n98m1auc"},{"name":"卷积运算","_id":"cjiomv74y000gkou84ndc7pr9"},{"name":"guass","_id":"cjiomv74z000ikou8jg4n4r6c"},{"name":"soble","_id":"cjiomv750000kkou8ykq2a7gt"},{"name":"prewitt","_id":"cjiomv753000nkou8qcd3jce5"},{"name":"laplacian","_id":"cjiomv754000qkou8l8qwith4"},{"name":"卷积","_id":"cjiomv755000tkou847sqipmd"},{"name":"池化","_id":"cjiomv757000zkou8tz44o4l7"},{"name":"tensorflow","_id":"cjiomv7580014kou8rfl2kr81"},{"name":"LeNet-5","_id":"cjiomv7590019kou8pbirf9nq"},{"name":"表面粗糙度","_id":"cjiomv75a001bkou8v80nt17q"},{"name":"matlab模拟粗糙度","_id":"cjiomv75a001ekou8nme4pb65"},{"name":"3dsMax仿真","_id":"cjiomv75b001hkou8p1h6unzc"},{"name":"python","_id":"cjiomv75b001ikou83u0gongu"},{"name":"直方图均衡化","_id":"cjiomv75c001nkou8hc8kfil6"},{"name":"Mnist","_id":"cjiomv77b001ukou8ya8e1634"},{"name":"k阶矩","_id":"cjiomv77c001wkou83t0tfe16"},{"name":"随机过程","_id":"cjiomv77d0020kou862b9dkge"},{"name":"偏度","_id":"cjiomv77e0022kou8yaj5svdv"},{"name":"峰度","_id":"cjiomv77f0024kou8c0bpx76q"}]}}
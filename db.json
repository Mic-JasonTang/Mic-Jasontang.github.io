{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/imgs/alipay-rewardcode.jpg","path":"imgs/alipay-rewardcode.jpg","modified":1,"renderable":0},{"_id":"source/imgs/avatar.jpg","path":"imgs/avatar.jpg","modified":1,"renderable":0},{"_id":"source/imgs/conv-kernel.png","path":"imgs/conv-kernel.png","modified":1,"renderable":0},{"_id":"source/imgs/conv_3d.jpg","path":"imgs/conv_3d.jpg","modified":1,"renderable":0},{"_id":"source/imgs/conv_no_padding.gif","path":"imgs/conv_no_padding.gif","modified":1,"renderable":0},{"_id":"source/imgs/conv_padding.gif","path":"imgs/conv_padding.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.1.gif","path":"imgs/figure2.1.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.1_2.png","path":"imgs/figure2.1_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.2.gif","path":"imgs/figure2.2.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.2_2.png","path":"imgs/figure2.2_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.3.gif","path":"imgs/figure2.3.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.3_2.png","path":"imgs/figure2.3_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.4.gif","path":"imgs/figure2.4.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.4_2.png","path":"imgs/figure2.4_2.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.5.gif","path":"imgs/figure2.5.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.5_5.png","path":"imgs/figure2.5_5.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.6.gif","path":"imgs/figure2.6.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure2.6_2.png","path":"imgs/figure2.6_2.png","modified":1,"renderable":0},{"_id":"source/imgs/join_us.jpg","path":"imgs/join_us.jpg","modified":1,"renderable":0},{"_id":"source/imgs/machine-api.jpg","path":"imgs/machine-api.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"source/imgs/AI_Live.jpg","path":"imgs/AI_Live.jpg","modified":1,"renderable":0},{"_id":"source/imgs/LeNet-5.png","path":"imgs/LeNet-5.png","modified":1,"renderable":0},{"_id":"source/imgs/fc.png","path":"imgs/fc.png","modified":1,"renderable":0},{"_id":"source/imgs/figure1.5.gif","path":"imgs/figure1.5.gif","modified":1,"renderable":0},{"_id":"source/imgs/figure1.6.gif","path":"imgs/figure1.6.gif","modified":1,"renderable":0},{"_id":"source/imgs/flower.jpg","path":"imgs/flower.jpg","modified":1,"renderable":0},{"_id":"source/imgs/iclass.png","path":"imgs/iclass.png","modified":1,"renderable":0},{"_id":"source/imgs/mnist_data.png","path":"imgs/mnist_data.png","modified":1,"renderable":0},{"_id":"source/imgs/mnist_result.png","path":"imgs/mnist_result.png","modified":1,"renderable":0},{"_id":"source/imgs/mnist_resultpng.png","path":"imgs/mnist_resultpng.png","modified":1,"renderable":0},{"_id":"source/imgs/pythoner.png","path":"imgs/pythoner.png","modified":1,"renderable":0},{"_id":"source/imgs/sky.jpg","path":"imgs/sky.jpg","modified":1,"renderable":0},{"_id":"source/imgs/surf_3dmax.png","path":"imgs/surf_3dmax.png","modified":1,"renderable":0},{"_id":"source/imgs/surf_roughness_128.png","path":"imgs/surf_roughness_128.png","modified":1,"renderable":0},{"_id":"source/imgs/water.jpg","path":"imgs/water.jpg","modified":1,"renderable":0},{"_id":"source/imgs/wetchat-rewardcode.jpg","path":"imgs/wetchat-rewardcode.jpg","modified":1,"renderable":0},{"_id":"source/imgs/conv_as_matrix.png","path":"imgs/conv_as_matrix.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.1.png","path":"imgs/figure2.1.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.4.png","path":"imgs/figure2.4.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.5.png","path":"imgs/figure2.5.png","modified":1,"renderable":0},{"_id":"source/imgs/surf_roughness_256.png","path":"imgs/surf_roughness_256.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/prettify.js","path":"js/prettify.js","modified":1,"renderable":1},{"_id":"source/imgs/figure1.5.png","path":"imgs/figure1.5.png","modified":1,"renderable":0},{"_id":"source/imgs/figure1.6.png","path":"imgs/figure1.6.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.3.png","path":"imgs/figure2.3.png","modified":1,"renderable":0},{"_id":"source/imgs/figure2.6.png","path":"imgs/figure2.6.png","modified":1,"renderable":0},{"_id":"source/imgs/histequa.png","path":"imgs/histequa.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/js/jquery.min.js","path":"js/jquery.min.js","modified":1,"renderable":1},{"_id":"source/imgs/figure2.2.png","path":"imgs/figure2.2.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","path":"css/images/side-user-cover.jpg","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","path":"css/lib/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","path":"css/lib/prettify-tomorrow-night-eighties.css","modified":1,"renderable":1},{"_id":"source/imgs/algorithm/string/rota_str2.png","path":"imgs/algorithm/string/rota_str2.png","modified":1,"renderable":0},{"_id":"source/imgs/conv_as_matrix_2.png","path":"imgs/conv_as_matrix_2.png","modified":1,"renderable":0},{"_id":"source/imgs/light.jpg","path":"imgs/light.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","path":"css/lib/materialize.min.css","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/js/materialize.min.js","path":"js/materialize.min.js","modified":1,"renderable":1},{"_id":"source/imgs/algorithm/string/rota_str10.png","path":"imgs/algorithm/string/rota_str10.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str4.png","path":"imgs/algorithm/string/rota_str4.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/tree/level_tree.png","path":"imgs/algorithm/tree/level_tree.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/tree/level_tree2.png","path":"imgs/algorithm/tree/level_tree2.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/tree/level_tree3.png","path":"imgs/algorithm/tree/level_tree3.png","modified":1,"renderable":0},{"_id":"source/imgs/orange.jpg","path":"imgs/orange.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","path":"css/font/roboto/Roboto-Bold.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","path":"css/font/roboto/Roboto-Bold.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","path":"css/font/roboto/Roboto-Bold.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","path":"css/font/roboto/Roboto-Light.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","path":"css/font/roboto/Roboto-Light.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","path":"css/font/roboto/Roboto-Medium.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","path":"css/font/roboto/Roboto-Medium.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","path":"css/font/roboto/Roboto-Regular.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","path":"css/font/roboto/Roboto-Regular.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","path":"css/font/roboto/Roboto-Regular.woff2","modified":1,"renderable":1},{"_id":"source/imgs/50mm-106-68-ans.png","path":"imgs/50mm-106-68-ans.png","modified":1,"renderable":0},{"_id":"source/imgs/50mm-106-68.png","path":"imgs/50mm-106-68.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str3.png","path":"imgs/algorithm/string/rota_str3.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str6.png","path":"imgs/algorithm/string/rota_str6.png","modified":1,"renderable":0},{"_id":"source/imgs/water-circle.jpg","path":"imgs/water-circle.jpg","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","path":"css/font/font-awesome/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","path":"css/font/font-awesome/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","path":"css/font/font-awesome/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","path":"css/font/font-awesome/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","path":"css/font/roboto/Roboto-Bold.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","path":"css/font/roboto/Roboto-Light.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","path":"css/font/roboto/Roboto-Medium.woff","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","path":"css/font/roboto/Roboto-Regular.ttf","modified":1,"renderable":1},{"_id":"source/imgs/algorithm/string/rota_str7.png","path":"imgs/algorithm/string/rota_str7.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","path":"css/font/font-awesome/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","path":"css/font/roboto/Roboto-Light.ttf","modified":1,"renderable":1},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","path":"css/font/roboto/Roboto-Medium.ttf","modified":1,"renderable":1},{"_id":"source/imgs/all_day.jpg","path":"imgs/all_day.jpg","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str9.png","path":"imgs/algorithm/string/rota_str9.png","modified":1,"renderable":0},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","path":"css/font/font-awesome/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"source/imgs/algorithm/string/rota_str5.png","path":"imgs/algorithm/string/rota_str5.png","modified":1,"renderable":0},{"_id":"source/imgs/algorithm/string/rota_str8.png","path":"imgs/algorithm/string/rota_str8.png","modified":1,"renderable":0},{"_id":"source/imgs/game.png","path":"imgs/game.png","modified":1,"renderable":0},{"_id":"source/imgs/sky-night.jpg","path":"imgs/sky-night.jpg","modified":1,"renderable":0},{"_id":"source/imgs/img-cov.png","path":"imgs/img-cov.png","modified":1,"renderable":0},{"_id":"source/imgs/wall.png","path":"imgs/wall.png","modified":1,"renderable":0},{"_id":"source/imgs/winter.jpg","path":"imgs/winter.jpg","modified":1,"renderable":0},{"_id":"source/imgs/rain.png","path":"imgs/rain.png","modified":1,"renderable":0},{"_id":"source/imgs/coloreggs.jpg","path":"imgs/coloreggs.jpg","modified":1,"renderable":0}],"Cache":[{"_id":"source/.DS_Store","hash":"8bc4631da31e0019963272b07697c4f907704d86","modified":1557313062382},{"_id":"themes/raytaylorism/.gitignore","hash":"cda50c55bb8864e0d96101140b62f880f690da5e","modified":1557312625686},{"_id":"themes/raytaylorism/Gruntfile.js","hash":"f69b2e716f955c9d5a23ca1b75394098c1494858","modified":1557312625687},{"_id":"themes/raytaylorism/LICENSE","hash":"115cd028ae511ac9e3d30eb4933da38136a68513","modified":1557312625687},{"_id":"themes/raytaylorism/README.md","hash":"a195db5b7c40e99d4da1fdf252e78c496f51f48e","modified":1557312625688},{"_id":"themes/raytaylorism/_config.yml","hash":"f7d5a7363be91e7dd73ee4209baec3a9a39c24cb","modified":1557312625688},{"_id":"themes/raytaylorism/log.md","hash":"99d57a50f8f328d1a313b47bb636d0dc5656d813","modified":1557312625713},{"_id":"themes/raytaylorism/.DS_Store","hash":"96e9cac52892d20c4a93e117e1fa75c9ba27bf65","modified":1557313388423},{"_id":"source/_data/about.json","hash":"c74a6298b37f190c5cf940abf3accc463cb771a6","modified":1557312625477},{"_id":"source/_data/hint.json","hash":"7433c56bdcc76fab584670a80442200d9b605f5e","modified":1557312625478},{"_id":"source/_data/link.json","hash":"fb7c0deefc9952cf7819234fadb2ea751777e609","modified":1557312625478},{"_id":"source/_data/reading.json","hash":"4b91b2b0d7d06d3cd3bf2fbe59c51884d6304b13","modified":1557312625479},{"_id":"source/_data/slider.json","hash":"4937a7d6e13a75114a1a4a47f0b66c43d8eb85fb","modified":1557312625480},{"_id":"source/_posts/.DS_Store","hash":"8216fe5b112ae6ab97bdc8920a9f1136874fcd8e","modified":1557312625480},{"_id":"source/_posts/Hexo多电脑同步写作.md","hash":"03307813d58aa842e930a62b5e8557bfbbe856d9","modified":1557312625480},{"_id":"source/_posts/Mnist手写数字识别-tensorflow.md","hash":"a191f3b253a4f1faa9ea1dfb1a4241f5211387aa","modified":1557312625481},{"_id":"source/_posts/二叉树按层打印.md","hash":"0d5cfdc1dd0fb59ee6d9c80ca38fce8542314693","modified":1557312625482},{"_id":"source/_posts/关于k阶矩的理解.md","hash":"5381d5d5739318fd2c871d308d16a050ba37aae5","modified":1557312625482},{"_id":"source/_posts/图像常用算子进行卷积运算.md","hash":"9c09f52c640cf82e5afbbf4fc5667152963d5969","modified":1557312625483},{"_id":"source/_posts/字符串-旋转词.md","hash":"434b3786f0addacb4ccc2e8d1c1de156e1caf04a","modified":1557312625483},{"_id":"source/_posts/深度学习中的卷积和池化.md","hash":"d1552807cc71a57f2af52cd3b79a010b8c25bdd5","modified":1557312625483},{"_id":"source/_posts/直方图均衡化图片.md","hash":"d568f92365f0964a7a3673b276a19dc6373943b2","modified":1557312625484},{"_id":"source/_posts/第 1 周学习分享.md","hash":"6f77a86b584afc9c3775dce9b887a6153af8ae4d","modified":1557312625484},{"_id":"source/_posts/第 2 周学习分享.md","hash":"8a4e89e9dd5e781f40035711bda31178b92977be","modified":1557312625484},{"_id":"source/_posts/第 3 周学习分享.md","hash":"6dc1e9e8e97b3110c7e08b00770c66c6e45395f0","modified":1557312625485},{"_id":"source/_posts/第 4 周学习分享.md","hash":"31579782afab7ce4b5d8a6b5ff13f3b5a96d9feb","modified":1557312625485},{"_id":"source/_posts/第 5 周学习分享.md","hash":"5d5602985211ff7366d8a3460636bb7a7ecd68ad","modified":1557312625485},{"_id":"source/_posts/第 6 周学习分享.md","hash":"5e458e1d71159b6c1dc6fd14617a5fd1d250cd0f","modified":1557312625486},{"_id":"source/_posts/第 7 周学习分享.md","hash":"8d905ba0936bf8b0f54d759fb9c3ff2e23a5efeb","modified":1557312625486},{"_id":"source/_posts/粗糙表面计算机模拟.md","hash":"acc52e1ece0fbb35e33875bb16291871b46f4a32","modified":1557312625486},{"_id":"source/about/index.md","hash":"fa416d307e7d2e4f0162c58a0d6ffe8a40e28ee8","modified":1557312625487},{"_id":"source/reading/index.md","hash":"8f8179d7dac09b88bfc085465350a753e9eebded","modified":1557312625655},{"_id":"source/imgs/alipay-rewardcode.jpg","hash":"7093a60c54438b347cb13d79b7a34c99b0d6a4e3","modified":1557312625522},{"_id":"source/imgs/avatar.jpg","hash":"ad4c9872fa0b5c143a8778d95437452e2186a2e6","modified":1557312625527},{"_id":"source/imgs/conv-kernel.png","hash":"49385ef7cb512818b398d0a7b0f05ef87eb13815","modified":1557312625544},{"_id":"source/imgs/conv_3d.jpg","hash":"685e9b2737808127c0dc5dd31f3974429ec79ba8","modified":1557312625545},{"_id":"source/imgs/conv_no_padding.gif","hash":"b7e6480ca8666308bb6d682fcfb3a1de52541800","modified":1557312625554},{"_id":"source/imgs/conv_padding.gif","hash":"766855e42a62f80f33e2e91eacd2c81de0cc4a71","modified":1557312625556},{"_id":"source/imgs/figure2.1.gif","hash":"65d8d06d3b173689605608cc0873d9f8ec54e704","modified":1557312625570},{"_id":"source/imgs/figure2.1_2.png","hash":"33324d6a1f36eebba5e4ced6c545046e1dcf34a5","modified":1557312625573},{"_id":"source/imgs/figure2.2.gif","hash":"0475289e221e9f6fab182681ea7dc2b5a5d86b35","modified":1557312625574},{"_id":"source/imgs/figure2.2_2.png","hash":"30de37808d2e242d9673db8bfce4cf61a6783c3c","modified":1557312625578},{"_id":"source/imgs/figure2.3.gif","hash":"f0c73fc225bd7ce39516671adcc152fb242173f3","modified":1557312625579},{"_id":"source/imgs/figure2.3_2.png","hash":"f5a66b81e556847aec8ddea0b91c598bf27a8cce","modified":1557312625583},{"_id":"source/imgs/figure2.4.gif","hash":"83e9c625f85dedfae3250cc70a5cca2e97d9d912","modified":1557312625584},{"_id":"source/imgs/figure2.4_2.png","hash":"51cc4ee92b896f32f3755d08ad2f80be85c2b889","modified":1557312625587},{"_id":"source/imgs/figure2.5.gif","hash":"30f768150fdd4ac706c4276b5875aa491d9c2f0d","modified":1557312625587},{"_id":"source/imgs/figure2.5_5.png","hash":"2549be25e02bb9da01ec076b2d590b488217d139","modified":1557312625590},{"_id":"source/imgs/figure2.6.gif","hash":"6e983967bb3807df17c97047fe7fc4c63c3bd0c0","modified":1557312625591},{"_id":"source/imgs/figure2.6_2.png","hash":"c134b016f110977a0329f10ecd99700db685917b","modified":1557312625594},{"_id":"source/imgs/join_us.jpg","hash":"5d1f0623f4ec8f3b5f8ac40981d4d306a1b55e63","modified":1557312625605},{"_id":"source/imgs/machine-api.jpg","hash":"cdb314781288a551f97244988f29cff0d4bc0db9","modified":1557312625608},{"_id":"themes/raytaylorism/_data/about.json","hash":"c74a6298b37f190c5cf940abf3accc463cb771a6","modified":1557312625689},{"_id":"themes/raytaylorism/_data/hint.json","hash":"7433c56bdcc76fab584670a80442200d9b605f5e","modified":1557312625689},{"_id":"themes/raytaylorism/_data/link.json","hash":"fb7c0deefc9952cf7819234fadb2ea751777e609","modified":1557312625689},{"_id":"themes/raytaylorism/_data/reading.json","hash":"4b91b2b0d7d06d3cd3bf2fbe59c51884d6304b13","modified":1557312625689},{"_id":"themes/raytaylorism/_data/slider.json","hash":"2f350790b3d2a42dcd6b7adb12c730633a2964c4","modified":1557312625690},{"_id":"themes/raytaylorism/_md/.DS_Store","hash":"7cd85de2429f029bf1ad9797e71cf44e78fb4aee","modified":1557308759601},{"_id":"themes/raytaylorism/languages/en.yml","hash":"ac672903f9c45f244db56e9408b4546d026fee8f","modified":1557312625691},{"_id":"themes/raytaylorism/languages/zh-CN.yml","hash":"b2211c4d88a3f319316f6ecbad748a0ae4b4b91b","modified":1557312625692},{"_id":"themes/raytaylorism/languages/zh-TW.yml","hash":"ae281c898cea81f4c897c0a69c45e2ce6a4314a6","modified":1557312625692},{"_id":"themes/raytaylorism/layout/.DS_Store","hash":"de8bfb1f4cd01717c7f2e3ec1558b06e2cd4139f","modified":1557308753710},{"_id":"themes/raytaylorism/layout/about.ejs","hash":"599b3bb334b3f88b918e67f7a709287b8effee6d","modified":1557312625710},{"_id":"themes/raytaylorism/layout/archive.ejs","hash":"0a21af8903e95c6d8bb7554b089ac219e8708ad7","modified":1557312625710},{"_id":"themes/raytaylorism/layout/index.ejs","hash":"50c1e7dab5a065fd10dd3a28fdffa5e3d342de82","modified":1557312625711},{"_id":"themes/raytaylorism/layout/layout.ejs","hash":"43beb54ac81519cf5e88a3a1494649beeb856066","modified":1557312625711},{"_id":"themes/raytaylorism/layout/page.ejs","hash":"90441f114859ce63ef7c7d93d668dbe5939995c2","modified":1557312625711},{"_id":"themes/raytaylorism/layout/post.ejs","hash":"8e550fd95ef761909294ed3a4aa428ff0509fbf0","modified":1557312625712},{"_id":"themes/raytaylorism/layout/reading.ejs","hash":"3b2f77f0a154d2f6966b684eee69f26709968936","modified":1557312625712},{"_id":"themes/raytaylorism/layout/tag.ejs","hash":"42ecab14917abd40c0a38e6ab629f089352a24b1","modified":1557312625713},{"_id":"themes/raytaylorism/layout/title.ejs","hash":"f0733a134b375172a2cec830d7d09bdba33891fe","modified":1557312625713},{"_id":"themes/raytaylorism/source/.DS_Store","hash":"69d990a26752d1783fb51bf84639eddfc0a63fd9","modified":1557308748510},{"_id":"themes/raytaylorism/source/favicon.png","hash":"f28180f9a5026132b36b4a786c0577e68ea1fe55","modified":1521800510000},{"_id":"source/imgs/AI_Live.jpg","hash":"607ada237766e13b74d21265094fa50b1f198946","modified":1557312625498},{"_id":"source/imgs/LeNet-5.png","hash":"e0b98c516f321c5a7d3462894216bb29c41c46e1","modified":1557312625500},{"_id":"source/imgs/fc.png","hash":"d0c8bf012b0155c31f7f7fb1d4b0a73ea6fdc666","modified":1557312625558},{"_id":"source/imgs/figure1.5.gif","hash":"8a0d1d60844e6df01cee4ebe6c5b78a973242804","modified":1557312625560},{"_id":"source/imgs/figure1.6.gif","hash":"dff4a86a663d6dd26902ec7b74d6d2a6d07d84a1","modified":1557312625565},{"_id":"source/imgs/flower.jpg","hash":"c182da7dd384e63ffc854e083bc2b7a035f6abed","modified":1557312625594},{"_id":"source/imgs/iclass.png","hash":"ee48ed2068acfcd8f3bbc1101134a9a68043ff4b","modified":1557312625599},{"_id":"source/imgs/mnist_data.png","hash":"8ab2f5139eaea8e13f9560ddcf03e1480d80e166","modified":1557312625609},{"_id":"source/imgs/mnist_result.png","hash":"836352838b8e8d5736b0f1d15371d326e55ed6ba","modified":1557312625610},{"_id":"source/imgs/mnist_resultpng.png","hash":"836352838b8e8d5736b0f1d15371d326e55ed6ba","modified":1557312625611},{"_id":"source/imgs/pythoner.png","hash":"926f3e215b9b527227303e2a6ec13f3cf0612d5c","modified":1557312625613},{"_id":"source/imgs/sky.jpg","hash":"9e6292ef9a088fe81b8becc64683e66839561bf0","modified":1557312625629},{"_id":"source/imgs/surf_3dmax.png","hash":"e29f0ff7249d89d6a7c62fa6249ab36cb0f4205e","modified":1557312625630},{"_id":"source/imgs/surf_roughness_128.png","hash":"435a40961079504345286e199c450dd6caf151ac","modified":1557312625631},{"_id":"source/imgs/water.jpg","hash":"d70d3a35840bd6f4a785ceb68bc97459836b9886","modified":1557312625643},{"_id":"source/imgs/wetchat-rewardcode.jpg","hash":"37f65c6d09fca7a09dd10da6987268daa42203b4","modified":1557312625643},{"_id":"source/imgs/conv_as_matrix.png","hash":"983cde6950719d5167edb8b1c647a63f897df7d5","modified":1557312625548},{"_id":"source/imgs/figure2.1.png","hash":"cd375642745e34ffde4aaa4031ad0819b9d64d43","modified":1557312625573},{"_id":"source/imgs/figure2.4.png","hash":"0ff077e4226522b003ae45d9399f8505e2000156","modified":1557312625586},{"_id":"source/imgs/figure2.5.png","hash":"37752f80f83e750295334fc4cc72deace76c60d3","modified":1557312625590},{"_id":"source/imgs/surf_roughness_256.png","hash":"f450d97d71a44c51dc4af9d2b7c73b9125240b28","modified":1557312625632},{"_id":"themes/raytaylorism/_md/about/index.md","hash":"fa416d307e7d2e4f0162c58a0d6ffe8a40e28ee8","modified":1557312625690},{"_id":"themes/raytaylorism/_md/reading/index.md","hash":"ab4ae4fad36f371f60b49973797a115423a784d4","modified":1557312625691},{"_id":"themes/raytaylorism/layout/_partial/.DS_Store","hash":"3081e9e7bacb19d2bf2242f4c76a01b132f2f1b7","modified":1557302306975},{"_id":"themes/raytaylorism/layout/_partial/after_footer.ejs","hash":"9fafc2cb14cbca89e48335d64ab058b5f256a36e","modified":1557312625693},{"_id":"themes/raytaylorism/layout/_partial/archive.ejs","hash":"68c7db951ffb5323d49d4de74e3b0de7f70fb4c3","modified":1557312625693},{"_id":"themes/raytaylorism/layout/_partial/archive_title.ejs","hash":"dfc6c670702e64abce5fd87e3e2ea43c966ace32","modified":1557312625694},{"_id":"themes/raytaylorism/layout/_partial/article.ejs","hash":"c86891ff7ce5f83858e24ec8c382ab1b58c401e2","modified":1557313247336},{"_id":"themes/raytaylorism/layout/_partial/construction.ejs","hash":"21190b5a0d567ed4ea5d5289459690b72c1452f0","modified":1557312625694},{"_id":"themes/raytaylorism/layout/_partial/feature_guide.ejs","hash":"7aefb6bdc65d1e6113cb83190fcd2f29af2c9125","modified":1557312625695},{"_id":"themes/raytaylorism/layout/_partial/float.ejs","hash":"a5594e23bff2047156b647fbdd0ef8247ee4ec65","modified":1557312625695},{"_id":"themes/raytaylorism/layout/_partial/footer.ejs","hash":"7d8ade0e17012bf0006d234a8e8efd633d2658f2","modified":1557312625696},{"_id":"themes/raytaylorism/layout/_partial/head.ejs","hash":"7ceea72401426588cd7778f92585ab9487b463da","modified":1557312625696},{"_id":"themes/raytaylorism/layout/_partial/header.ejs","hash":"0616dd744262dd4cc98cd1cabe959643c845141f","modified":1557312625697},{"_id":"themes/raytaylorism/layout/_partial/menu_drawer.ejs","hash":"028ecbf59089cc4d1907a2d91d8da937f92d321c","modified":1557312625697},{"_id":"themes/raytaylorism/layout/_partial/pagenav.ejs","hash":"e7ada8faaee878ea4dde267d1b420bb45421670d","modified":1557312625697},{"_id":"themes/raytaylorism/layout/_partial/pagination.ejs","hash":"00de7746cf4ef8c4b67a72e825e5ff236f9d5814","modified":1557312625698},{"_id":"themes/raytaylorism/layout/_partial/search.ejs","hash":"0eca40de0d39c1ae52040fcb8c9d7f79afce35dc","modified":1557312625706},{"_id":"themes/raytaylorism/layout/_partial/side_nav.ejs","hash":"c69c45de069c348bf3906f1bd941920887a85c98","modified":1557312625706},{"_id":"themes/raytaylorism/layout/_partial/simple_article.ejs","hash":"6480e101b2f29dddd661410c56516c767d88b79f","modified":1557312625707},{"_id":"themes/raytaylorism/layout/_partial/slider.ejs","hash":"bb7b53f6ca9c852808d955fb074f88112e51ea59","modified":1557312625707},{"_id":"themes/raytaylorism/layout/_widget/blogroll.ejs","hash":"1a6808fa62906e7fb1fac3e16208fa6b1fc8d0ea","modified":1557312625708},{"_id":"themes/raytaylorism/layout/_widget/recent_posts.ejs","hash":"935bfacce10a726eed6cd82fe39d2c6f9cce9e2a","modified":1557312625709},{"_id":"themes/raytaylorism/layout/_widget/category.ejs","hash":"95292eb643be63d98f08e28f759c9b01bbfcb9b8","modified":1557312625708},{"_id":"themes/raytaylorism/layout/_widget/tag.ejs","hash":"90e0ba4412285903420ee3b43125a56743edf0c6","modified":1557312625709},{"_id":"themes/raytaylorism/layout/_widget/tagcloud.ejs","hash":"f256f028c247bdcb7927351df89f2284c64b7b6c","modified":1557312625709},{"_id":"themes/raytaylorism/source/css/style.styl","hash":"a05bcd2543b7bdcd3f725db6d053cd76ccf154be","modified":1557312625730},{"_id":"themes/raytaylorism/source/js/prettify.js","hash":"d592e6f771c2955cea3764d819221b91bc343961","modified":1557312625734},{"_id":"source/imgs/figure1.5.png","hash":"dfc7b73284a3bc732295eb915b557f0a8e7df92a","modified":1557312625564},{"_id":"source/imgs/figure1.6.png","hash":"082f7688f09bca65b53b77e8dcb73492e3f6ef82","modified":1557312625569},{"_id":"source/imgs/figure2.3.png","hash":"8a21087ecf5badbcc1897715495e89a3d204a3ac","modified":1557312625582},{"_id":"source/imgs/figure2.6.png","hash":"2b6a7a9394a9ede424487a3de96b8ff2febe7844","modified":1557312625593},{"_id":"source/imgs/histequa.png","hash":"89eb9d30577401536f20ab07ae8e5647ce79e867","modified":1557312625598},{"_id":"themes/raytaylorism/source/js/jquery.min.js","hash":"f694238d616f579a0690001f37984af430c19963","modified":1557312625732},{"_id":"source/imgs/figure2.2.png","hash":"e02638ef5be764c4f8be1168bbb555db1694f075","modified":1557312625578},{"_id":"themes/raytaylorism/layout/_partial/plugin/analytics.ejs","hash":"b7dbd8342866929e683e9b013caa7324547ff704","modified":1557312625698},{"_id":"themes/raytaylorism/layout/_partial/plugin/comment.ejs","hash":"9d8e3cda9e11cfcb199da90e79baf11e71c2cfec","modified":1557312625699},{"_id":"themes/raytaylorism/layout/_partial/plugin/google_code_prettify.ejs","hash":"336f01048440f0c9f7b75f24aafcc3a1ffefd9a0","modified":1557312625699},{"_id":"themes/raytaylorism/layout/_partial/plugin/main_javascript.ejs","hash":"6629eec982aa789767b83e80af12fa40189ac344","modified":1557312625700},{"_id":"themes/raytaylorism/layout/_partial/plugin/mathjax.ejs","hash":"6f6b85a5876ae150d3e5f08e384aff68652c0335","modified":1557312625700},{"_id":"themes/raytaylorism/layout/_partial/plugin/noscript.ejs","hash":"182650c8be93b093997ac4d5fe14af2f835b98d9","modified":1557312625700},{"_id":"themes/raytaylorism/layout/_partial/plugin/page_stat.ejs","hash":"03b9126acbf80247ad6586d49072e6a767746a1e","modified":1557312625701},{"_id":"themes/raytaylorism/layout/_partial/plugin/reward.ejs","hash":"284ab1d5cb4f43eb23b6d7a8aba2477b34abdc00","modified":1557312625701},{"_id":"themes/raytaylorism/layout/_partial/post/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1557305794441},{"_id":"themes/raytaylorism/layout/_partial/post/gallery.ejs","hash":"bd2285802766572736663e61852eb49f6acc744f","modified":1557312625702},{"_id":"themes/raytaylorism/layout/_partial/post/livere.ejs","hash":"bfad3e53acffd56b950017ed1754c7fdb8ec1486","modified":1557312625703},{"_id":"themes/raytaylorism/layout/_partial/post/category.ejs","hash":"e17f452079201bd2a5a37bc76b51b132afd04faa","modified":1557312625702},{"_id":"themes/raytaylorism/layout/_partial/post/prevnext.ejs","hash":"6556eea4fb351639006c16e9831fd72ab46076ba","modified":1557312625703},{"_id":"themes/raytaylorism/layout/_partial/post/readtimes.ejs","hash":"c829d0598f9906f663a8ace1c86f2aa6024d642c","modified":1557312625704},{"_id":"themes/raytaylorism/layout/_partial/post/tablecontents.ejs","hash":"585ea42410648f193184931864a64b41635af956","modified":1557312625704},{"_id":"themes/raytaylorism/layout/_partial/post/tag.ejs","hash":"0f84c1aded9ba1887566d34e7f0d696c015295f0","modified":1557312625705},{"_id":"themes/raytaylorism/layout/_partial/post/time.ejs","hash":"42210d6b5a132f5c18352dcff2983d3fdbe26956","modified":1557312625705},{"_id":"themes/raytaylorism/layout/_partial/post/title.ejs","hash":"f0733a134b375172a2cec830d7d09bdba33891fe","modified":1557312625705},{"_id":"themes/raytaylorism/source/css/_base/icons.css","hash":"ab167f1694ffe10c3c51d18a633efd41be121555","modified":1557312625714},{"_id":"themes/raytaylorism/source/css/_base/layout.styl","hash":"b2f718418de61946504a3f8bf28b75be165913a7","modified":1557312625714},{"_id":"themes/raytaylorism/source/css/_base/lib_customize.styl","hash":"5f25b295a3ad99991952f864573c0f1ccc6a1591","modified":1557312625716},{"_id":"themes/raytaylorism/source/css/_base/variable.styl","hash":"ce4e056d1bbfb80734d98a6898950e7c0136edf4","modified":1557312625716},{"_id":"themes/raytaylorism/source/css/_partial/about.styl","hash":"def183d6908ebcbd59341b09e9f7e06dc277b9ca","modified":1557312625716},{"_id":"themes/raytaylorism/source/css/_partial/archive.styl","hash":"4d48566e9f72b8eac8875b6985885418f56fbafa","modified":1557312625717},{"_id":"themes/raytaylorism/source/css/_partial/article.styl","hash":"293e38a8ab9aee346cc8e52421f1519c5a46a667","modified":1557312625717},{"_id":"themes/raytaylorism/source/css/_partial/comment.styl","hash":"590f1386581181ab588be06e4189861f5a209467","modified":1557312625718},{"_id":"themes/raytaylorism/source/css/_partial/footer.styl","hash":"7f2c22ebc3fe551496625e9453017e512d670aea","modified":1557312625718},{"_id":"themes/raytaylorism/source/css/_partial/header.styl","hash":"ebfd0155cda8a0876c36595708f02c294a7c82a0","modified":1557312625719},{"_id":"themes/raytaylorism/source/css/_partial/index.styl","hash":"ac83523dd14a1fc1fe55f98c84ed84cb03be864b","modified":1557312625720},{"_id":"themes/raytaylorism/source/css/_partial/link_context.styl","hash":"5b23db4dee53cbbe9eef257f4a542823100fde72","modified":1557312625720},{"_id":"themes/raytaylorism/source/css/_partial/other.styl","hash":"32bf499037a45ad2e0007a9ab3054067adc02506","modified":1557312625721},{"_id":"themes/raytaylorism/source/css/_partial/reading.styl","hash":"f81929fa12212465b02456d0bb3b8263355e3281","modified":1557312625721},{"_id":"themes/raytaylorism/source/css/_partial/search.styl","hash":"f9ca6f5626c795ae73ff7412ff58207b62fd64ac","modified":1557312625722},{"_id":"themes/raytaylorism/source/css/_partial/side_nav.styl","hash":"b239b6b55e87e86d038d6aa821beeb66a9cbaf39","modified":1557312625722},{"_id":"themes/raytaylorism/source/css/_partial/slider.styl","hash":"ad757e74b3500aa774636ebbe5bdcee7e52e5ad7","modified":1557312625723},{"_id":"themes/raytaylorism/source/css/_partial/syntax.styl","hash":"f39e7bb08abcc220f7c57fb413e76f4043ab9c35","modified":1557312625724},{"_id":"themes/raytaylorism/source/css/_partial/tablecontents.styl","hash":"e04fa0e7664065077750a7223ae3390cc84a4c56","modified":1557312625724},{"_id":"themes/raytaylorism/source/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/lib/font-awesome.min.css","hash":"14be7d7ae1894d2cc7c1a8e847df4db42a310b2f","modified":1557312625728},{"_id":"themes/raytaylorism/source/css/lib/prettify-tomorrow-night-eighties.css","hash":"e320b2be926124d30998af0e149b7f06303b8f8b","modified":1557312625730},{"_id":"source/imgs/algorithm/string/rota_str2.png","hash":"169ddce44c2ae804501828b906bd9cece5a3d775","modified":1557312625502},{"_id":"source/imgs/conv_as_matrix_2.png","hash":"e97001c70d7a66fa8df60a93df5d52982de843df","modified":1557312625552},{"_id":"source/imgs/light.jpg","hash":"75230c1891009503598822c19d41e4a85e211244","modified":1557312625608},{"_id":"themes/raytaylorism/source/css/lib/materialize.min.css","hash":"2cdb74e6b61dc8f08352ba61979d3de314fe2af7","modified":1557312625729},{"_id":"themes/raytaylorism/source/js/materialize.min.js","hash":"04fe8bbc9a3165eb7bfb13b7166306ed671268d8","modified":1557312625733},{"_id":"source/imgs/algorithm/string/rota_str10.png","hash":"feb768182712b66df3ba47c626ffa8c2ae50dde7","modified":1557312625502},{"_id":"source/imgs/algorithm/string/rota_str4.png","hash":"5437c9889d8cd642d4e31a122efc8ee7abeab6e2","modified":1557312625505},{"_id":"source/imgs/algorithm/tree/level_tree.png","hash":"6fe7f8c02badf03b66d7f0ae1b39271ec2108396","modified":1557312625518},{"_id":"source/imgs/algorithm/tree/level_tree2.png","hash":"bf2a0461ff3501f0f436c332e872afd932322ad7","modified":1557312625520},{"_id":"source/imgs/algorithm/tree/level_tree3.png","hash":"7f3a2d38dd3f4bb79c713d1f88455bc114640108","modified":1557312625522},{"_id":"source/imgs/orange.jpg","hash":"0c112866bd377b63b171771422d6b20b997d7e46","modified":1557312625613},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1521800510000},{"_id":"source/imgs/50mm-106-68-ans.png","hash":"b02f3c530b8275a198f2624444f3c6b2a7740fb4","modified":1557312625492},{"_id":"source/imgs/50mm-106-68.png","hash":"391095396ddd75705ce82f1a74ee225557860764","modified":1557312625497},{"_id":"source/imgs/algorithm/string/rota_str3.png","hash":"cbc60e259bd9b649cce3b93479a620cb66fd84ab","modified":1557312625504},{"_id":"source/imgs/algorithm/string/rota_str6.png","hash":"e64e096d5ce26167180ccc2f3332a75050669def","modified":1557312625509},{"_id":"source/imgs/water-circle.jpg","hash":"f8cc3871a46549b703063c864b764e22ec3d13c2","modified":1557312625642},{"_id":"themes/raytaylorism/source/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1521800510000},{"_id":"source/imgs/algorithm/string/rota_str7.png","hash":"c3504ff240f1a5964954c3b39cf5ed3e0a34a775","modified":1557312625511},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1521800510000},{"_id":"themes/raytaylorism/source/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1521800510000},{"_id":"source/imgs/all_day.jpg","hash":"e25f7319e8c3a178dffc6d93676bd184e4234926","modified":1557312625527},{"_id":"source/imgs/algorithm/string/rota_str9.png","hash":"6e19ad313c849c3c692ba46c091fd4c8765aa76a","modified":1557312625516},{"_id":"themes/raytaylorism/source/css/font/font-awesome/fontawesome-webfont.svg","hash":"550ef5c1253c8376f2ead32b654eb58d3c106ca3","modified":1557312625727},{"_id":"source/imgs/algorithm/string/rota_str5.png","hash":"48ed77eb529a4fa605be982ec03cb9d776ab71bd","modified":1557312625507},{"_id":"source/imgs/algorithm/string/rota_str8.png","hash":"ddd356d7e348611ad9f8bda46e32db52873135bf","modified":1557312625514},{"_id":"source/imgs/game.png","hash":"ca05a21991384b3fe0eaf3f529b1fbfb09439e10","modified":1557312625597},{"_id":"source/imgs/sky-night.jpg","hash":"1c15377e38819ad6c8bc4feb25d9a98a8b679c8e","modified":1557312625628},{"_id":"source/imgs/img-cov.png","hash":"71f7616a6e54befc3d7019b6e0b85e1bfb7cc784","modified":1557312625604},{"_id":"source/imgs/wall.png","hash":"d47ce4e9c164c9563cd230767bbc29f29a738981","modified":1557312625637},{"_id":"source/imgs/winter.jpg","hash":"d62f8b169f87e09f46884582b5652e78834433c4","modified":1557312625654},{"_id":"source/imgs/rain.png","hash":"5d382d60591561923d178dd24d89cb1189b193f6","modified":1557312625622},{"_id":"source/imgs/coloreggs.jpg","hash":"518341c5e974a98f8bc00b9e5972954071bf645a","modified":1557312625543},{"_id":"public/search.xml","hash":"8ac6f7330061867afdc14dc84c7033cca535cf44","modified":1557313723053},{"_id":"public/about/index.html","hash":"d4b9a7d49933058a7081966ca07267fcc0406475","modified":1557313723740},{"_id":"public/reading/index.html","hash":"d55472feb85a8f8401dbc2ec338e01da2d671f06","modified":1557313723740},{"_id":"public/2019/05/08/第 7 周学习分享/index.html","hash":"594281737fee69d695ea486550b118bef3eca967","modified":1557313723740},{"_id":"public/2019/05/04/第 6 周学习分享/index.html","hash":"118bba54a0c457e817ca397802a5d17f3831e07e","modified":1557313723740},{"_id":"public/2019/04/28/第 5 周学习分享/index.html","hash":"4d128c622163ff70d9e0585e81b505d96be33b6e","modified":1557313723740},{"_id":"public/2019/04/20/第 4 周学习分享/index.html","hash":"f30a373669c1ad75252e32e7e230a54174537be7","modified":1557313723741},{"_id":"public/2019/04/13/第 3 周学习分享/index.html","hash":"1dff9d55d9ba8d0165b12fb2f88569d33a19083c","modified":1557313723741},{"_id":"public/2019/04/06/第 2 周学习分享/index.html","hash":"4184511a5ee3fa09fb4ceedf4b0376384b5be85e","modified":1557313723741},{"_id":"public/2019/03/30/第 1 周学习分享/index.html","hash":"77fe2fbe0164bc3074bc74fb062da35105c70c5d","modified":1557313723741},{"_id":"public/2018/06/21/深度学习中的卷积和池化/index.html","hash":"91c81df2a4e2548319cd7aa1e715b91e542fc67a","modified":1557313723741},{"_id":"public/2018/03/25/字符串-旋转词/index.html","hash":"677150e863b1e78c77195531e10db3aad68e8d2a","modified":1557313723741},{"_id":"public/2018/03/25/二叉树按层打印/index.html","hash":"21008758e03a20b48849da92ef427397a7811f09","modified":1557313723741},{"_id":"public/2018/03/23/粗糙表面计算机模拟/index.html","hash":"3e27ba379af6b009789854b84ce420793cb788e3","modified":1557313723741},{"_id":"public/2018/03/22/关于k阶矩的理解/index.html","hash":"8c5ab92fae9629dc4542f57400db71a1970addb9","modified":1557313723741},{"_id":"public/2018/03/22/Hexo多电脑同步写作/index.html","hash":"78f2be86e09aaf7295645978acffe463a6080ac0","modified":1557313723741},{"_id":"public/2018/03/20/Mnist手写数字识别-tensorflow/index.html","hash":"086d2304fe8b8d03698e0a6e7e75b318bbd7abd8","modified":1557313723741},{"_id":"public/2018/03/20/直方图均衡化图片/index.html","hash":"53ca46a4073df2c927857bb7e64a7169cd62e80d","modified":1557313723741},{"_id":"public/2018/03/19/图像常用算子进行卷积运算/index.html","hash":"aaafbcb511feb220fd28a94962a2229e9f7235d0","modified":1557313723741},{"_id":"public/categories/技术文章/index.html","hash":"15a9afdee2a89018f527bb9210e59f7c02be2a4f","modified":1557313723742},{"_id":"public/categories/基础算法/index.html","hash":"ccbd865abd880581580198a06f4979ab051b6c88","modified":1557313723742},{"_id":"public/categories/数学基础/index.html","hash":"d1662519ac163327bf54363ba566ecefac2d10ed","modified":1557313723742},{"_id":"public/categories/图像处理/index.html","hash":"ea53064be1299c2a456794ece1598006b8e641a8","modified":1557313723742},{"_id":"public/categories/技术文章/博客搭建/index.html","hash":"50912b41cc68aa9b0b87b2d8a0d4d87162ab179a","modified":1557313723742},{"_id":"public/categories/基础算法/树/index.html","hash":"f09c441a245d7ee298a782988d89985bbfc2e4b9","modified":1557313723742},{"_id":"public/categories/周总结/index.html","hash":"8335b55a34125fc9a9fa3ba4b264487a29578993","modified":1557313723742},{"_id":"public/categories/数学基础/随机过程/index.html","hash":"7c182e51a347754020dfc01d5f3305371966e100","modified":1557313723742},{"_id":"public/categories/图像处理/图像增强/index.html","hash":"940835fd50cfb014c294858a274e88b069237f81","modified":1557313723742},{"_id":"public/categories/研磨-粗糙度/index.html","hash":"c368f0c354d6a2e877a55fab2b4caa637475ad07","modified":1557313723742},{"_id":"public/categories/基础算法/字符串/index.html","hash":"16f40e64a5faa0e107b14c500b30c9fc8f544e24","modified":1557313723742},{"_id":"public/categories/研磨-粗糙度/研磨表面仿真/index.html","hash":"1125faa69d2abedeb0aad8da76c5e19048d456f8","modified":1557313723742},{"_id":"public/categories/tensorflow学习/index.html","hash":"d71e79177ff12246064fdfbfc726d041e8589bde","modified":1557313723742},{"_id":"public/categories/tensorflow学习/tensorflow-Demo/index.html","hash":"fea152aa5e27208fad619cfbcce5865360a97ba5","modified":1557313723742},{"_id":"public/categories/深度学习/index.html","hash":"f99533fbf68edaa62f5d2c870cc79af748a335d4","modified":1557313723742},{"_id":"public/categories/深度学习/深度学习基础/index.html","hash":"7157e89adc1b12629b39f360004b96c741718d15","modified":1557313723742},{"_id":"public/archives/index.html","hash":"362c7a2aa839e0219aa2bf9662fae14a8f0724d0","modified":1557313723743},{"_id":"public/archives/page/2/index.html","hash":"54698661ac496a55286c835266a6f345a05a6f4c","modified":1557313723743},{"_id":"public/archives/2018/index.html","hash":"cf50f214d8726ba6a13fe2e126354cf6b27e4502","modified":1557313723743},{"_id":"public/archives/2018/03/index.html","hash":"4c7530c241521d727e435732fc8af64f6597e755","modified":1557313723743},{"_id":"public/archives/2018/06/index.html","hash":"6e650d9d9d24b5eeb596b02c78db3a815be1b67c","modified":1557313723743},{"_id":"public/archives/2019/index.html","hash":"4e0630b87a1ddea10551ee0bddac80ed34c8149e","modified":1557313723743},{"_id":"public/archives/2019/03/index.html","hash":"b8961c3c2d807f8a7e9041d792a9d5e17b2079a2","modified":1557313723743},{"_id":"public/archives/2019/04/index.html","hash":"e12d673adb9e0dffbf0da08c2f2cfcf50f498ee5","modified":1557313723743},{"_id":"public/archives/2019/05/index.html","hash":"1a26d34d50673c9c08103852abdecd3992c67c93","modified":1557313723743},{"_id":"public/index.html","hash":"dd839cf1aeb25eba9db94659765b953a2a52f8ad","modified":1557313723743},{"_id":"public/page/2/index.html","hash":"f9de4a2ce17a1b816c13126cbaa241458b7334d9","modified":1557313723743},{"_id":"public/tags/hexo/index.html","hash":"fb0ff56307923c3451686e98080c852aab0e66ad","modified":1557313723743},{"_id":"public/tags/多电脑同步/index.html","hash":"79149f5d4d51b43cf3e501ef0a813a10fbfb3aab","modified":1557313723743},{"_id":"public/tags/二叉树/index.html","hash":"65006164ff05cdbe51858cbe729e590cbc532eb2","modified":1557313723743},{"_id":"public/tags/层次遍历/index.html","hash":"7fe7012fb78382a4b44a9bf1f84ff8095339ce0c","modified":1557313723743},{"_id":"public/tags/java实现/index.html","hash":"a049a970a8d97e86f10e8d4ce80da0902a7fb5db","modified":1557313723743},{"_id":"public/tags/Python实现/index.html","hash":"f71232eff0ba54c572b4fac8217d104a8fef765d","modified":1557313723743},{"_id":"public/tags/k阶矩/index.html","hash":"26f3097212a7698cba4d31fc23674e2cb7754c95","modified":1557313723743},{"_id":"public/tags/随机过程/index.html","hash":"13fe018a6043c9b42b376b84489abbeedd273241","modified":1557313723744},{"_id":"public/tags/偏度/index.html","hash":"4b56a4bb4f9c0d2aafe86da893ddf81ccd2ffeec","modified":1557313723744},{"_id":"public/tags/峰度/index.html","hash":"9ef103f708c9dcf818e51d36c4154f870e064e98","modified":1557313723744},{"_id":"public/tags/图像处理/index.html","hash":"f7e4c00a17a6ba36cc4b51679f50cfdffc14e4f2","modified":1557313723744},{"_id":"public/tags/卷积运算/index.html","hash":"7c2afd1026e56d731d4dd1944957fdfbe1e7eb9f","modified":1557313723744},{"_id":"public/tags/guass/index.html","hash":"3b2ea4e30fc686d68816db9685d68ef247631f35","modified":1557313723744},{"_id":"public/tags/soble/index.html","hash":"785b056d060d5da93d1c5f361160498284b85c92","modified":1557313723744},{"_id":"public/tags/prewitt/index.html","hash":"f57b6b61d3a7274903aa256d4d0ff338134507c5","modified":1557313723744},{"_id":"public/tags/laplacian/index.html","hash":"1dcab4712ecf1df91d0adc728ab07536a2fda4d9","modified":1557313723744},{"_id":"public/tags/字符串/index.html","hash":"83e58856fe56234eb8edb06c34e2ef01dd64614c","modified":1557313723744},{"_id":"public/tags/旋转词/index.html","hash":"47d6e64746da101bc16bce5e32cb5953c40ec0c2","modified":1557313723744},{"_id":"public/tags/python/index.html","hash":"3d81152d30863ecd3442af813f15ffc789493198","modified":1557313723744},{"_id":"public/tags/直方图均衡化/index.html","hash":"56c7c753df9993923d6491d0bf26fb2ed7a41df7","modified":1557313723744},{"_id":"public/tags/风格迁移/index.html","hash":"c0729bd399d7118a0b92dc3e009cd0be40c6b1ac","modified":1557313723744},{"_id":"public/tags/序列模型/index.html","hash":"6600545e0d756413b78b271d7b9a0db22935fdf9","modified":1557313723744},{"_id":"public/tags/deep-learning/index.html","hash":"23b4f515943bb68359e182e3b79149e7b81262c2","modified":1557313723744},{"_id":"public/tags/NLP基础/index.html","hash":"1680b62cd2ee18cf30786ffe456a3cf4afd75ec8","modified":1557313723744},{"_id":"public/tags/CV基础/index.html","hash":"5824d3a87505c429b0826dd39db14ad2d09a132b","modified":1557313723744},{"_id":"public/tags/相机模型/index.html","hash":"54c623bd4935a0f0b50ce74ff4e972a502a96a3a","modified":1557313723745},{"_id":"public/tags/YOLO/index.html","hash":"7632bae4949a9313b8ca6687f28e5488e9d58b80","modified":1557313723745},{"_id":"public/tags/Attention/index.html","hash":"acc555b8cfb7ba5f57c20e8b6e3330afd1489dff","modified":1557313723745},{"_id":"public/tags/GN/index.html","hash":"3e946c3fe2fa384f5cddd206f73a4c8a1c57d970","modified":1557313723745},{"_id":"public/tags/目标检测/index.html","hash":"44bfaa19344526e23216c5c9239648d7c89c515e","modified":1557313723745},{"_id":"public/tags/R-CNN系列/index.html","hash":"7a2cb37be91e57e210d131c26c77ce2ec442b1d7","modified":1557313723745},{"_id":"public/tags/SSD/index.html","hash":"73eee235e32d7f55a3c91fb8d57375994e2e2268","modified":1557313723745},{"_id":"public/tags/表面粗糙度/index.html","hash":"22efaf6475235d89a4b8ae19261217f915764df8","modified":1557313723745},{"_id":"public/tags/matlab模拟粗糙度/index.html","hash":"4e5e63544745f694478a32790c4571190fd0758f","modified":1557313723745},{"_id":"public/tags/3dsMax仿真/index.html","hash":"8e3593469fc9430e29914f5acccb2eaf90fb48f1","modified":1557313723745},{"_id":"public/tags/FCN/index.html","hash":"874745f53e393850416494d6caeb61fe15a4c873","modified":1557313723745},{"_id":"public/tags/Mnist/index.html","hash":"5f3d0559b9a16fe34d297a66c60e5692117bcfbf","modified":1557313723745},{"_id":"public/tags/tensorflow/index.html","hash":"5aa80e79ac2b3a04810be843714968f0723e71ed","modified":1557313723745},{"_id":"public/tags/Normalization/index.html","hash":"a393bdc60b2f628796ce8b539fdfc2b8acff5c07","modified":1557313723745},{"_id":"public/tags/BN/index.html","hash":"62755b1c44493775b6687139d11f4ad71e143701","modified":1557313723745},{"_id":"public/tags/卷积/index.html","hash":"5d2fc9daaf5c11eb3956d60978af2354c646f20e","modified":1557313723745},{"_id":"public/tags/池化/index.html","hash":"ccaa9a7d53ea94898f3d0b6405563a49dce3a7ef","modified":1557313723745},{"_id":"public/tags/LeNet-5/index.html","hash":"c1d492bfbae8209bc3c6ab518f2c7f09166eb6f1","modified":1557313723745},{"_id":"public/imgs/alipay-rewardcode.jpg","hash":"7093a60c54438b347cb13d79b7a34c99b0d6a4e3","modified":1557313723772},{"_id":"public/imgs/conv-kernel.png","hash":"49385ef7cb512818b398d0a7b0f05ef87eb13815","modified":1557313723772},{"_id":"public/imgs/conv_3d.jpg","hash":"685e9b2737808127c0dc5dd31f3974429ec79ba8","modified":1557313723772},{"_id":"public/imgs/conv_no_padding.gif","hash":"b7e6480ca8666308bb6d682fcfb3a1de52541800","modified":1557313723772},{"_id":"public/imgs/figure2.1_2.png","hash":"33324d6a1f36eebba5e4ced6c545046e1dcf34a5","modified":1557313723772},{"_id":"public/imgs/figure2.1.gif","hash":"65d8d06d3b173689605608cc0873d9f8ec54e704","modified":1557313723772},{"_id":"public/imgs/figure2.2_2.png","hash":"30de37808d2e242d9673db8bfce4cf61a6783c3c","modified":1557313723772},{"_id":"public/imgs/figure2.2.gif","hash":"0475289e221e9f6fab182681ea7dc2b5a5d86b35","modified":1557313723773},{"_id":"public/imgs/conv_padding.gif","hash":"766855e42a62f80f33e2e91eacd2c81de0cc4a71","modified":1557313723773},{"_id":"public/imgs/figure2.3.gif","hash":"f0c73fc225bd7ce39516671adcc152fb242173f3","modified":1557313723773},{"_id":"public/imgs/figure2.3_2.png","hash":"f5a66b81e556847aec8ddea0b91c598bf27a8cce","modified":1557313723773},{"_id":"public/imgs/figure2.4_2.png","hash":"51cc4ee92b896f32f3755d08ad2f80be85c2b889","modified":1557313723773},{"_id":"public/imgs/figure2.4.gif","hash":"83e9c625f85dedfae3250cc70a5cca2e97d9d912","modified":1557313723773},{"_id":"public/imgs/figure2.5_5.png","hash":"2549be25e02bb9da01ec076b2d590b488217d139","modified":1557313723773},{"_id":"public/imgs/figure2.5.gif","hash":"30f768150fdd4ac706c4276b5875aa491d9c2f0d","modified":1557313723773},{"_id":"public/imgs/figure2.6.gif","hash":"6e983967bb3807df17c97047fe7fc4c63c3bd0c0","modified":1557313723773},{"_id":"public/imgs/figure2.6_2.png","hash":"c134b016f110977a0329f10ecd99700db685917b","modified":1557313723773},{"_id":"public/imgs/join_us.jpg","hash":"5d1f0623f4ec8f3b5f8ac40981d4d306a1b55e63","modified":1557313723773},{"_id":"public/favicon.png","hash":"f28180f9a5026132b36b4a786c0577e68ea1fe55","modified":1557313723773},{"_id":"public/imgs/machine-api.jpg","hash":"cdb314781288a551f97244988f29cff0d4bc0db9","modified":1557313723773},{"_id":"public/css/images/side-user-cover.jpg","hash":"d8d73a64d6d5af83a27e6af1d4fedef808955ba0","modified":1557313723773},{"_id":"public/css/font/roboto/Roboto-Bold.eot","hash":"a76cd602f5188b9fbd4ba7443dcb9c064e3dbf10","modified":1557313723773},{"_id":"public/css/font/roboto/Roboto-Bold.woff","hash":"ee99cd87a59a9a5d4092c83232bb3eec67547425","modified":1557313723773},{"_id":"public/css/font/roboto/Roboto-Bold.woff2","hash":"933b866d09c2b087707a98dab64b3888865eeb96","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Light.eot","hash":"42fe156996197e5eb0c0264c5d1bb3b4681f4595","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Light.woff2","hash":"bbdc28b887400fcb340b504ec2904993af42a5d7","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Medium.woff2","hash":"6cc1b73571af9e827c4e7e91418f476703cd4c4b","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Medium.eot","hash":"1517f4b6e1c5d0e5198f937557253aac8fab0416","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Regular.eot","hash":"77ae3e980ec03863ebe2587a8ef9ddfd06941db0","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Regular.woff","hash":"74734dde8d94e7268170f9b994dedfbdcb5b3a15","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Regular.woff2","hash":"ed1558b0541f5e01ce48c7db1588371b990eec19","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Light.woff","hash":"6300f659be9e834ab263efe2fb3c581d48b1e7b2","modified":1557313723774},{"_id":"public/css/font/roboto/Roboto-Medium.woff","hash":"d45f84922131364989ad6578c7a06b6b4fc22c34","modified":1557313723775},{"_id":"public/imgs/avatar.jpg","hash":"ad4c9872fa0b5c143a8778d95437452e2186a2e6","modified":1557313724064},{"_id":"public/imgs/AI_Live.jpg","hash":"607ada237766e13b74d21265094fa50b1f198946","modified":1557313724064},{"_id":"public/imgs/fc.png","hash":"d0c8bf012b0155c31f7f7fb1d4b0a73ea6fdc666","modified":1557313724064},{"_id":"public/imgs/LeNet-5.png","hash":"e0b98c516f321c5a7d3462894216bb29c41c46e1","modified":1557313724064},{"_id":"public/imgs/figure1.5.gif","hash":"8a0d1d60844e6df01cee4ebe6c5b78a973242804","modified":1557313724064},{"_id":"public/imgs/figure1.6.gif","hash":"dff4a86a663d6dd26902ec7b74d6d2a6d07d84a1","modified":1557313724064},{"_id":"public/imgs/flower.jpg","hash":"c182da7dd384e63ffc854e083bc2b7a035f6abed","modified":1557313724064},{"_id":"public/imgs/iclass.png","hash":"ee48ed2068acfcd8f3bbc1101134a9a68043ff4b","modified":1557313724065},{"_id":"public/imgs/mnist_data.png","hash":"8ab2f5139eaea8e13f9560ddcf03e1480d80e166","modified":1557313724065},{"_id":"public/imgs/mnist_resultpng.png","hash":"836352838b8e8d5736b0f1d15371d326e55ed6ba","modified":1557313724065},{"_id":"public/imgs/sky.jpg","hash":"9e6292ef9a088fe81b8becc64683e66839561bf0","modified":1557313724065},{"_id":"public/imgs/pythoner.png","hash":"926f3e215b9b527227303e2a6ec13f3cf0612d5c","modified":1557313724065},{"_id":"public/imgs/mnist_result.png","hash":"836352838b8e8d5736b0f1d15371d326e55ed6ba","modified":1557313724065},{"_id":"public/imgs/surf_3dmax.png","hash":"e29f0ff7249d89d6a7c62fa6249ab36cb0f4205e","modified":1557313724065},{"_id":"public/imgs/surf_roughness_128.png","hash":"435a40961079504345286e199c450dd6caf151ac","modified":1557313724065},{"_id":"public/imgs/water.jpg","hash":"d70d3a35840bd6f4a785ceb68bc97459836b9886","modified":1557313724065},{"_id":"public/imgs/wetchat-rewardcode.jpg","hash":"37f65c6d09fca7a09dd10da6987268daa42203b4","modified":1557313724065},{"_id":"public/imgs/algorithm/string/rota_str2.png","hash":"169ddce44c2ae804501828b906bd9cece5a3d775","modified":1557313724065},{"_id":"public/css/font/font-awesome/FontAwesome.otf","hash":"42c179eef588854b5ec151bcf6a3f58aa8b79b11","modified":1557313724066},{"_id":"public/css/font/font-awesome/fontawesome-webfont.eot","hash":"986eed8dca049714e43eeebcb3932741a4bec76d","modified":1557313724066},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff2","hash":"638c652d623280a58144f93e7b552c66d1667a11","modified":1557313724066},{"_id":"public/css/font/font-awesome/fontawesome-webfont.woff","hash":"4a313eb93b959cc4154c684b915b0a31ddb68d84","modified":1557313724066},{"_id":"public/css/font/roboto/Roboto-Bold.ttf","hash":"47327df0f35e7cd7c8645874897a7449697544ae","modified":1557313724066},{"_id":"public/css/font/roboto/Roboto-Regular.ttf","hash":"824b5480c977a8166e177e5357d13164ccc45f47","modified":1557313724066},{"_id":"public/css/font/roboto/Roboto-Medium.ttf","hash":"6060ca726b9760b76f7c347dce9d2fa1fe42ec92","modified":1557313724067},{"_id":"public/css/font/roboto/Roboto-Light.ttf","hash":"e321c183e2b75ee19813892b7bac8d7c411cb88a","modified":1557313724067},{"_id":"public/css/lib/prettify-tomorrow-night-eighties.css","hash":"35e07bd7a4585363060edd558a0e9939e7e68323","modified":1557313724075},{"_id":"public/css/style.css","hash":"55a917def994d7b8bd35785f6b3d147435bc0b88","modified":1557313724075},{"_id":"public/imgs/conv_as_matrix.png","hash":"983cde6950719d5167edb8b1c647a63f897df7d5","modified":1557313724076},{"_id":"public/imgs/figure2.1.png","hash":"cd375642745e34ffde4aaa4031ad0819b9d64d43","modified":1557313724076},{"_id":"public/imgs/figure2.5.png","hash":"37752f80f83e750295334fc4cc72deace76c60d3","modified":1557313724076},{"_id":"public/imgs/figure2.4.png","hash":"0ff077e4226522b003ae45d9399f8505e2000156","modified":1557313724076},{"_id":"public/imgs/surf_roughness_256.png","hash":"f450d97d71a44c51dc4af9d2b7c73b9125240b28","modified":1557313724076},{"_id":"public/imgs/figure2.6.png","hash":"2b6a7a9394a9ede424487a3de96b8ff2febe7844","modified":1557313724076},{"_id":"public/imgs/algorithm/string/rota_str10.png","hash":"feb768182712b66df3ba47c626ffa8c2ae50dde7","modified":1557313724077},{"_id":"public/imgs/algorithm/string/rota_str4.png","hash":"5437c9889d8cd642d4e31a122efc8ee7abeab6e2","modified":1557313724077},{"_id":"public/imgs/algorithm/tree/level_tree2.png","hash":"bf2a0461ff3501f0f436c332e872afd932322ad7","modified":1557313724077},{"_id":"public/js/prettify.js","hash":"d24b1da342b5c2d0582f0922118aaf0b2a6840d5","modified":1557313724085},{"_id":"public/imgs/figure1.6.png","hash":"082f7688f09bca65b53b77e8dcb73492e3f6ef82","modified":1557313724085},{"_id":"public/imgs/figure2.3.png","hash":"8a21087ecf5badbcc1897715495e89a3d204a3ac","modified":1557313724085},{"_id":"public/imgs/figure2.2.png","hash":"e02638ef5be764c4f8be1168bbb555db1694f075","modified":1557313724085},{"_id":"public/imgs/algorithm/tree/level_tree3.png","hash":"7f3a2d38dd3f4bb79c713d1f88455bc114640108","modified":1557313724086},{"_id":"public/imgs/algorithm/string/rota_str3.png","hash":"cbc60e259bd9b649cce3b93479a620cb66fd84ab","modified":1557313724086},{"_id":"public/imgs/algorithm/string/rota_str6.png","hash":"e64e096d5ce26167180ccc2f3332a75050669def","modified":1557313724086},{"_id":"public/imgs/figure1.5.png","hash":"dfc7b73284a3bc732295eb915b557f0a8e7df92a","modified":1557313724093},{"_id":"public/imgs/algorithm/tree/level_tree.png","hash":"6fe7f8c02badf03b66d7f0ae1b39271ec2108396","modified":1557313724093},{"_id":"public/css/font/font-awesome/fontawesome-webfont.ttf","hash":"6484f1af6b485d5096b71b344e67f4164c33dd1f","modified":1557313724093},{"_id":"public/css/lib/font-awesome.min.css","hash":"683d12731b7429d32ec7de00a6706602e403013f","modified":1557313724098},{"_id":"public/imgs/histequa.png","hash":"89eb9d30577401536f20ab07ae8e5647ce79e867","modified":1557313724098},{"_id":"public/imgs/conv_as_matrix_2.png","hash":"e97001c70d7a66fa8df60a93df5d52982de843df","modified":1557313724104},{"_id":"public/imgs/orange.jpg","hash":"0c112866bd377b63b171771422d6b20b997d7e46","modified":1557313724104},{"_id":"public/imgs/algorithm/string/rota_str7.png","hash":"c3504ff240f1a5964954c3b39cf5ed3e0a34a775","modified":1557313724104},{"_id":"public/css/font/font-awesome/fontawesome-webfont.svg","hash":"550ef5c1253c8376f2ead32b654eb58d3c106ca3","modified":1557313724104},{"_id":"public/imgs/algorithm/string/rota_str9.png","hash":"6e19ad313c849c3c692ba46c091fd4c8765aa76a","modified":1557313724105},{"_id":"public/imgs/algorithm/string/rota_str5.png","hash":"48ed77eb529a4fa605be982ec03cb9d776ab71bd","modified":1557313724105},{"_id":"public/imgs/algorithm/string/rota_str8.png","hash":"ddd356d7e348611ad9f8bda46e32db52873135bf","modified":1557313724105},{"_id":"public/imgs/light.jpg","hash":"75230c1891009503598822c19d41e4a85e211244","modified":1557313724113},{"_id":"public/imgs/50mm-106-68-ans.png","hash":"b02f3c530b8275a198f2624444f3c6b2a7740fb4","modified":1557313724118},{"_id":"public/imgs/50mm-106-68.png","hash":"391095396ddd75705ce82f1a74ee225557860764","modified":1557313724118},{"_id":"public/imgs/all_day.jpg","hash":"e25f7319e8c3a178dffc6d93676bd184e4234926","modified":1557313724119},{"_id":"public/imgs/water-circle.jpg","hash":"f8cc3871a46549b703063c864b764e22ec3d13c2","modified":1557313724121},{"_id":"public/imgs/game.png","hash":"ca05a21991384b3fe0eaf3f529b1fbfb09439e10","modified":1557313724122},{"_id":"public/js/jquery.min.js","hash":"69bb69e25ca7d5ef0935317584e6153f3fd9a88c","modified":1557313724126},{"_id":"public/imgs/sky-night.jpg","hash":"1c15377e38819ad6c8bc4feb25d9a98a8b679c8e","modified":1557313724129},{"_id":"public/css/lib/materialize.min.css","hash":"0f3f6e0cc632835eca104efc5c26dfc5f4d7e5f1","modified":1557313724131},{"_id":"public/imgs/img-cov.png","hash":"71f7616a6e54befc3d7019b6e0b85e1bfb7cc784","modified":1557313724131},{"_id":"public/js/materialize.min.js","hash":"c9308fbe808a149aa11061af40a4be5f391cccee","modified":1557313724134},{"_id":"public/imgs/wall.png","hash":"d47ce4e9c164c9563cd230767bbc29f29a738981","modified":1557313724136},{"_id":"public/imgs/winter.jpg","hash":"d62f8b169f87e09f46884582b5652e78834433c4","modified":1557313724142},{"_id":"public/imgs/rain.png","hash":"5d382d60591561923d178dd24d89cb1189b193f6","modified":1557313724145},{"_id":"public/imgs/coloreggs.jpg","hash":"518341c5e974a98f8bc00b9e5972954071bf645a","modified":1557313724147}],"Category":[{"name":"技术文章","_id":"cjvf4dqpu0003lnvvqxuqbq4s"},{"name":"基础算法","_id":"cjvf4dqpz0008lnvvd749vsg9"},{"name":"数学基础","_id":"cjvf4dqq3000clnvv10w9pbon"},{"name":"图像处理","_id":"cjvf4dqq6000ilnvvdvnf6my1"},{"name":"博客搭建","parent":"cjvf4dqpu0003lnvvqxuqbq4s","_id":"cjvf4dqq7000mlnvvafa7bpkz"},{"name":"树","parent":"cjvf4dqpz0008lnvvd749vsg9","_id":"cjvf4dqqb000xlnvvc6efwab4"},{"name":"周总结","_id":"cjvf4dqqc0012lnvvu3getomw"},{"name":"随机过程","parent":"cjvf4dqq3000clnvv10w9pbon","_id":"cjvf4dqqd0018lnvvqcczxa40"},{"name":"图像增强","parent":"cjvf4dqq6000ilnvvdvnf6my1","_id":"cjvf4dqqg001plnvvu94qaz3c"},{"name":"研磨&粗糙度","_id":"cjvf4dqqg001tlnvvgy0o05q8"},{"name":"字符串","parent":"cjvf4dqpz0008lnvvd749vsg9","_id":"cjvf4dqqh001zlnvvlwd2w2ms"},{"name":"研磨表面仿真","parent":"cjvf4dqqg001tlnvvgy0o05q8","_id":"cjvf4dqqi0028lnvvhunxoxvx"},{"name":"tensorflow学习","_id":"cjvf4dqra0041lnvvv03zzox1"},{"name":"tensorflow-Demo","parent":"cjvf4dqra0041lnvvv03zzox1","_id":"cjvf4dqre0045lnvv822mcibm"},{"name":"深度学习","_id":"cjvf4dqs3004hlnvvvvoipfhs"},{"name":"深度学习基础","parent":"cjvf4dqs3004hlnvvvvoipfhs","_id":"cjvf4dqs4004klnvv6laregtv"}],"Data":[{"_id":"about","data":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]}},{"_id":"hint","data":{"new":{"selector":[".menu-reading"]}}},{"_id":"link","data":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}}},{"_id":"reading","data":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}}},{"_id":"slider","data":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}],"Page":[{"title":"关于","layout":"about","_content":"大家好，我是tech.radish。欢迎来到我的个人技术博客。\n","source":"about/index.md","raw":"title: 关于\nlayout: about\n---\n大家好，我是tech.radish。欢迎来到我的个人技术博客。\n","date":"2019-05-08T10:50:25.487Z","updated":"2019-05-08T10:50:25.487Z","path":"about/index.html","comments":1,"_id":"cjvf4dqps0001lnvvflfpt81p","content":"<p>大家好，我是tech.radish。欢迎来到我的个人技术博客。</p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"","more":"<p>大家好，我是tech.radish。欢迎来到我的个人技术博客。</p>\n"},{"title":"读书","layout":"reading","_content":"# 我想读 #\n\n书籍1\n\n----------\n\n书籍2\n","source":"reading/index.md","raw":"title: 读书\nlayout: reading\n---\n# 我想读 #\n\n书籍1\n\n----------\n\n书籍2\n","date":"2019-05-08T10:50:25.655Z","updated":"2019-05-08T10:50:25.655Z","path":"reading/index.html","comments":1,"_id":"cjvf4dqr9003zlnvvxsfdylit","content":"<h1 id=\"我想读\"><a href=\"#我想读\" class=\"headerlink\" title=\"我想读\"></a>我想读</h1><p>书籍1</p>\n<hr>\n<p>书籍2</p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"","more":"<h1 id=\"我想读\"><a href=\"#我想读\" class=\"headerlink\" title=\"我想读\"></a>我想读</h1><p>书籍1</p>\n<hr>\n<p>书籍2</p>\n"}],"Post":[{"title":"Hexo多电脑同步写作","date":"2018-03-22T13:01:20.000Z","_content":"利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明\n\n# 1.上传文件到仓库 #\n前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。\n首先你要明白，你创建的博客通过`hexo d`命令部署到github之后,和你的本地博客根目录下的`.deploy_git`文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。\n\n<!-- more -->\n\n1. 首先在你Github账户上新建一个仓库，例如名为`hexo-blog`\n2. 将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。\n\t1. scaffolds\n\t2. source\n\t3. themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，`ls -a && rm .git`）\n\t4. _config.yml\n\t5. package.json\n3.  在hexo-blog目录中执行\n\t1.  `git init`\n\t2.  `git add .`\n\t3.  `git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`（使用你新建的仓库的SSH地址）\n\t4.  `git commit -m 'blog source bakcup'`(commit之后才能创建分支)\n\t5.  `git branch hexo`创建一个hexo分支\n\t6.  `git checkout hexo`切换到hexo分支\n\t5.  `git push origin hexo`\n4. 在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。\n\n# 2. 下载文件 #\n上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。\n\n1. 进入到你放置博客的目录中，然后执行`git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`\n2. `cd hexo-blog`进入此仓库目录中\n3. 执行`npm install`安装所需组件\n4. 使用`hexo g && hexo s -p 8080` 在本地打开浏览器输入`localhost:8080` 查看与在线的博客是否一致。\n5. 使用`hexo new \"page name\"`新建一片博客，写完一篇博客，然后部署`hexo clean && hexo g && hexo d`,再执行以下命令来完成同步\n\t1. `git add .`\n\t2. `git commit -m 'add a new page'`\n\t3. `git push origin hexo`\n6. 此时就可以在你原先电脑上执行`git pull origin hexo`来完成同步了。 \n\n# 留言 #\n如果还有不懂请在下面留言，我会及时回复。","source":"_posts/Hexo多电脑同步写作.md","raw":"---\ntitle: Hexo多电脑同步写作\ndate: 2018-03-22 21:01:20\ncategories:\n- 技术文章\n- 博客搭建\ntags:\n- hexo\n- 多电脑同步\n---\n利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明\n\n# 1.上传文件到仓库 #\n前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。\n首先你要明白，你创建的博客通过`hexo d`命令部署到github之后,和你的本地博客根目录下的`.deploy_git`文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。\n\n<!-- more -->\n\n1. 首先在你Github账户上新建一个仓库，例如名为`hexo-blog`\n2. 将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。\n\t1. scaffolds\n\t2. source\n\t3. themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，`ls -a && rm .git`）\n\t4. _config.yml\n\t5. package.json\n3.  在hexo-blog目录中执行\n\t1.  `git init`\n\t2.  `git add .`\n\t3.  `git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`（使用你新建的仓库的SSH地址）\n\t4.  `git commit -m 'blog source bakcup'`(commit之后才能创建分支)\n\t5.  `git branch hexo`创建一个hexo分支\n\t6.  `git checkout hexo`切换到hexo分支\n\t5.  `git push origin hexo`\n4. 在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。\n\n# 2. 下载文件 #\n上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。\n\n1. 进入到你放置博客的目录中，然后执行`git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git`\n2. `cd hexo-blog`进入此仓库目录中\n3. 执行`npm install`安装所需组件\n4. 使用`hexo g && hexo s -p 8080` 在本地打开浏览器输入`localhost:8080` 查看与在线的博客是否一致。\n5. 使用`hexo new \"page name\"`新建一片博客，写完一篇博客，然后部署`hexo clean && hexo g && hexo d`,再执行以下命令来完成同步\n\t1. `git add .`\n\t2. `git commit -m 'add a new page'`\n\t3. `git push origin hexo`\n6. 此时就可以在你原先电脑上执行`git pull origin hexo`来完成同步了。 \n\n# 留言 #\n如果还有不懂请在下面留言，我会及时回复。","slug":"Hexo多电脑同步写作","published":1,"updated":"2019-05-08T10:50:25.480Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqpo0000lnvvkd0ws8cv","content":"<p>利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明</p>\n<h1 id=\"1-上传文件到仓库\"><a href=\"#1-上传文件到仓库\" class=\"headerlink\" title=\"1.上传文件到仓库\"></a>1.上传文件到仓库</h1><p>前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。<br>首先你要明白，你创建的博客通过<code>hexo d</code>命令部署到github之后,和你的本地博客根目录下的<code>.deploy_git</code>文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。</p>\n<a id=\"more\"></a>\n<ol>\n<li>首先在你Github账户上新建一个仓库，例如名为<code>hexo-blog</code></li>\n<li>将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。<ol>\n<li>scaffolds</li>\n<li>source</li>\n<li>themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，<code>ls -a &amp;&amp; rm .git</code>）</li>\n<li>_config.yml</li>\n<li>package.json</li>\n</ol>\n</li>\n<li>在hexo-blog目录中执行<ol>\n<li><code>git init</code></li>\n<li><code>git add .</code></li>\n<li><code>git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code>（使用你新建的仓库的SSH地址）</li>\n<li><code>git commit -m &#39;blog source bakcup&#39;</code>(commit之后才能创建分支)</li>\n<li><code>git branch hexo</code>创建一个hexo分支</li>\n<li><code>git checkout hexo</code>切换到hexo分支</li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。</li>\n</ol>\n<h1 id=\"2-下载文件\"><a href=\"#2-下载文件\" class=\"headerlink\" title=\"2. 下载文件\"></a>2. 下载文件</h1><p>上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。</p>\n<ol>\n<li>进入到你放置博客的目录中，然后执行<code>git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code></li>\n<li><code>cd hexo-blog</code>进入此仓库目录中</li>\n<li>执行<code>npm install</code>安装所需组件</li>\n<li>使用<code>hexo g &amp;&amp; hexo s -p 8080</code> 在本地打开浏览器输入<code>localhost:8080</code> 查看与在线的博客是否一致。</li>\n<li>使用<code>hexo new &quot;page name&quot;</code>新建一片博客，写完一篇博客，然后部署<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code>,再执行以下命令来完成同步<ol>\n<li><code>git add .</code></li>\n<li><code>git commit -m &#39;add a new page&#39;</code></li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>此时就可以在你原先电脑上执行<code>git pull origin hexo</code>来完成同步了。 </li>\n</ol>\n<h1 id=\"留言\"><a href=\"#留言\" class=\"headerlink\" title=\"留言\"></a>留言</h1><p>如果还有不懂请在下面留言，我会及时回复。</p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>利用Hexo安装完博客之后，如何实现多电脑写作捏？下面分几步来说明</p>\n<h1 id=\"1-上传文件到仓库\"><a href=\"#1-上传文件到仓库\" class=\"headerlink\" title=\"1.上传文件到仓库\"></a>1.上传文件到仓库</h1><p>前提是已经安装好Git客户端。这个应该在你安装博客的时候就已经安装好了吧。不会的话，百度下下载链接安装就好。<br>首先你要明白，你创建的博客通过<code>hexo d</code>命令部署到github之后,和你的本地博客根目录下的<code>.deploy_git</code>文件夹中的目录结构是一样的，所以，这只能算是个web工程，若要想实现多客户端写作的话，需要通过下面的步骤。</p>","more":"<ol>\n<li>首先在你Github账户上新建一个仓库，例如名为<code>hexo-blog</code></li>\n<li>将本地博客根目录下的5个文件分别copy到一个新文件夹(例：hexo-blog)里面。<ol>\n<li>scaffolds</li>\n<li>source</li>\n<li>themes（记得删除你下载主题的.git目录，它通常是隐藏的，需要取消隐藏之后删除，或者使用Git客户端来删除，<code>ls -a &amp;&amp; rm .git</code>）</li>\n<li>_config.yml</li>\n<li>package.json</li>\n</ol>\n</li>\n<li>在hexo-blog目录中执行<ol>\n<li><code>git init</code></li>\n<li><code>git add .</code></li>\n<li><code>git remote add origin git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code>（使用你新建的仓库的SSH地址）</li>\n<li><code>git commit -m &#39;blog source bakcup&#39;</code>(commit之后才能创建分支)</li>\n<li><code>git branch hexo</code>创建一个hexo分支</li>\n<li><code>git checkout hexo</code>切换到hexo分支</li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>在第2步中，起始可以直接执行第3步的命令，也即可以不用复制那5个文件到新的目录中，只是因为那5个目录是必须的，其他的都是次要的。</li>\n</ol>\n<h1 id=\"2-下载文件\"><a href=\"#2-下载文件\" class=\"headerlink\" title=\"2. 下载文件\"></a>2. 下载文件</h1><p>上一步已经将你本地的博客托管到了github仓库中，接下来需要在你另一台需要写博客的电脑中，安装Node.js（这个自行百度吧，直接next安装即可）然后执行clone命令即可。</p>\n<ol>\n<li>进入到你放置博客的目录中，然后执行<code>git clone -b hexo git@github@你github用户名/hexo-blog(换成你仓库名).github.io.git</code></li>\n<li><code>cd hexo-blog</code>进入此仓库目录中</li>\n<li>执行<code>npm install</code>安装所需组件</li>\n<li>使用<code>hexo g &amp;&amp; hexo s -p 8080</code> 在本地打开浏览器输入<code>localhost:8080</code> 查看与在线的博客是否一致。</li>\n<li>使用<code>hexo new &quot;page name&quot;</code>新建一片博客，写完一篇博客，然后部署<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code>,再执行以下命令来完成同步<ol>\n<li><code>git add .</code></li>\n<li><code>git commit -m &#39;add a new page&#39;</code></li>\n<li><code>git push origin hexo</code></li>\n</ol>\n</li>\n<li>此时就可以在你原先电脑上执行<code>git pull origin hexo</code>来完成同步了。 </li>\n</ol>\n<h1 id=\"留言\"><a href=\"#留言\" class=\"headerlink\" title=\"留言\"></a>留言</h1><p>如果还有不懂请在下面留言，我会及时回复。</p>"},{"title":"二叉树按层打印","date":"2018-03-25T02:17:19.000Z","_content":"# 题目 #\n有一棵二叉树，请设计一个算法，按照层次打印这棵二叉树。\n\n给定二叉树的根结点root，请返回打印结果，结果按照每一层一个数组进行储存，所有数组的顺序按照层数从上往下，且每一层的数组内元素按照从左往右排列。保证结点数小于等于500。\n\n<!-- more -->\n\n# 思路 #\n使用队列和两个变量，如图（其实需要一个变量来指向当前打印的元素）。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png)\n\n首先last=root（1），将root（1）入队，然后出队并打印，再用一个变量（temp）记录打印的元素。然后将root（1）的两个孩子（2和3）结点分别入队，入队的同时，让nlast指向两个孩子结点，即入队就更新nlast。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png)\n\n然后此时比较打印的元素（1）是否和last指向的元素（1）相同，若相同表示要换行了（因为last始终指向正在打印的当前行最右的元素），则更新last=nlast（last指向结点3），然后将结点2出队并打印，让temp更新并记录结点2，并将结点2的孩子结点（4）入队，并将nlast指向结点2的孩子结点（4），然后将结点（3）出队并打印，让temp更新并记录结点3，让结点（3）的孩子结点（5和6）入队，此时比较temp结点和last指向的结点（3）是否相同，相同则更新last=nlast，依次下去，即可完成打印。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree3.png)\n\n# Java代码 #\n\timport java.util.*;\n\t\n\t\n\tpublic class TreePrinter {\n\t\t// 结点定义\n\t\tclass TreeNode {\n\t\t    int val = 0;\n\t\t    TreeNode left = null;\n\t\t    TreeNode right = null;\n\t\t    public TreeNode(int val) {\n\t\t        this.val = val;\n\t\t    }\n\t\t}    \n\t\tpublic int[][] printTree(TreeNode root) {\n\t\t\t// 在Java中LinkedList实现了Queue接口。\n\t        Queue<TreeNode> queue = new LinkedList<>();\n\t        TreeNode last, nlast = null;\n\t        last = root; // 初始化last结点\n\t        queue.add(last);  // 根节点入队\n\t\t\t// 结果要返回int[][],但不清楚需要几行几列，所以需要用到集合，这里相当于定义了一个二维数组\n\t\t\t// <>表示泛型，表示这个集合只能装Integer类型，可以忽略，因为编译之后泛型就没了\n\t        List<List<Integer>> res = new ArrayList<>();\n\t\t\t// 定义每一行,即row作为res的元素\n\t        List<Integer> row = new ArrayList<>();\n\t\t\t// 通过上面分析可以看到循环的条件是队列不为空，因为初始已经将根节点入队了\n\t        while (!queue.isEmpty()) {\n\t\t\t\t// 结点首先出队，并用temp变量记录。\n\t            TreeNode temp = queue.poll();\n\t\t\t\t// 这个可以使用不换行的打印，为了满足题目的返回条件，这里将它加入到行集合中\n\t\t\t\trow.add(temp.val);\n\t\t\t\t// 出队元素的左孩子不为空，入队并跟新nlast\n\t            if (temp.left != null) {\n\t                nlast = temp.left;\n\t                queue.add(nlast);\n\t            }\n\t\t\t\t// 出队元素的右孩子不为空，入队并跟新nlast\n\t            if (temp.right != null) {\n\t                nlast = temp.right;\n\t                queue.add(nlast);\n\t            }\n\t\t\t\t// 如果已经出队并被打印的元素和last指向的元素相同\n\t\t\t\t// 则需要换行了，并更新last=nlast\n\t            if (temp.val == last.val) {\n\t                last = nlast;\n\t\t\t\t\t// 将这一行加入到结果集合中\n\t                res.add(row);\n\t\t\t\t\t// 清空这一行,这里不能用row.clear()，需要重新new一个对象出来。\n\t                row = new ArrayList<>();\n\t            }\n\t        }\n\t\t\t// 到此结果就被遍历出来了，下面构造符合题目返回结果的元素。\n\t\t\t// 首先开辟多上行的元素，列是空的\n\t        int [][] ans = new int[res.size()][];\n\t        for (int i = 0; i < res.size(); i ++){\n\t\t\t\t// 元素对每一行开辟多少列\n\t            ans[i] = new int[res.get(i).size()];\n\t            for (int j = 0; j < res.get(i).size(); j++) {\n\t\t\t\t\t// 完成赋值\n\t                ans[i][j] = res.get(i).get(j);\n\t\t\t\t\t// 测试使用\n\t\t\t\t\t//System.out.print(ans[i][j]);\n\t            }\n\t\t\t\t// 测试使用\n\t\t\t\t//System.out.println();\n\t        }\n\t\t\t//返回结果\n\t        return ans;\n\t    }\n\t}\n\n# 测试程序 #\n\tpublic static void main(String[] args) {\n        TreeNode root = new TreeNode(1);\n        root.left = new TreeNode(2);\n        root.right = new TreeNode(3);\n        root.left.left = new TreeNode(4);\n        root.right.left = new TreeNode(5);\n        root.right.right = new TreeNode(6);\n        root.right.left.left = new TreeNode(7);\n        root.right.left.right = new TreeNode(8);\n        printTree(root);\n    }\n\n# Python代码 #\n\t# -*- coding:utf-8 -*-\n\t# 结点定义 \n\tclass TreeNode:\n\t     def __init__(self, x):\n\t         self.val = x\n\t         self.left = None\n\t         self.right = None\n\t\n\tclass TreePrinter:\n\t    def printTree(self, root):\n\t\t\t# 结果定义\n\t        res=[]  \n \t\t\t# 队列（使用列表可以模拟队列）\n\t        queue=[] \n\t\t\t# 判空\n\t        if root==None:\n\t            return res\n\t\t\t# 初始化结点\n\t        last=nlast=root\n\t\t\t# 入队使用append()函数\n\t        queue.append(root)\n\t\t\t# 一行，相当于Java代码中的row数组\n\t        row=[]\n\t\t\t# 队列不为空用循环\n\t        while len(queue):\n\t\t\t\t# 出队，并用temp结点来记录，使用pop(0)函数，即弹出列表的第一个元素（队首元素）\n\t            temp=queue.pop(0)\n\t\t\t\t# 入队\n\t            row.append(temp.val)\n\t\t\t\t# 如果出队元素的左右孩子不为空，则入队并更新nlast\n\t            if temp.left!=None:\n\t                queue.append(temp.left)\n\t                nlast=temp.left\n\t            if temp.right!=None:\n\t                queue.append(temp.right)\n\t                nlast=temp.right\n\t\t\t\t# 如果出队元素和last指向的元素相同，则完成一行的打印\n\t            if temp==last:\n\t\t\t\t\t# 加入到结果中row[:]表示将row中所有元素放入到res中,是一个列表\n\t                res.append(row[:])\n\t\t\t\t\t# 清空这一行\n\t                row=[]\n\t\t\t\t\t# 更新last指向nlast\n\t                last=nlast\n\t         return res","source":"_posts/二叉树按层打印.md","raw":"---\ntitle: 二叉树按层打印\ndate: 2018-03-25 10:17:19\ncategories:\n- 基础算法\n- 树\ntags:\n- 二叉树\n- 层次遍历\n- java实现\n- Python实现\n---\n# 题目 #\n有一棵二叉树，请设计一个算法，按照层次打印这棵二叉树。\n\n给定二叉树的根结点root，请返回打印结果，结果按照每一层一个数组进行储存，所有数组的顺序按照层数从上往下，且每一层的数组内元素按照从左往右排列。保证结点数小于等于500。\n\n<!-- more -->\n\n# 思路 #\n使用队列和两个变量，如图（其实需要一个变量来指向当前打印的元素）。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png)\n\n首先last=root（1），将root（1）入队，然后出队并打印，再用一个变量（temp）记录打印的元素。然后将root（1）的两个孩子（2和3）结点分别入队，入队的同时，让nlast指向两个孩子结点，即入队就更新nlast。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png)\n\n然后此时比较打印的元素（1）是否和last指向的元素（1）相同，若相同表示要换行了（因为last始终指向正在打印的当前行最右的元素），则更新last=nlast（last指向结点3），然后将结点2出队并打印，让temp更新并记录结点2，并将结点2的孩子结点（4）入队，并将nlast指向结点2的孩子结点（4），然后将结点（3）出队并打印，让temp更新并记录结点3，让结点（3）的孩子结点（5和6）入队，此时比较temp结点和last指向的结点（3）是否相同，相同则更新last=nlast，依次下去，即可完成打印。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree3.png)\n\n# Java代码 #\n\timport java.util.*;\n\t\n\t\n\tpublic class TreePrinter {\n\t\t// 结点定义\n\t\tclass TreeNode {\n\t\t    int val = 0;\n\t\t    TreeNode left = null;\n\t\t    TreeNode right = null;\n\t\t    public TreeNode(int val) {\n\t\t        this.val = val;\n\t\t    }\n\t\t}    \n\t\tpublic int[][] printTree(TreeNode root) {\n\t\t\t// 在Java中LinkedList实现了Queue接口。\n\t        Queue<TreeNode> queue = new LinkedList<>();\n\t        TreeNode last, nlast = null;\n\t        last = root; // 初始化last结点\n\t        queue.add(last);  // 根节点入队\n\t\t\t// 结果要返回int[][],但不清楚需要几行几列，所以需要用到集合，这里相当于定义了一个二维数组\n\t\t\t// <>表示泛型，表示这个集合只能装Integer类型，可以忽略，因为编译之后泛型就没了\n\t        List<List<Integer>> res = new ArrayList<>();\n\t\t\t// 定义每一行,即row作为res的元素\n\t        List<Integer> row = new ArrayList<>();\n\t\t\t// 通过上面分析可以看到循环的条件是队列不为空，因为初始已经将根节点入队了\n\t        while (!queue.isEmpty()) {\n\t\t\t\t// 结点首先出队，并用temp变量记录。\n\t            TreeNode temp = queue.poll();\n\t\t\t\t// 这个可以使用不换行的打印，为了满足题目的返回条件，这里将它加入到行集合中\n\t\t\t\trow.add(temp.val);\n\t\t\t\t// 出队元素的左孩子不为空，入队并跟新nlast\n\t            if (temp.left != null) {\n\t                nlast = temp.left;\n\t                queue.add(nlast);\n\t            }\n\t\t\t\t// 出队元素的右孩子不为空，入队并跟新nlast\n\t            if (temp.right != null) {\n\t                nlast = temp.right;\n\t                queue.add(nlast);\n\t            }\n\t\t\t\t// 如果已经出队并被打印的元素和last指向的元素相同\n\t\t\t\t// 则需要换行了，并更新last=nlast\n\t            if (temp.val == last.val) {\n\t                last = nlast;\n\t\t\t\t\t// 将这一行加入到结果集合中\n\t                res.add(row);\n\t\t\t\t\t// 清空这一行,这里不能用row.clear()，需要重新new一个对象出来。\n\t                row = new ArrayList<>();\n\t            }\n\t        }\n\t\t\t// 到此结果就被遍历出来了，下面构造符合题目返回结果的元素。\n\t\t\t// 首先开辟多上行的元素，列是空的\n\t        int [][] ans = new int[res.size()][];\n\t        for (int i = 0; i < res.size(); i ++){\n\t\t\t\t// 元素对每一行开辟多少列\n\t            ans[i] = new int[res.get(i).size()];\n\t            for (int j = 0; j < res.get(i).size(); j++) {\n\t\t\t\t\t// 完成赋值\n\t                ans[i][j] = res.get(i).get(j);\n\t\t\t\t\t// 测试使用\n\t\t\t\t\t//System.out.print(ans[i][j]);\n\t            }\n\t\t\t\t// 测试使用\n\t\t\t\t//System.out.println();\n\t        }\n\t\t\t//返回结果\n\t        return ans;\n\t    }\n\t}\n\n# 测试程序 #\n\tpublic static void main(String[] args) {\n        TreeNode root = new TreeNode(1);\n        root.left = new TreeNode(2);\n        root.right = new TreeNode(3);\n        root.left.left = new TreeNode(4);\n        root.right.left = new TreeNode(5);\n        root.right.right = new TreeNode(6);\n        root.right.left.left = new TreeNode(7);\n        root.right.left.right = new TreeNode(8);\n        printTree(root);\n    }\n\n# Python代码 #\n\t# -*- coding:utf-8 -*-\n\t# 结点定义 \n\tclass TreeNode:\n\t     def __init__(self, x):\n\t         self.val = x\n\t         self.left = None\n\t         self.right = None\n\t\n\tclass TreePrinter:\n\t    def printTree(self, root):\n\t\t\t# 结果定义\n\t        res=[]  \n \t\t\t# 队列（使用列表可以模拟队列）\n\t        queue=[] \n\t\t\t# 判空\n\t        if root==None:\n\t            return res\n\t\t\t# 初始化结点\n\t        last=nlast=root\n\t\t\t# 入队使用append()函数\n\t        queue.append(root)\n\t\t\t# 一行，相当于Java代码中的row数组\n\t        row=[]\n\t\t\t# 队列不为空用循环\n\t        while len(queue):\n\t\t\t\t# 出队，并用temp结点来记录，使用pop(0)函数，即弹出列表的第一个元素（队首元素）\n\t            temp=queue.pop(0)\n\t\t\t\t# 入队\n\t            row.append(temp.val)\n\t\t\t\t# 如果出队元素的左右孩子不为空，则入队并更新nlast\n\t            if temp.left!=None:\n\t                queue.append(temp.left)\n\t                nlast=temp.left\n\t            if temp.right!=None:\n\t                queue.append(temp.right)\n\t                nlast=temp.right\n\t\t\t\t# 如果出队元素和last指向的元素相同，则完成一行的打印\n\t            if temp==last:\n\t\t\t\t\t# 加入到结果中row[:]表示将row中所有元素放入到res中,是一个列表\n\t                res.append(row[:])\n\t\t\t\t\t# 清空这一行\n\t                row=[]\n\t\t\t\t\t# 更新last指向nlast\n\t                last=nlast\n\t         return res","slug":"二叉树按层打印","published":1,"updated":"2019-05-08T10:50:25.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqpt0002lnvv28dpdcny","content":"<h1 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h1><p>有一棵二叉树，请设计一个算法，按照层次打印这棵二叉树。</p>\n<p>给定二叉树的根结点root，请返回打印结果，结果按照每一层一个数组进行储存，所有数组的顺序按照层数从上往下，且每一层的数组内元素按照从左往右排列。保证结点数小于等于500。</p>\n<a id=\"more\"></a>\n<h1 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h1><p>使用队列和两个变量，如图（其实需要一个变量来指向当前打印的元素）。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png\" alt></p>\n<p>首先last=root（1），将root（1）入队，然后出队并打印，再用一个变量（temp）记录打印的元素。然后将root（1）的两个孩子（2和3）结点分别入队，入队的同时，让nlast指向两个孩子结点，即入队就更新nlast。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png\" alt></p>\n<p>然后此时比较打印的元素（1）是否和last指向的元素（1）相同，若相同表示要换行了（因为last始终指向正在打印的当前行最右的元素），则更新last=nlast（last指向结点3），然后将结点2出队并打印，让temp更新并记录结点2，并将结点2的孩子结点（4）入队，并将nlast指向结点2的孩子结点（4），然后将结点（3）出队并打印，让temp更新并记录结点3，让结点（3）的孩子结点（5和6）入队，此时比较temp结点和last指向的结点（3）是否相同，相同则更新last=nlast，依次下去，即可完成打印。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree3.png\" alt></p>\n<h1 id=\"Java代码\"><a href=\"#Java代码\" class=\"headerlink\" title=\"Java代码\"></a>Java代码</h1><pre><code>import java.util.*;\n\n\npublic class TreePrinter {\n    // 结点定义\n    class TreeNode {\n        int val = 0;\n        TreeNode left = null;\n        TreeNode right = null;\n        public TreeNode(int val) {\n            this.val = val;\n        }\n    }    \n    public int[][] printTree(TreeNode root) {\n        // 在Java中LinkedList实现了Queue接口。\n        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();\n        TreeNode last, nlast = null;\n        last = root; // 初始化last结点\n        queue.add(last);  // 根节点入队\n        // 结果要返回int[][],但不清楚需要几行几列，所以需要用到集合，这里相当于定义了一个二维数组\n        // &lt;&gt;表示泛型，表示这个集合只能装Integer类型，可以忽略，因为编译之后泛型就没了\n        List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();\n        // 定义每一行,即row作为res的元素\n        List&lt;Integer&gt; row = new ArrayList&lt;&gt;();\n        // 通过上面分析可以看到循环的条件是队列不为空，因为初始已经将根节点入队了\n        while (!queue.isEmpty()) {\n            // 结点首先出队，并用temp变量记录。\n            TreeNode temp = queue.poll();\n            // 这个可以使用不换行的打印，为了满足题目的返回条件，这里将它加入到行集合中\n            row.add(temp.val);\n            // 出队元素的左孩子不为空，入队并跟新nlast\n            if (temp.left != null) {\n                nlast = temp.left;\n                queue.add(nlast);\n            }\n            // 出队元素的右孩子不为空，入队并跟新nlast\n            if (temp.right != null) {\n                nlast = temp.right;\n                queue.add(nlast);\n            }\n            // 如果已经出队并被打印的元素和last指向的元素相同\n            // 则需要换行了，并更新last=nlast\n            if (temp.val == last.val) {\n                last = nlast;\n                // 将这一行加入到结果集合中\n                res.add(row);\n                // 清空这一行,这里不能用row.clear()，需要重新new一个对象出来。\n                row = new ArrayList&lt;&gt;();\n            }\n        }\n        // 到此结果就被遍历出来了，下面构造符合题目返回结果的元素。\n        // 首先开辟多上行的元素，列是空的\n        int [][] ans = new int[res.size()][];\n        for (int i = 0; i &lt; res.size(); i ++){\n            // 元素对每一行开辟多少列\n            ans[i] = new int[res.get(i).size()];\n            for (int j = 0; j &lt; res.get(i).size(); j++) {\n                // 完成赋值\n                ans[i][j] = res.get(i).get(j);\n                // 测试使用\n                //System.out.print(ans[i][j]);\n            }\n            // 测试使用\n            //System.out.println();\n        }\n        //返回结果\n        return ans;\n    }\n}\n</code></pre><h1 id=\"测试程序\"><a href=\"#测试程序\" class=\"headerlink\" title=\"测试程序\"></a>测试程序</h1><pre><code>public static void main(String[] args) {\n    TreeNode root = new TreeNode(1);\n    root.left = new TreeNode(2);\n    root.right = new TreeNode(3);\n    root.left.left = new TreeNode(4);\n    root.right.left = new TreeNode(5);\n    root.right.right = new TreeNode(6);\n    root.right.left.left = new TreeNode(7);\n    root.right.left.right = new TreeNode(8);\n    printTree(root);\n}\n</code></pre><h1 id=\"Python代码\"><a href=\"#Python代码\" class=\"headerlink\" title=\"Python代码\"></a>Python代码</h1><pre><code># -*- coding:utf-8 -*-\n# 结点定义 \nclass TreeNode:\n     def __init__(self, x):\n         self.val = x\n         self.left = None\n         self.right = None\n\nclass TreePrinter:\n    def printTree(self, root):\n        # 结果定义\n        res=[]  \n         # 队列（使用列表可以模拟队列）\n        queue=[] \n        # 判空\n        if root==None:\n            return res\n        # 初始化结点\n        last=nlast=root\n        # 入队使用append()函数\n        queue.append(root)\n        # 一行，相当于Java代码中的row数组\n        row=[]\n        # 队列不为空用循环\n        while len(queue):\n            # 出队，并用temp结点来记录，使用pop(0)函数，即弹出列表的第一个元素（队首元素）\n            temp=queue.pop(0)\n            # 入队\n            row.append(temp.val)\n            # 如果出队元素的左右孩子不为空，则入队并更新nlast\n            if temp.left!=None:\n                queue.append(temp.left)\n                nlast=temp.left\n            if temp.right!=None:\n                queue.append(temp.right)\n                nlast=temp.right\n            # 如果出队元素和last指向的元素相同，则完成一行的打印\n            if temp==last:\n                # 加入到结果中row[:]表示将row中所有元素放入到res中,是一个列表\n                res.append(row[:])\n                # 清空这一行\n                row=[]\n                # 更新last指向nlast\n                last=nlast\n         return res\n</code></pre>","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<h1 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h1><p>有一棵二叉树，请设计一个算法，按照层次打印这棵二叉树。</p>\n<p>给定二叉树的根结点root，请返回打印结果，结果按照每一层一个数组进行储存，所有数组的顺序按照层数从上往下，且每一层的数组内元素按照从左往右排列。保证结点数小于等于500。</p>","more":"<h1 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h1><p>使用队列和两个变量，如图（其实需要一个变量来指向当前打印的元素）。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png\" alt></p>\n<p>首先last=root（1），将root（1）入队，然后出队并打印，再用一个变量（temp）记录打印的元素。然后将root（1）的两个孩子（2和3）结点分别入队，入队的同时，让nlast指向两个孩子结点，即入队就更新nlast。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree.png\" alt></p>\n<p>然后此时比较打印的元素（1）是否和last指向的元素（1）相同，若相同表示要换行了（因为last始终指向正在打印的当前行最右的元素），则更新last=nlast（last指向结点3），然后将结点2出队并打印，让temp更新并记录结点2，并将结点2的孩子结点（4）入队，并将nlast指向结点2的孩子结点（4），然后将结点（3）出队并打印，让temp更新并记录结点3，让结点（3）的孩子结点（5和6）入队，此时比较temp结点和last指向的结点（3）是否相同，相同则更新last=nlast，依次下去，即可完成打印。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/tree/level_tree3.png\" alt></p>\n<h1 id=\"Java代码\"><a href=\"#Java代码\" class=\"headerlink\" title=\"Java代码\"></a>Java代码</h1><pre><code>import java.util.*;\n\n\npublic class TreePrinter {\n    // 结点定义\n    class TreeNode {\n        int val = 0;\n        TreeNode left = null;\n        TreeNode right = null;\n        public TreeNode(int val) {\n            this.val = val;\n        }\n    }    \n    public int[][] printTree(TreeNode root) {\n        // 在Java中LinkedList实现了Queue接口。\n        Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();\n        TreeNode last, nlast = null;\n        last = root; // 初始化last结点\n        queue.add(last);  // 根节点入队\n        // 结果要返回int[][],但不清楚需要几行几列，所以需要用到集合，这里相当于定义了一个二维数组\n        // &lt;&gt;表示泛型，表示这个集合只能装Integer类型，可以忽略，因为编译之后泛型就没了\n        List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();\n        // 定义每一行,即row作为res的元素\n        List&lt;Integer&gt; row = new ArrayList&lt;&gt;();\n        // 通过上面分析可以看到循环的条件是队列不为空，因为初始已经将根节点入队了\n        while (!queue.isEmpty()) {\n            // 结点首先出队，并用temp变量记录。\n            TreeNode temp = queue.poll();\n            // 这个可以使用不换行的打印，为了满足题目的返回条件，这里将它加入到行集合中\n            row.add(temp.val);\n            // 出队元素的左孩子不为空，入队并跟新nlast\n            if (temp.left != null) {\n                nlast = temp.left;\n                queue.add(nlast);\n            }\n            // 出队元素的右孩子不为空，入队并跟新nlast\n            if (temp.right != null) {\n                nlast = temp.right;\n                queue.add(nlast);\n            }\n            // 如果已经出队并被打印的元素和last指向的元素相同\n            // 则需要换行了，并更新last=nlast\n            if (temp.val == last.val) {\n                last = nlast;\n                // 将这一行加入到结果集合中\n                res.add(row);\n                // 清空这一行,这里不能用row.clear()，需要重新new一个对象出来。\n                row = new ArrayList&lt;&gt;();\n            }\n        }\n        // 到此结果就被遍历出来了，下面构造符合题目返回结果的元素。\n        // 首先开辟多上行的元素，列是空的\n        int [][] ans = new int[res.size()][];\n        for (int i = 0; i &lt; res.size(); i ++){\n            // 元素对每一行开辟多少列\n            ans[i] = new int[res.get(i).size()];\n            for (int j = 0; j &lt; res.get(i).size(); j++) {\n                // 完成赋值\n                ans[i][j] = res.get(i).get(j);\n                // 测试使用\n                //System.out.print(ans[i][j]);\n            }\n            // 测试使用\n            //System.out.println();\n        }\n        //返回结果\n        return ans;\n    }\n}\n</code></pre><h1 id=\"测试程序\"><a href=\"#测试程序\" class=\"headerlink\" title=\"测试程序\"></a>测试程序</h1><pre><code>public static void main(String[] args) {\n    TreeNode root = new TreeNode(1);\n    root.left = new TreeNode(2);\n    root.right = new TreeNode(3);\n    root.left.left = new TreeNode(4);\n    root.right.left = new TreeNode(5);\n    root.right.right = new TreeNode(6);\n    root.right.left.left = new TreeNode(7);\n    root.right.left.right = new TreeNode(8);\n    printTree(root);\n}\n</code></pre><h1 id=\"Python代码\"><a href=\"#Python代码\" class=\"headerlink\" title=\"Python代码\"></a>Python代码</h1><pre><code># -*- coding:utf-8 -*-\n# 结点定义 \nclass TreeNode:\n     def __init__(self, x):\n         self.val = x\n         self.left = None\n         self.right = None\n\nclass TreePrinter:\n    def printTree(self, root):\n        # 结果定义\n        res=[]  \n         # 队列（使用列表可以模拟队列）\n        queue=[] \n        # 判空\n        if root==None:\n            return res\n        # 初始化结点\n        last=nlast=root\n        # 入队使用append()函数\n        queue.append(root)\n        # 一行，相当于Java代码中的row数组\n        row=[]\n        # 队列不为空用循环\n        while len(queue):\n            # 出队，并用temp结点来记录，使用pop(0)函数，即弹出列表的第一个元素（队首元素）\n            temp=queue.pop(0)\n            # 入队\n            row.append(temp.val)\n            # 如果出队元素的左右孩子不为空，则入队并更新nlast\n            if temp.left!=None:\n                queue.append(temp.left)\n                nlast=temp.left\n            if temp.right!=None:\n                queue.append(temp.right)\n                nlast=temp.right\n            # 如果出队元素和last指向的元素相同，则完成一行的打印\n            if temp==last:\n                # 加入到结果中row[:]表示将row中所有元素放入到res中,是一个列表\n                res.append(row[:])\n                # 清空这一行\n                row=[]\n                # 更新last指向nlast\n                last=nlast\n         return res\n</code></pre>"},{"title":"关于k阶矩的理解","date":"2018-03-22T13:44:18.000Z","_content":"\n# k阶原点矩、2阶矩、3阶矩该怎么理解？ #\n\n下面使用语言描述和代码来讲解。\n<!-- more -->\n\n> 阶矩是用来描述随机变量的概率分布的特性.\n\n> 一阶矩指的是随机变量的平均值,即期望值\n> \n> 二阶矩指的是随机变量的方差\n> \n> 三阶矩指的是随机变量的偏度\n> \n> 四阶矩指的是随机变量的峰度\n \n> 因此通过计算矩,则可以得出随机变量的分布形状\n\n# 代码实现 #\n使用Python2.0实现\n\n    import numpy as np\n\tfrom scipy import stats\n\t\n\t\n\tdef calc_statistics(x):\n\tn = x.shape[0]   #样本个数\n\n\t# 手动计算\n\tm = 0\n\tm2 = 0\n\tm3 = 0\n\tm4 = 0\n\tfor t in x:\n\t\tm += t\n\t\tm2 += t*t\n\t\tm3 += t**3\n\t\tm4 += t**4\n\tm /= n\n\tm2 /= n\n\tm3 /= n\n\tm4 /= n\n\n\tmu = m    # 一阶矩\n\tsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\n\tskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\n\tkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3\t# 四阶矩（峰度）\n\tprint \"手动计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\n\t# 使用系统函数验证\n\tmu = np.mean(x, axis=0)\n\tsigma = np.std(x, axis=0)\n\tskew = stats.skew(x)\n\tkurtosis = stats.kurtosis(x)\n\treturn mu, sigma, skew, kurtosis\n\n\tif __name__ == '__main__':\n\td = np.random.randn(10000)\n\tprint d\n\tprint d.shape\n\tmu, sigma, skew, kurtosis = calc_statistics(d)\n\tprint \"函数库计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\t\n执行结果:\n\t\n\t\n> [-0.42751577  0.36230961  0.37899409 ...,  0.09176115 -1.38955563\n -0.57570736]\n\n\n> (10000L,)\n\n\n> 手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n\n\n> 函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n","source":"_posts/关于k阶矩的理解.md","raw":"---\ntitle: 关于k阶矩的理解\ndate: 2018-03-22 21:44:18\ncategories:\n- 数学基础\n- 随机过程\ntags:\n- k阶矩\n- 随机过程\n- 偏度\n- 峰度\n---\n\n# k阶原点矩、2阶矩、3阶矩该怎么理解？ #\n\n下面使用语言描述和代码来讲解。\n<!-- more -->\n\n> 阶矩是用来描述随机变量的概率分布的特性.\n\n> 一阶矩指的是随机变量的平均值,即期望值\n> \n> 二阶矩指的是随机变量的方差\n> \n> 三阶矩指的是随机变量的偏度\n> \n> 四阶矩指的是随机变量的峰度\n \n> 因此通过计算矩,则可以得出随机变量的分布形状\n\n# 代码实现 #\n使用Python2.0实现\n\n    import numpy as np\n\tfrom scipy import stats\n\t\n\t\n\tdef calc_statistics(x):\n\tn = x.shape[0]   #样本个数\n\n\t# 手动计算\n\tm = 0\n\tm2 = 0\n\tm3 = 0\n\tm4 = 0\n\tfor t in x:\n\t\tm += t\n\t\tm2 += t*t\n\t\tm3 += t**3\n\t\tm4 += t**4\n\tm /= n\n\tm2 /= n\n\tm3 /= n\n\tm4 /= n\n\n\tmu = m    # 一阶矩\n\tsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\n\tskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\n\tkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3\t# 四阶矩（峰度）\n\tprint \"手动计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\n\t# 使用系统函数验证\n\tmu = np.mean(x, axis=0)\n\tsigma = np.std(x, axis=0)\n\tskew = stats.skew(x)\n\tkurtosis = stats.kurtosis(x)\n\treturn mu, sigma, skew, kurtosis\n\n\tif __name__ == '__main__':\n\td = np.random.randn(10000)\n\tprint d\n\tprint d.shape\n\tmu, sigma, skew, kurtosis = calc_statistics(d)\n\tprint \"函数库计算均值、标准差、偏度、峰度：\", mu, sigma, skew, kurtosis\n\t\n执行结果:\n\t\n\t\n> [-0.42751577  0.36230961  0.37899409 ...,  0.09176115 -1.38955563\n -0.57570736]\n\n\n> (10000L,)\n\n\n> 手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n\n\n> 函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446\n","slug":"关于k阶矩的理解","published":1,"updated":"2019-05-08T10:50:25.482Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqpw0005lnvv0lvpncee","content":"<h1 id=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"><a href=\"#k阶原点矩、2阶矩、3阶矩该怎么理解？\" class=\"headerlink\" title=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"></a>k阶原点矩、2阶矩、3阶矩该怎么理解？</h1><p>下面使用语言描述和代码来讲解。<br><a id=\"more\"></a></p>\n<blockquote>\n<p>阶矩是用来描述随机变量的概率分布的特性.</p>\n</blockquote>\n<blockquote>\n<p>一阶矩指的是随机变量的平均值,即期望值</p>\n<p>二阶矩指的是随机变量的方差</p>\n<p>三阶矩指的是随机变量的偏度</p>\n<p>四阶矩指的是随机变量的峰度</p>\n</blockquote>\n<blockquote>\n<p>因此通过计算矩,则可以得出随机变量的分布形状</p>\n</blockquote>\n<h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h1><p>使用Python2.0实现</p>\n<pre><code>import numpy as np\nfrom scipy import stats\n\n\ndef calc_statistics(x):\nn = x.shape[0]   #样本个数\n\n# 手动计算\nm = 0\nm2 = 0\nm3 = 0\nm4 = 0\nfor t in x:\n    m += t\n    m2 += t*t\n    m3 += t**3\n    m4 += t**4\nm /= n\nm2 /= n\nm3 /= n\nm4 /= n\n\nmu = m    # 一阶矩\nsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\nskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\nkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3    # 四阶矩（峰度）\nprint &quot;手动计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n\n# 使用系统函数验证\nmu = np.mean(x, axis=0)\nsigma = np.std(x, axis=0)\nskew = stats.skew(x)\nkurtosis = stats.kurtosis(x)\nreturn mu, sigma, skew, kurtosis\n\nif __name__ == &apos;__main__&apos;:\nd = np.random.randn(10000)\nprint d\nprint d.shape\nmu, sigma, skew, kurtosis = calc_statistics(d)\nprint &quot;函数库计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n</code></pre><p>执行结果:</p>\n<blockquote>\n<p>[-0.42751577  0.36230961  0.37899409 …,  0.09176115 -1.38955563<br> -0.57570736]</p>\n</blockquote>\n<blockquote>\n<p>(10000L,)</p>\n</blockquote>\n<blockquote>\n<p>手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>\n<blockquote>\n<p>函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<h1 id=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"><a href=\"#k阶原点矩、2阶矩、3阶矩该怎么理解？\" class=\"headerlink\" title=\"k阶原点矩、2阶矩、3阶矩该怎么理解？\"></a>k阶原点矩、2阶矩、3阶矩该怎么理解？</h1><p>下面使用语言描述和代码来讲解。<br>","more":"</p>\n<blockquote>\n<p>阶矩是用来描述随机变量的概率分布的特性.</p>\n</blockquote>\n<blockquote>\n<p>一阶矩指的是随机变量的平均值,即期望值</p>\n<p>二阶矩指的是随机变量的方差</p>\n<p>三阶矩指的是随机变量的偏度</p>\n<p>四阶矩指的是随机变量的峰度</p>\n</blockquote>\n<blockquote>\n<p>因此通过计算矩,则可以得出随机变量的分布形状</p>\n</blockquote>\n<h1 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h1><p>使用Python2.0实现</p>\n<pre><code>import numpy as np\nfrom scipy import stats\n\n\ndef calc_statistics(x):\nn = x.shape[0]   #样本个数\n\n# 手动计算\nm = 0\nm2 = 0\nm3 = 0\nm4 = 0\nfor t in x:\n    m += t\n    m2 += t*t\n    m3 += t**3\n    m4 += t**4\nm /= n\nm2 /= n\nm3 /= n\nm4 /= n\n\nmu = m    # 一阶矩\nsigma = np.sqrt(m2 - mu*mu)   # 二阶矩\nskew = (m3 - 3*mu*m2 + 2*mu**3) / sigma**3    # 三阶矩（偏度）\nkurtosis = (m4 - 4*mu*m3 + 6*mu*mu*m2 - 4*mu**3*mu + mu**4) / sigma**4 - 3    # 四阶矩（峰度）\nprint &quot;手动计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n\n# 使用系统函数验证\nmu = np.mean(x, axis=0)\nsigma = np.std(x, axis=0)\nskew = stats.skew(x)\nkurtosis = stats.kurtosis(x)\nreturn mu, sigma, skew, kurtosis\n\nif __name__ == &apos;__main__&apos;:\nd = np.random.randn(10000)\nprint d\nprint d.shape\nmu, sigma, skew, kurtosis = calc_statistics(d)\nprint &quot;函数库计算均值、标准差、偏度、峰度：&quot;, mu, sigma, skew, kurtosis\n</code></pre><p>执行结果:</p>\n<blockquote>\n<p>[-0.42751577  0.36230961  0.37899409 …,  0.09176115 -1.38955563<br> -0.57570736]</p>\n</blockquote>\n<blockquote>\n<p>(10000L,)</p>\n</blockquote>\n<blockquote>\n<p>手动计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>\n<blockquote>\n<p>函数库计算均值、标准差、偏度、峰度： -0.00189350820374 0.995018151945 -0.00589521484127 -0.0590604043446</p>\n</blockquote>"},{"title":"图像与常用算子进行卷积运算","date":"2018-03-19T11:11:57.000Z","_content":"> 图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。\n\n<!-- more -->\n\n# 实现代码 #\n\n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/9/18 16:57\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : image_convolve.py\n\t# @ToDo    :  图像卷积\n\t\n\timport numpy as np\n\timport os\n\tfrom PIL import Image\n\t\n\t\n\tdef convolve(image, weight):\n\t\theight, width = image.shape\n\t\th, w = weight.shape\n\t\theight_new = height - h + 1\n\t\twidth_new = width - w + 1\n\t\tprint image.shape\n\t\timage_new = np.zeros((height_new, width_new), dtype=np.float)\n\t\tfor i in range(height_new):\n\t\t\tfor j in range(width_new):\n\t\t\t\timage_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n\t\timage_new = image_new.clip(0, 255)\n\t\timage_new = np.rint(image_new).astype(\"uint8\")\n\t\tprint image_new.shape\n\t\treturn image_new\n\t\n\t\n\t# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\t\n\tif __name__ == '__main__':\n\t\timage = Image.open(\"son.png\", \"r\")\n\t\toutput_path = \".\\\\ImageConvolve\\\\\"\n\t\tif not os.path.exists(output_path):\n\t\t\tos.mkdir(output_path)\n\t\ta = np.array(image)\n\t\tavg3 = np.ones((3, 3))\n\t\tavg3 /= avg3.sum()\n\t\tavg5 = np.ones((5, 5))\n\t\tavg5 /= avg5.sum()\n\t\n\t\tgauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.022, 0.097, 0.159, 0.097, 0.022],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.003, 0.013, 0.022, 0.013, 0.003]))\n\t\n\t\tsoble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n\t\tsoble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n\t\tsoble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\t\n\t\tprewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n\t\tprewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n\t\tprewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\t\n\t\tlaplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n\t\tlaplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n\t\tweight_list = (\n\t\t\t'avg3', 'avg5', 'gauss', 'soble_x', 'soble_y', 'soble', 'prewitt_x', 'prewitt_y', 'prewitt', 'laplacian4',\n\t\t\t'laplacian8')\n\t\n\t\tprint \"梯度检测\"\n\t\tfor weight in weight_list:\n\t\t\tprint weight, \"R\",\n\t\t\tR = convolve(a[:, :, 0], eval(weight))\n\t\t\tprint \"G\",\n\t\t\tG = convolve(a[:, :, 1], eval(weight))\n\t\t\tprint \"B\"\n\t\t\tB = convolve(a[:, :, 2], eval(weight))\n\t\t\tI = np.stack((R, G, B), 2)\n\t\t# Image.fromarray(I).save(output_path + weight + \".png\")\n\n# 实验结果 #\n![图像卷积运算实验结果](https://mic-jasontang.github.io/imgs/img-cov.png)","source":"_posts/图像常用算子进行卷积运算.md","raw":"---\ntitle: 图像与常用算子进行卷积运算\ndate: 2018-03-19 19:11:57\ncategories:\n- 图像处理\n- 图像增强\ntags:\n- 图像处理 \n- 卷积运算 \n- guass \n- soble \n- prewitt \n- laplacian\n---\n> 图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。\n\n<!-- more -->\n\n# 实现代码 #\n\n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/9/18 16:57\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : image_convolve.py\n\t# @ToDo    :  图像卷积\n\t\n\timport numpy as np\n\timport os\n\tfrom PIL import Image\n\t\n\t\n\tdef convolve(image, weight):\n\t\theight, width = image.shape\n\t\th, w = weight.shape\n\t\theight_new = height - h + 1\n\t\twidth_new = width - w + 1\n\t\tprint image.shape\n\t\timage_new = np.zeros((height_new, width_new), dtype=np.float)\n\t\tfor i in range(height_new):\n\t\t\tfor j in range(width_new):\n\t\t\t\timage_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n\t\timage_new = image_new.clip(0, 255)\n\t\timage_new = np.rint(image_new).astype(\"uint8\")\n\t\tprint image_new.shape\n\t\treturn image_new\n\t\n\t\n\t# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\t\n\tif __name__ == '__main__':\n\t\timage = Image.open(\"son.png\", \"r\")\n\t\toutput_path = \".\\\\ImageConvolve\\\\\"\n\t\tif not os.path.exists(output_path):\n\t\t\tos.mkdir(output_path)\n\t\ta = np.array(image)\n\t\tavg3 = np.ones((3, 3))\n\t\tavg3 /= avg3.sum()\n\t\tavg5 = np.ones((5, 5))\n\t\tavg5 /= avg5.sum()\n\t\n\t\tgauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.022, 0.097, 0.159, 0.097, 0.022],\n\t\t\t\t\t\t  [0.013, 0.059, 0.097, 0.059, 0.013],\n\t\t\t\t\t\t  [0.003, 0.013, 0.022, 0.013, 0.003]))\n\t\n\t\tsoble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n\t\tsoble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n\t\tsoble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\t\n\t\tprewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n\t\tprewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n\t\tprewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\t\n\t\tlaplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n\t\tlaplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n\t\tweight_list = (\n\t\t\t'avg3', 'avg5', 'gauss', 'soble_x', 'soble_y', 'soble', 'prewitt_x', 'prewitt_y', 'prewitt', 'laplacian4',\n\t\t\t'laplacian8')\n\t\n\t\tprint \"梯度检测\"\n\t\tfor weight in weight_list:\n\t\t\tprint weight, \"R\",\n\t\t\tR = convolve(a[:, :, 0], eval(weight))\n\t\t\tprint \"G\",\n\t\t\tG = convolve(a[:, :, 1], eval(weight))\n\t\t\tprint \"B\"\n\t\t\tB = convolve(a[:, :, 2], eval(weight))\n\t\t\tI = np.stack((R, G, B), 2)\n\t\t# Image.fromarray(I).save(output_path + weight + \".png\")\n\n# 实验结果 #\n![图像卷积运算实验结果](https://mic-jasontang.github.io/imgs/img-cov.png)","slug":"图像常用算子进行卷积运算","published":1,"updated":"2019-05-08T10:50:25.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqpx0006lnvvs3uog4r7","content":"<blockquote>\n<p>图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1 id=\"实现代码\"><a href=\"#实现代码\" class=\"headerlink\" title=\"实现代码\"></a>实现代码</h1><pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/9/18 16:57\n# @Author  : Jasontang\n# @Site    : \n# @File    : image_convolve.py\n# @ToDo    :  图像卷积\n\nimport numpy as np\nimport os\nfrom PIL import Image\n\n\ndef convolve(image, weight):\n    height, width = image.shape\n    h, w = weight.shape\n    height_new = height - h + 1\n    width_new = width - w + 1\n    print image.shape\n    image_new = np.zeros((height_new, width_new), dtype=np.float)\n    for i in range(height_new):\n        for j in range(width_new):\n            image_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n    image_new = image_new.clip(0, 255)\n    image_new = np.rint(image_new).astype(&quot;uint8&quot;)\n    print image_new.shape\n    return image_new\n\n\n# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\nif __name__ == &apos;__main__&apos;:\n    image = Image.open(&quot;son.png&quot;, &quot;r&quot;)\n    output_path = &quot;.\\\\ImageConvolve\\\\&quot;\n    if not os.path.exists(output_path):\n        os.mkdir(output_path)\n    a = np.array(image)\n    avg3 = np.ones((3, 3))\n    avg3 /= avg3.sum()\n    avg5 = np.ones((5, 5))\n    avg5 /= avg5.sum()\n\n    gauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.022, 0.097, 0.159, 0.097, 0.022],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.003, 0.013, 0.022, 0.013, 0.003]))\n\n    soble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n    soble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n    soble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\n    prewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n    prewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n    prewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\n    laplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n    laplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n    weight_list = (\n        &apos;avg3&apos;, &apos;avg5&apos;, &apos;gauss&apos;, &apos;soble_x&apos;, &apos;soble_y&apos;, &apos;soble&apos;, &apos;prewitt_x&apos;, &apos;prewitt_y&apos;, &apos;prewitt&apos;, &apos;laplacian4&apos;,\n        &apos;laplacian8&apos;)\n\n    print &quot;梯度检测&quot;\n    for weight in weight_list:\n        print weight, &quot;R&quot;,\n        R = convolve(a[:, :, 0], eval(weight))\n        print &quot;G&quot;,\n        G = convolve(a[:, :, 1], eval(weight))\n        print &quot;B&quot;\n        B = convolve(a[:, :, 2], eval(weight))\n        I = np.stack((R, G, B), 2)\n    # Image.fromarray(I).save(output_path + weight + &quot;.png&quot;)\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p><img src=\"https://mic-jasontang.github.io/imgs/img-cov.png\" alt=\"图像卷积运算实验结果\"></p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<blockquote>\n<p>图像卷积实验，使用guass、soble、prewitt、 laplacian算子进行图像增强。</p>\n</blockquote>","more":"<h1 id=\"实现代码\"><a href=\"#实现代码\" class=\"headerlink\" title=\"实现代码\"></a>实现代码</h1><pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/9/18 16:57\n# @Author  : Jasontang\n# @Site    : \n# @File    : image_convolve.py\n# @ToDo    :  图像卷积\n\nimport numpy as np\nimport os\nfrom PIL import Image\n\n\ndef convolve(image, weight):\n    height, width = image.shape\n    h, w = weight.shape\n    height_new = height - h + 1\n    width_new = width - w + 1\n    print image.shape\n    image_new = np.zeros((height_new, width_new), dtype=np.float)\n    for i in range(height_new):\n        for j in range(width_new):\n            image_new[i, j] = np.sum(image[i:i + h, j:j + w] * weight)\n    image_new = image_new.clip(0, 255)\n    image_new = np.rint(image_new).astype(&quot;uint8&quot;)\n    print image_new.shape\n    return image_new\n\n\n# image_new = 255 * (image_new - image_new.min()) / (image_new.max() - image_new.min())\n\nif __name__ == &apos;__main__&apos;:\n    image = Image.open(&quot;son.png&quot;, &quot;r&quot;)\n    output_path = &quot;.\\\\ImageConvolve\\\\&quot;\n    if not os.path.exists(output_path):\n        os.mkdir(output_path)\n    a = np.array(image)\n    avg3 = np.ones((3, 3))\n    avg3 /= avg3.sum()\n    avg5 = np.ones((5, 5))\n    avg5 /= avg5.sum()\n\n    gauss = np.array(([0.003, 0.013, 0.022, 0.013, 0.003],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.022, 0.097, 0.159, 0.097, 0.022],\n                      [0.013, 0.059, 0.097, 0.059, 0.013],\n                      [0.003, 0.013, 0.022, 0.013, 0.003]))\n\n    soble_x = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]))\n    soble_y = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]))\n    soble = np.array(([-1, -1, 0], [-1, 0, 1], [0, 1, 1]))\n\n    prewitt_x = np.array(([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]))\n    prewitt_y = np.array(([-1, -1, -1], [0, 0, 0], [1, 1, 1]))\n    prewitt = np.array(([-2, -1, 0], [-1, 0, 1], [0, 1, 2]))\n\n    laplacian4 = np.array(([0, -1, 0], [-1, 4, -1], [0, -1, 0]))\n    laplacian8 = np.array(([-1, -1, -1], [-1, 8, -1], [-1, -1, -1]))\n    weight_list = (\n        &apos;avg3&apos;, &apos;avg5&apos;, &apos;gauss&apos;, &apos;soble_x&apos;, &apos;soble_y&apos;, &apos;soble&apos;, &apos;prewitt_x&apos;, &apos;prewitt_y&apos;, &apos;prewitt&apos;, &apos;laplacian4&apos;,\n        &apos;laplacian8&apos;)\n\n    print &quot;梯度检测&quot;\n    for weight in weight_list:\n        print weight, &quot;R&quot;,\n        R = convolve(a[:, :, 0], eval(weight))\n        print &quot;G&quot;,\n        G = convolve(a[:, :, 1], eval(weight))\n        print &quot;B&quot;\n        B = convolve(a[:, :, 2], eval(weight))\n        I = np.stack((R, G, B), 2)\n    # Image.fromarray(I).save(output_path + weight + &quot;.png&quot;)\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p><img src=\"https://mic-jasontang.github.io/imgs/img-cov.png\" alt=\"图像卷积运算实验结果\"></p>"},{"title":"字符串-旋转词","date":"2018-03-25T03:00:46.000Z","_content":"# 题目 #\n如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。比如A=\"12345\",A的旋转词有\"12345\",\"23451\",\"34512\",\"45123\"和\"51234\"。对于两个字符串A和B，请判断A和B是否互为旋转词。\n\n给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。\n\n\n\n测试样例：\n\n> \"cdab\",4,\"abcd\",4\n\n\n> 返回：true\n\n<!-- more -->\n\n# 思路 #\n\n\n1. 首先判断lena和lenb是否相同\n2. 如果长度相同，则生成A + A的新字符串newA\n3. 使用kmp算法或者别的查找子串的方法看newA中是否包含B\n\n下面举例说明，这样做为什么是对的。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str2.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str3.png)\n\n# Java代码 #\n\timport java.util.*;\n\n\tpublic class Rotation {\n\t    public boolean chkRotation(String A, int lena, String B, int lenb) {\n\t        \n\t        if (lena != lenb){\n\t            return false;\n\t        }\n\t        String str = A + A;\n\t\t\t// 这里我偷懒了，直接使用indexOf()函数来寻找子串，此函数找不到时会返回-1，找到时返回第一个字符出现的下标\n\t        if (str.indexOf(B) != -1) {\n\t            return true;\n\t        }\n\t        return false;\n\t    }\n\t}\n\n# Python代码 #\n\t# -*- coding:utf-8 -*-\n\t\n\tclass Rotation:\n\t    def chkRotation(self, A, lena, B, lenb):\n\t        \n\t        if lena != lenb:\n\t            return False\n\t        str = A + A\n\t        if str.find(B) != -1:\n\t            return True\n\t        else:\n\t            return False\n或者可以写成这样\n\n\t# -*- coding:utf-8 -*-\n\t\n\tclass Rotation:\n\t    def chkRotation(self, A, lena, B, lenb):\n\t        \n\t        return lena == lenb and B in A + A\n\n还有一些类似的题目。\n\n题目2\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str4.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str5.png)\n\n题目3\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str6.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str7.png)\n\n题目4\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str8.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str9.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str10.png)\n","source":"_posts/字符串-旋转词.md","raw":"---\ntitle: 字符串-旋转词\ndate: 2018-03-25 11:00:46\ncategories:\n- 基础算法\n- 字符串\ntags:\n- 字符串\n- 旋转词\n- java实现\n- Python实现\n---\n# 题目 #\n如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。比如A=\"12345\",A的旋转词有\"12345\",\"23451\",\"34512\",\"45123\"和\"51234\"。对于两个字符串A和B，请判断A和B是否互为旋转词。\n\n给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。\n\n\n\n测试样例：\n\n> \"cdab\",4,\"abcd\",4\n\n\n> 返回：true\n\n<!-- more -->\n\n# 思路 #\n\n\n1. 首先判断lena和lenb是否相同\n2. 如果长度相同，则生成A + A的新字符串newA\n3. 使用kmp算法或者别的查找子串的方法看newA中是否包含B\n\n下面举例说明，这样做为什么是对的。\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str2.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str3.png)\n\n# Java代码 #\n\timport java.util.*;\n\n\tpublic class Rotation {\n\t    public boolean chkRotation(String A, int lena, String B, int lenb) {\n\t        \n\t        if (lena != lenb){\n\t            return false;\n\t        }\n\t        String str = A + A;\n\t\t\t// 这里我偷懒了，直接使用indexOf()函数来寻找子串，此函数找不到时会返回-1，找到时返回第一个字符出现的下标\n\t        if (str.indexOf(B) != -1) {\n\t            return true;\n\t        }\n\t        return false;\n\t    }\n\t}\n\n# Python代码 #\n\t# -*- coding:utf-8 -*-\n\t\n\tclass Rotation:\n\t    def chkRotation(self, A, lena, B, lenb):\n\t        \n\t        if lena != lenb:\n\t            return False\n\t        str = A + A\n\t        if str.find(B) != -1:\n\t            return True\n\t        else:\n\t            return False\n或者可以写成这样\n\n\t# -*- coding:utf-8 -*-\n\t\n\tclass Rotation:\n\t    def chkRotation(self, A, lena, B, lenb):\n\t        \n\t        return lena == lenb and B in A + A\n\n还有一些类似的题目。\n\n题目2\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str4.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str5.png)\n\n题目3\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str6.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str7.png)\n\n题目4\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str8.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str9.png)\n\n![](https://mic-jasontang.github.io/imgs/algorithm/string/rota_str10.png)\n","slug":"字符串-旋转词","published":1,"updated":"2019-05-08T10:50:25.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqpy0007lnvvgeyrrh27","content":"<h1 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h1><p>如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。比如A=”12345”,A的旋转词有”12345”,”23451”,”34512”,”45123”和”51234”。对于两个字符串A和B，请判断A和B是否互为旋转词。</p>\n<p>给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。</p>\n<p>测试样例：</p>\n<blockquote>\n<p>“cdab”,4,”abcd”,4</p>\n</blockquote>\n<blockquote>\n<p>返回：true</p>\n</blockquote>\n<a id=\"more\"></a>\n<h1 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h1><ol>\n<li>首先判断lena和lenb是否相同</li>\n<li>如果长度相同，则生成A + A的新字符串newA</li>\n<li>使用kmp算法或者别的查找子串的方法看newA中是否包含B</li>\n</ol>\n<p>下面举例说明，这样做为什么是对的。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str2.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str3.png\" alt></p>\n<h1 id=\"Java代码\"><a href=\"#Java代码\" class=\"headerlink\" title=\"Java代码\"></a>Java代码</h1><pre><code>import java.util.*;\n\npublic class Rotation {\n    public boolean chkRotation(String A, int lena, String B, int lenb) {\n\n        if (lena != lenb){\n            return false;\n        }\n        String str = A + A;\n        // 这里我偷懒了，直接使用indexOf()函数来寻找子串，此函数找不到时会返回-1，找到时返回第一个字符出现的下标\n        if (str.indexOf(B) != -1) {\n            return true;\n        }\n        return false;\n    }\n}\n</code></pre><h1 id=\"Python代码\"><a href=\"#Python代码\" class=\"headerlink\" title=\"Python代码\"></a>Python代码</h1><pre><code># -*- coding:utf-8 -*-\n\nclass Rotation:\n    def chkRotation(self, A, lena, B, lenb):\n\n        if lena != lenb:\n            return False\n        str = A + A\n        if str.find(B) != -1:\n            return True\n        else:\n            return False\n</code></pre><p>或者可以写成这样</p>\n<pre><code># -*- coding:utf-8 -*-\n\nclass Rotation:\n    def chkRotation(self, A, lena, B, lenb):\n\n        return lena == lenb and B in A + A\n</code></pre><p>还有一些类似的题目。</p>\n<p>题目2</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str4.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str5.png\" alt></p>\n<p>题目3</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str6.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str7.png\" alt></p>\n<p>题目4</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str8.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str9.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str10.png\" alt></p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<h1 id=\"题目\"><a href=\"#题目\" class=\"headerlink\" title=\"题目\"></a>题目</h1><p>如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。比如A=”12345”,A的旋转词有”12345”,”23451”,”34512”,”45123”和”51234”。对于两个字符串A和B，请判断A和B是否互为旋转词。</p>\n<p>给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。</p>\n<p>测试样例：</p>\n<blockquote>\n<p>“cdab”,4,”abcd”,4</p>\n</blockquote>\n<blockquote>\n<p>返回：true</p>\n</blockquote>","more":"<h1 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h1><ol>\n<li>首先判断lena和lenb是否相同</li>\n<li>如果长度相同，则生成A + A的新字符串newA</li>\n<li>使用kmp算法或者别的查找子串的方法看newA中是否包含B</li>\n</ol>\n<p>下面举例说明，这样做为什么是对的。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str2.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str3.png\" alt></p>\n<h1 id=\"Java代码\"><a href=\"#Java代码\" class=\"headerlink\" title=\"Java代码\"></a>Java代码</h1><pre><code>import java.util.*;\n\npublic class Rotation {\n    public boolean chkRotation(String A, int lena, String B, int lenb) {\n\n        if (lena != lenb){\n            return false;\n        }\n        String str = A + A;\n        // 这里我偷懒了，直接使用indexOf()函数来寻找子串，此函数找不到时会返回-1，找到时返回第一个字符出现的下标\n        if (str.indexOf(B) != -1) {\n            return true;\n        }\n        return false;\n    }\n}\n</code></pre><h1 id=\"Python代码\"><a href=\"#Python代码\" class=\"headerlink\" title=\"Python代码\"></a>Python代码</h1><pre><code># -*- coding:utf-8 -*-\n\nclass Rotation:\n    def chkRotation(self, A, lena, B, lenb):\n\n        if lena != lenb:\n            return False\n        str = A + A\n        if str.find(B) != -1:\n            return True\n        else:\n            return False\n</code></pre><p>或者可以写成这样</p>\n<pre><code># -*- coding:utf-8 -*-\n\nclass Rotation:\n    def chkRotation(self, A, lena, B, lenb):\n\n        return lena == lenb and B in A + A\n</code></pre><p>还有一些类似的题目。</p>\n<p>题目2</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str4.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str5.png\" alt></p>\n<p>题目3</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str6.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str7.png\" alt></p>\n<p>题目4</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str8.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str9.png\" alt></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/algorithm/string/rota_str10.png\" alt></p>"},{"title":"直方图均衡化图片","date":"2018-03-20T10:31:55.000Z","_content":"\n# 直方图均衡化 #\n\n\n- 1.实验原理\n> 用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。\n \n\n\n\n\n- 2.实验步骤\n>\t1、对图像进行灰度统计，求灰度统计直方图。\n>\t\n>\t2、对灰度统计直方图进行归一化。\n>\t\n>\t3、求累积分布函数，求累积分布直方图。\n>\t\n>\t4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].\n>\t\n>\t5、确定映射对应关系，根据映射关系计算均衡化直方图。\n\n<!-- more -->\n\n- 3.代码\n\n\n> 代码采用python2.0实现   \n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/10/15 18:49\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : histequa.py\n\t# @ToDo    : 直方图均衡化(8bit)\n\t\n\t\n\tfrom PIL import Image\n\timport matplotlib as mpl\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\t\n\tmpl.rcParams['font.sans-serif'] = \"SimHei\"\n\tmpl.rcParams['axes.unicode_minus'] = False\n\t\n\t\n\tdef image2vector():\n\t    return np.array(Image.open(\"images/lena512.bmp\", \"r\").convert(\"L\"))\n\t\n\t\n\tdef equalization(data):\n\t    # 得到图像的高度、宽度\n\t    h = data.shape[0]\n\t    w = data.shape[1]\n\t    # 灰度数组\n\t    grayArr = np.zeros(255)\n\t    # 进行像素灰度统计\n\t    for i in range(h):\n\t        for j in range(w):\n\t            grayArr[data[i][j]] += 1\n\t    print grayArr.shape, grayArr.max()\n\t    # 归一化\n\t    idx = 0\n\t    for item in grayArr:\n\t        grayArr[idx] = item / (h * w)\n\t        idx += 1\n\t    # print grayArr\n\t    cdf = np.zeros(grayArr.shape)\n\t    sum = 0\n\t    # 计算灰度分布密度\n\t    # print cdf.shape\n\t    for i in range(len(grayArr)):\n\t        sum += grayArr[i]\n\t        cdf[i] = sum\n\t    L = 255\n\t    # print cdf\n\t    # 累计分布取整\n\t    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n\t    # print indexArr\n\t    # 对灰度值进行映射（均衡化）\n\t    for i in range(h):\n\t        for j in range(w):\n\t            data[i, j] = indexArr[data[i, j]]\n\t    return grayArr, cdf, data\n\t\n\t\n\tif __name__ == '__main__':\n\t    data = image2vector()\n\t    # print data.shape\n\t    plt.figure(figsize=(7, 9))\n\t    plt.subplot(321)\n\t    plt.title(u\"原始图像\")\n\t    plt.imshow(data, cmap='gray')\n\t    plt.subplot(322)\n\t    plt.title(u\"原始灰度\")\n\t    plt.hist(data.flatten(), normed=True, bins=256)\n\t    srcGray, cdf, equlArr = equalization(data)\n\t    plt.subplot(323)\n\t    plt.title(u\"归一化直方图\")\n\t    plt.hist(srcGray, 255)\n\t    plt.subplot(324)\n\t    plt.title(u\"累积直方图\")\n\t    plt.hist(cdf, 255)\n\t    plt.subplot(325)\n\t    plt.title(u\"均衡化图像\")\n\t    plt.imshow(equlArr, cmap='gray')\n\t    plt.subplot(326)\n\t    plt.title(u\"均衡化的直方图\")\n\t    plt.hist(equlArr.flatten(), normed=True, bins=256)\n\t    # print equlArr\n\t    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n\t    plt.show()\n\n- 4.实验结果\n![实验结果](https://mic-jasontang.github.io/imgs/histequa.png)\n\n\n- 5.实验总结\n> 在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。\n","source":"_posts/直方图均衡化图片.md","raw":"---\ntitle: 直方图均衡化图片\ndate: 2018-03-20 18:31:55\ncategories:\n- 图像处理\n- 图像增强\ntags: \n- python \n- 图像处理 \n- 直方图均衡化\n---\n\n# 直方图均衡化 #\n\n\n- 1.实验原理\n> 用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。\n \n\n\n\n\n- 2.实验步骤\n>\t1、对图像进行灰度统计，求灰度统计直方图。\n>\t\n>\t2、对灰度统计直方图进行归一化。\n>\t\n>\t3、求累积分布函数，求累积分布直方图。\n>\t\n>\t4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].\n>\t\n>\t5、确定映射对应关系，根据映射关系计算均衡化直方图。\n\n<!-- more -->\n\n- 3.代码\n\n\n> 代码采用python2.0实现   \n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2017/10/15 18:49\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : histequa.py\n\t# @ToDo    : 直方图均衡化(8bit)\n\t\n\t\n\tfrom PIL import Image\n\timport matplotlib as mpl\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\t\n\tmpl.rcParams['font.sans-serif'] = \"SimHei\"\n\tmpl.rcParams['axes.unicode_minus'] = False\n\t\n\t\n\tdef image2vector():\n\t    return np.array(Image.open(\"images/lena512.bmp\", \"r\").convert(\"L\"))\n\t\n\t\n\tdef equalization(data):\n\t    # 得到图像的高度、宽度\n\t    h = data.shape[0]\n\t    w = data.shape[1]\n\t    # 灰度数组\n\t    grayArr = np.zeros(255)\n\t    # 进行像素灰度统计\n\t    for i in range(h):\n\t        for j in range(w):\n\t            grayArr[data[i][j]] += 1\n\t    print grayArr.shape, grayArr.max()\n\t    # 归一化\n\t    idx = 0\n\t    for item in grayArr:\n\t        grayArr[idx] = item / (h * w)\n\t        idx += 1\n\t    # print grayArr\n\t    cdf = np.zeros(grayArr.shape)\n\t    sum = 0\n\t    # 计算灰度分布密度\n\t    # print cdf.shape\n\t    for i in range(len(grayArr)):\n\t        sum += grayArr[i]\n\t        cdf[i] = sum\n\t    L = 255\n\t    # print cdf\n\t    # 累计分布取整\n\t    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n\t    # print indexArr\n\t    # 对灰度值进行映射（均衡化）\n\t    for i in range(h):\n\t        for j in range(w):\n\t            data[i, j] = indexArr[data[i, j]]\n\t    return grayArr, cdf, data\n\t\n\t\n\tif __name__ == '__main__':\n\t    data = image2vector()\n\t    # print data.shape\n\t    plt.figure(figsize=(7, 9))\n\t    plt.subplot(321)\n\t    plt.title(u\"原始图像\")\n\t    plt.imshow(data, cmap='gray')\n\t    plt.subplot(322)\n\t    plt.title(u\"原始灰度\")\n\t    plt.hist(data.flatten(), normed=True, bins=256)\n\t    srcGray, cdf, equlArr = equalization(data)\n\t    plt.subplot(323)\n\t    plt.title(u\"归一化直方图\")\n\t    plt.hist(srcGray, 255)\n\t    plt.subplot(324)\n\t    plt.title(u\"累积直方图\")\n\t    plt.hist(cdf, 255)\n\t    plt.subplot(325)\n\t    plt.title(u\"均衡化图像\")\n\t    plt.imshow(equlArr, cmap='gray')\n\t    plt.subplot(326)\n\t    plt.title(u\"均衡化的直方图\")\n\t    plt.hist(equlArr.flatten(), normed=True, bins=256)\n\t    # print equlArr\n\t    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n\t    plt.show()\n\n- 4.实验结果\n![实验结果](https://mic-jasontang.github.io/imgs/histequa.png)\n\n\n- 5.实验总结\n> 在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。\n","slug":"直方图均衡化图片","published":1,"updated":"2019-05-08T10:50:25.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq0000alnvvptqmpgyq","content":"<h1 id=\"直方图均衡化\"><a href=\"#直方图均衡化\" class=\"headerlink\" title=\"直方图均衡化\"></a>直方图均衡化</h1><ul>\n<li>1.实验原理<blockquote>\n<p>用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。</p>\n</blockquote>\n</li>\n</ul>\n<ul>\n<li>2.实验步骤<blockquote>\n<p>   1、对图像进行灰度统计，求灰度统计直方图。</p>\n<p>   2、对灰度统计直方图进行归一化。</p>\n<p>   3、求累积分布函数，求累积分布直方图。</p>\n<p>   4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].</p>\n<p>   5、确定映射对应关系，根据映射关系计算均衡化直方图。</p>\n</blockquote>\n</li>\n</ul>\n<a id=\"more\"></a>\n<ul>\n<li>3.代码</li>\n</ul>\n<blockquote>\n<p>代码采用python2.0实现   </p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/10/15 18:49\n# @Author  : Jasontang\n# @Site    : \n# @File    : histequa.py\n# @ToDo    : 直方图均衡化(8bit)\n\n\nfrom PIL import Image\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmpl.rcParams[&apos;font.sans-serif&apos;] = &quot;SimHei&quot;\nmpl.rcParams[&apos;axes.unicode_minus&apos;] = False\n\n\ndef image2vector():\n    return np.array(Image.open(&quot;images/lena512.bmp&quot;, &quot;r&quot;).convert(&quot;L&quot;))\n\n\ndef equalization(data):\n    # 得到图像的高度、宽度\n    h = data.shape[0]\n    w = data.shape[1]\n    # 灰度数组\n    grayArr = np.zeros(255)\n    # 进行像素灰度统计\n    for i in range(h):\n        for j in range(w):\n            grayArr[data[i][j]] += 1\n    print grayArr.shape, grayArr.max()\n    # 归一化\n    idx = 0\n    for item in grayArr:\n        grayArr[idx] = item / (h * w)\n        idx += 1\n    # print grayArr\n    cdf = np.zeros(grayArr.shape)\n    sum = 0\n    # 计算灰度分布密度\n    # print cdf.shape\n    for i in range(len(grayArr)):\n        sum += grayArr[i]\n        cdf[i] = sum\n    L = 255\n    # print cdf\n    # 累计分布取整\n    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n    # print indexArr\n    # 对灰度值进行映射（均衡化）\n    for i in range(h):\n        for j in range(w):\n            data[i, j] = indexArr[data[i, j]]\n    return grayArr, cdf, data\n\n\nif __name__ == &apos;__main__&apos;:\n    data = image2vector()\n    # print data.shape\n    plt.figure(figsize=(7, 9))\n    plt.subplot(321)\n    plt.title(u&quot;原始图像&quot;)\n    plt.imshow(data, cmap=&apos;gray&apos;)\n    plt.subplot(322)\n    plt.title(u&quot;原始灰度&quot;)\n    plt.hist(data.flatten(), normed=True, bins=256)\n    srcGray, cdf, equlArr = equalization(data)\n    plt.subplot(323)\n    plt.title(u&quot;归一化直方图&quot;)\n    plt.hist(srcGray, 255)\n    plt.subplot(324)\n    plt.title(u&quot;累积直方图&quot;)\n    plt.hist(cdf, 255)\n    plt.subplot(325)\n    plt.title(u&quot;均衡化图像&quot;)\n    plt.imshow(equlArr, cmap=&apos;gray&apos;)\n    plt.subplot(326)\n    plt.title(u&quot;均衡化的直方图&quot;)\n    plt.hist(equlArr.flatten(), normed=True, bins=256)\n    # print equlArr\n    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n    plt.show()\n</code></pre><ul>\n<li>4.实验结果<br><img src=\"https://mic-jasontang.github.io/imgs/histequa.png\" alt=\"实验结果\"></li>\n</ul>\n<ul>\n<li>5.实验总结<blockquote>\n<p>在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。</p>\n</blockquote>\n</li>\n</ul>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<h1 id=\"直方图均衡化\"><a href=\"#直方图均衡化\" class=\"headerlink\" title=\"直方图均衡化\"></a>直方图均衡化</h1><ul>\n<li>1.实验原理<blockquote>\n<p>用直方图变换方法进行图像增强，通过改变图像的直方图来概念图像中像素的灰度，以达到图像增强的目的。</p>\n</blockquote>\n</li>\n</ul>\n<ul>\n<li>2.实验步骤<blockquote>\n<p>   1、对图像进行灰度统计，求灰度统计直方图。</p>\n<p>   2、对灰度统计直方图进行归一化。</p>\n<p>   3、求累积分布函数，求累积分布直方图。</p>\n<p>   4、对累积直方图各项进行取整扩展tk=int[(L-1)tk + 0.5].</p>\n<p>   5、确定映射对应关系，根据映射关系计算均衡化直方图。</p>\n</blockquote>\n</li>\n</ul>","more":"<ul>\n<li>3.代码</li>\n</ul>\n<blockquote>\n<p>代码采用python2.0实现   </p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2017/10/15 18:49\n# @Author  : Jasontang\n# @Site    : \n# @File    : histequa.py\n# @ToDo    : 直方图均衡化(8bit)\n\n\nfrom PIL import Image\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmpl.rcParams[&apos;font.sans-serif&apos;] = &quot;SimHei&quot;\nmpl.rcParams[&apos;axes.unicode_minus&apos;] = False\n\n\ndef image2vector():\n    return np.array(Image.open(&quot;images/lena512.bmp&quot;, &quot;r&quot;).convert(&quot;L&quot;))\n\n\ndef equalization(data):\n    # 得到图像的高度、宽度\n    h = data.shape[0]\n    w = data.shape[1]\n    # 灰度数组\n    grayArr = np.zeros(255)\n    # 进行像素灰度统计\n    for i in range(h):\n        for j in range(w):\n            grayArr[data[i][j]] += 1\n    print grayArr.shape, grayArr.max()\n    # 归一化\n    idx = 0\n    for item in grayArr:\n        grayArr[idx] = item / (h * w)\n        idx += 1\n    # print grayArr\n    cdf = np.zeros(grayArr.shape)\n    sum = 0\n    # 计算灰度分布密度\n    # print cdf.shape\n    for i in range(len(grayArr)):\n        sum += grayArr[i]\n        cdf[i] = sum\n    L = 255\n    # print cdf\n    # 累计分布取整\n    indexArr = ((L - 1) * cdf + 0.5).astype(np.uint8)\n    # print indexArr\n    # 对灰度值进行映射（均衡化）\n    for i in range(h):\n        for j in range(w):\n            data[i, j] = indexArr[data[i, j]]\n    return grayArr, cdf, data\n\n\nif __name__ == &apos;__main__&apos;:\n    data = image2vector()\n    # print data.shape\n    plt.figure(figsize=(7, 9))\n    plt.subplot(321)\n    plt.title(u&quot;原始图像&quot;)\n    plt.imshow(data, cmap=&apos;gray&apos;)\n    plt.subplot(322)\n    plt.title(u&quot;原始灰度&quot;)\n    plt.hist(data.flatten(), normed=True, bins=256)\n    srcGray, cdf, equlArr = equalization(data)\n    plt.subplot(323)\n    plt.title(u&quot;归一化直方图&quot;)\n    plt.hist(srcGray, 255)\n    plt.subplot(324)\n    plt.title(u&quot;累积直方图&quot;)\n    plt.hist(cdf, 255)\n    plt.subplot(325)\n    plt.title(u&quot;均衡化图像&quot;)\n    plt.imshow(equlArr, cmap=&apos;gray&apos;)\n    plt.subplot(326)\n    plt.title(u&quot;均衡化的直方图&quot;)\n    plt.hist(equlArr.flatten(), normed=True, bins=256)\n    # print equlArr\n    plt.tight_layout(0.3, rect=(0, 0, 1, 0.92))\n    plt.show()\n</code></pre><ul>\n<li>4.实验结果<br><img src=\"https://mic-jasontang.github.io/imgs/histequa.png\" alt=\"实验结果\"></li>\n</ul>\n<ul>\n<li>5.实验总结<blockquote>\n<p>在对数据进行归一化的时候，是用每个灰度值除以像素总数。在最后通过映射关系计算均衡化直方图时，是借助求出的映射关系，直接对原图的像素点进行映射。通过均衡化能增强图像的动态范围偏小的图像的反差，达到增强图像整体对比度的效果。</p>\n</blockquote>\n</li>\n</ul>"},{"title":"风格迁移与序列模型研究","date":"2019-03-30T03:11:00.000Z","_content":"\n<!-- more -->\n\n1. 风格迁移：将一副图片的风格迁移到另一幅新的图片中。\n\t- a. 使用两张图片作为输入（如一张毕加索的图S，一张普通照片C）生成一张新的图片G；\n\t- b. 风格迁移需要定义它的损失函数，内容损失L_content和风格损失L_style，整体损失为L=αL_content(C, G) + βL_style(S, G)； \n\t- c. 有了输入和损失函数，就可以来训练网络了，S和C都是已知的，那G怎么初始化呢？一般采用随机数来填充整幅图像；\n\t- d. 那么内容损失函数L_content具体是怎么定义呢？最简单的，取两幅图像像素差值的二范数。；\n\t- e. L_style如何定义呢？这里公式有点复杂，用文字描述就是单独计算S的通道之间像素相关性（像素值相乘）、G的通道之间像素相关性（像素值相乘），之后作差，求Frobenius范数。；有了输入、初始化、损失，就可以使用优化方法对损失函数进行优化了。\n2. 序列模型：学习RNN及其变种。\n\t- a. 序列模型偏向于NLP、语音等方面；\n\t- b. 首先是RNN网络，相比于标准的层级神经网路，RNN可以处理输入数据长度和输出数据长度不同的情况；不能在文本的不同位置共享已学得的特征。但是RNN不能利用后续的信息，在文本处理方面会有一定的局限性，比如Name Entity Recognition。而且RNN不能处理梯度消失的问题，当RNN层数很深的时候，反向传播时，后面的层，不能去影响前面的层的参数。；\n\t- c. GRU网络，可以说它解决了RNN梯度消失的问题，它引入了C(memory cell)来记忆和更新门(gamma_u)来控制是否更新C或者使用old_C。；\n\t- d. LSTM网络，GRU网络是LSTM的简化版本，后者多了遗忘门(gamma_f)和输出门(gamma_o)，使用gamma_u和gamma_f来控制是否更新C，所以GRU更适合搭建复杂网络，相比之下GRU搭建的网络参数会比较少。；\n\t- e. BRNN(Bidirectional RNN)，在RNN的基础上增加了一个反向的RNN，正因为有了双向，所以使得RNN可以使用整段文本的信息，不过这一般需要等待文字/语音输入完毕，不适合于实时系统。；\n\t- f. Deep RNNs，使用RNN、GRU、LSTM作为基本单元，横向、纵向地去搭建更深的网络以完成更复杂的任务。\n\n","source":"_posts/第 1 周学习分享.md","raw":"---\ntitle: 风格迁移与序列模型研究\ndate: 2019-03-30 11:11:00\ncategories:\n- 周总结\ntags: \n- 风格迁移\n- 序列模型 \n- deep learning\n---\n\n<!-- more -->\n\n1. 风格迁移：将一副图片的风格迁移到另一幅新的图片中。\n\t- a. 使用两张图片作为输入（如一张毕加索的图S，一张普通照片C）生成一张新的图片G；\n\t- b. 风格迁移需要定义它的损失函数，内容损失L_content和风格损失L_style，整体损失为L=αL_content(C, G) + βL_style(S, G)； \n\t- c. 有了输入和损失函数，就可以来训练网络了，S和C都是已知的，那G怎么初始化呢？一般采用随机数来填充整幅图像；\n\t- d. 那么内容损失函数L_content具体是怎么定义呢？最简单的，取两幅图像像素差值的二范数。；\n\t- e. L_style如何定义呢？这里公式有点复杂，用文字描述就是单独计算S的通道之间像素相关性（像素值相乘）、G的通道之间像素相关性（像素值相乘），之后作差，求Frobenius范数。；有了输入、初始化、损失，就可以使用优化方法对损失函数进行优化了。\n2. 序列模型：学习RNN及其变种。\n\t- a. 序列模型偏向于NLP、语音等方面；\n\t- b. 首先是RNN网络，相比于标准的层级神经网路，RNN可以处理输入数据长度和输出数据长度不同的情况；不能在文本的不同位置共享已学得的特征。但是RNN不能利用后续的信息，在文本处理方面会有一定的局限性，比如Name Entity Recognition。而且RNN不能处理梯度消失的问题，当RNN层数很深的时候，反向传播时，后面的层，不能去影响前面的层的参数。；\n\t- c. GRU网络，可以说它解决了RNN梯度消失的问题，它引入了C(memory cell)来记忆和更新门(gamma_u)来控制是否更新C或者使用old_C。；\n\t- d. LSTM网络，GRU网络是LSTM的简化版本，后者多了遗忘门(gamma_f)和输出门(gamma_o)，使用gamma_u和gamma_f来控制是否更新C，所以GRU更适合搭建复杂网络，相比之下GRU搭建的网络参数会比较少。；\n\t- e. BRNN(Bidirectional RNN)，在RNN的基础上增加了一个反向的RNN，正因为有了双向，所以使得RNN可以使用整段文本的信息，不过这一般需要等待文字/语音输入完毕，不适合于实时系统。；\n\t- f. Deep RNNs，使用RNN、GRU、LSTM作为基本单元，横向、纵向地去搭建更深的网络以完成更复杂的任务。\n\n","slug":"第 1 周学习分享","published":1,"updated":"2019-05-08T10:50:25.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq1000blnvvqctdzlhk","content":"<a id=\"more\"></a>\n<ol>\n<li>风格迁移：将一副图片的风格迁移到另一幅新的图片中。<ul>\n<li>a. 使用两张图片作为输入（如一张毕加索的图S，一张普通照片C）生成一张新的图片G；</li>\n<li>b. 风格迁移需要定义它的损失函数，内容损失L_content和风格损失L_style，整体损失为L=αL_content(C, G) + βL_style(S, G)； </li>\n<li>c. 有了输入和损失函数，就可以来训练网络了，S和C都是已知的，那G怎么初始化呢？一般采用随机数来填充整幅图像；</li>\n<li>d. 那么内容损失函数L_content具体是怎么定义呢？最简单的，取两幅图像像素差值的二范数。；</li>\n<li>e. L_style如何定义呢？这里公式有点复杂，用文字描述就是单独计算S的通道之间像素相关性（像素值相乘）、G的通道之间像素相关性（像素值相乘），之后作差，求Frobenius范数。；有了输入、初始化、损失，就可以使用优化方法对损失函数进行优化了。</li>\n</ul>\n</li>\n<li>序列模型：学习RNN及其变种。<ul>\n<li>a. 序列模型偏向于NLP、语音等方面；</li>\n<li>b. 首先是RNN网络，相比于标准的层级神经网路，RNN可以处理输入数据长度和输出数据长度不同的情况；不能在文本的不同位置共享已学得的特征。但是RNN不能利用后续的信息，在文本处理方面会有一定的局限性，比如Name Entity Recognition。而且RNN不能处理梯度消失的问题，当RNN层数很深的时候，反向传播时，后面的层，不能去影响前面的层的参数。；</li>\n<li>c. GRU网络，可以说它解决了RNN梯度消失的问题，它引入了C(memory cell)来记忆和更新门(gamma_u)来控制是否更新C或者使用old_C。；</li>\n<li>d. LSTM网络，GRU网络是LSTM的简化版本，后者多了遗忘门(gamma_f)和输出门(gamma_o)，使用gamma_u和gamma_f来控制是否更新C，所以GRU更适合搭建复杂网络，相比之下GRU搭建的网络参数会比较少。；</li>\n<li>e. BRNN(Bidirectional RNN)，在RNN的基础上增加了一个反向的RNN，正因为有了双向，所以使得RNN可以使用整段文本的信息，不过这一般需要等待文字/语音输入完毕，不适合于实时系统。；</li>\n<li>f. Deep RNNs，使用RNN、GRU、LSTM作为基本单元，横向、纵向地去搭建更深的网络以完成更复杂的任务。</li>\n</ul>\n</li>\n</ol>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"","more":"<ol>\n<li>风格迁移：将一副图片的风格迁移到另一幅新的图片中。<ul>\n<li>a. 使用两张图片作为输入（如一张毕加索的图S，一张普通照片C）生成一张新的图片G；</li>\n<li>b. 风格迁移需要定义它的损失函数，内容损失L_content和风格损失L_style，整体损失为L=αL_content(C, G) + βL_style(S, G)； </li>\n<li>c. 有了输入和损失函数，就可以来训练网络了，S和C都是已知的，那G怎么初始化呢？一般采用随机数来填充整幅图像；</li>\n<li>d. 那么内容损失函数L_content具体是怎么定义呢？最简单的，取两幅图像像素差值的二范数。；</li>\n<li>e. L_style如何定义呢？这里公式有点复杂，用文字描述就是单独计算S的通道之间像素相关性（像素值相乘）、G的通道之间像素相关性（像素值相乘），之后作差，求Frobenius范数。；有了输入、初始化、损失，就可以使用优化方法对损失函数进行优化了。</li>\n</ul>\n</li>\n<li>序列模型：学习RNN及其变种。<ul>\n<li>a. 序列模型偏向于NLP、语音等方面；</li>\n<li>b. 首先是RNN网络，相比于标准的层级神经网路，RNN可以处理输入数据长度和输出数据长度不同的情况；不能在文本的不同位置共享已学得的特征。但是RNN不能利用后续的信息，在文本处理方面会有一定的局限性，比如Name Entity Recognition。而且RNN不能处理梯度消失的问题，当RNN层数很深的时候，反向传播时，后面的层，不能去影响前面的层的参数。；</li>\n<li>c. GRU网络，可以说它解决了RNN梯度消失的问题，它引入了C(memory cell)来记忆和更新门(gamma_u)来控制是否更新C或者使用old_C。；</li>\n<li>d. LSTM网络，GRU网络是LSTM的简化版本，后者多了遗忘门(gamma_f)和输出门(gamma_o)，使用gamma_u和gamma_f来控制是否更新C，所以GRU更适合搭建复杂网络，相比之下GRU搭建的网络参数会比较少。；</li>\n<li>e. BRNN(Bidirectional RNN)，在RNN的基础上增加了一个反向的RNN，正因为有了双向，所以使得RNN可以使用整段文本的信息，不过这一般需要等待文字/语音输入完毕，不适合于实时系统。；</li>\n<li>f. Deep RNNs，使用RNN、GRU、LSTM作为基本单元，横向、纵向地去搭建更深的网络以完成更复杂的任务。</li>\n</ul>\n</li>\n</ol>"},{"title":"准备笔试 & NLP基础","date":"2019-04-06T03:11:00.000Z","_content":"\n<!-- more -->\n\n1. 图像基础复习： \n①：相机模型参数，相机畸变系数及矫正;相机模型参数分为内参与外参，内参有焦距f、像素宽度dx和像素高度dy、相机主点u0、v0，外参有R和T。畸变分为径向畸变（桶型、枕型）和切向畸变，前者有k1、k2、k3、k4 4个系数，后者有p1、p2 2个系数。矫正公式略。\n②：常用边缘检测算子，一阶算子：Sobel、Prewitt、Robert、canny；二阶算子：Laplacian，对噪声敏感。Sobel和Prewitt只是权值不同，效果上，Sobel要由于Prewitt，Robert是使用对角差分。其中canny是阶段性算法，分为3个阶段滤波、增强、检测，效果最好。\n2. NLP基础: \n①：学习了Word Embedding，它包含了词与词之间的关系，将一个词用一定维度的向量来表示，作为网络的输入。\n②：如何训练Word Embedding，通常采用context-target pairs的方法，Context的选定有：Last 4 words、4 words on left & right、Last 1 word、Nearby 1 word，通过这种方式来构造监督学习的样本。现成的方案有基于CBOW 的 Word2Vec方法和基于Skip-Gram的Word2Vec方法，CBOW方案是使用周围的词来预测中间的词，Skip-Gram是使用中间的词来预测周围的词，Skip-Gram使用的要多一点。但Skip-Gram中使用softmax进行分类，针对大数据集，softmax的分母计算较耗时，所以出现了层级softmax（二叉树）、Negative Sampling（使用1个正样本，k个负样本来组成一次迭代的样本）。\n③：GloVe：比较简单的Word Embedding方法，通过对一个公式求最小化的参数即可。\n④：用RNN来做情感分类，首先下载已经训练好的Word Embedding，然后将分词的结果通过Word Embedding转换为词向量，再通过搭建好的RNN来进行做情感分类。\n\n","source":"_posts/第 2 周学习分享.md","raw":"---\ntitle: 准备笔试 & NLP基础\ndate: 2019-04-06 11:11:00\ncategories:\n- 周总结\ntags: \n- NLP基础\n- CV基础 \n- 相机模型\n---\n\n<!-- more -->\n\n1. 图像基础复习： \n①：相机模型参数，相机畸变系数及矫正;相机模型参数分为内参与外参，内参有焦距f、像素宽度dx和像素高度dy、相机主点u0、v0，外参有R和T。畸变分为径向畸变（桶型、枕型）和切向畸变，前者有k1、k2、k3、k4 4个系数，后者有p1、p2 2个系数。矫正公式略。\n②：常用边缘检测算子，一阶算子：Sobel、Prewitt、Robert、canny；二阶算子：Laplacian，对噪声敏感。Sobel和Prewitt只是权值不同，效果上，Sobel要由于Prewitt，Robert是使用对角差分。其中canny是阶段性算法，分为3个阶段滤波、增强、检测，效果最好。\n2. NLP基础: \n①：学习了Word Embedding，它包含了词与词之间的关系，将一个词用一定维度的向量来表示，作为网络的输入。\n②：如何训练Word Embedding，通常采用context-target pairs的方法，Context的选定有：Last 4 words、4 words on left & right、Last 1 word、Nearby 1 word，通过这种方式来构造监督学习的样本。现成的方案有基于CBOW 的 Word2Vec方法和基于Skip-Gram的Word2Vec方法，CBOW方案是使用周围的词来预测中间的词，Skip-Gram是使用中间的词来预测周围的词，Skip-Gram使用的要多一点。但Skip-Gram中使用softmax进行分类，针对大数据集，softmax的分母计算较耗时，所以出现了层级softmax（二叉树）、Negative Sampling（使用1个正样本，k个负样本来组成一次迭代的样本）。\n③：GloVe：比较简单的Word Embedding方法，通过对一个公式求最小化的参数即可。\n④：用RNN来做情感分类，首先下载已经训练好的Word Embedding，然后将分词的结果通过Word Embedding转换为词向量，再通过搭建好的RNN来进行做情感分类。\n\n","slug":"第 2 周学习分享","published":1,"updated":"2019-05-08T10:50:25.484Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq3000elnvvyrp3m38o","content":"<a id=\"more\"></a>\n<ol>\n<li>图像基础复习：<br>①：相机模型参数，相机畸变系数及矫正;相机模型参数分为内参与外参，内参有焦距f、像素宽度dx和像素高度dy、相机主点u0、v0，外参有R和T。畸变分为径向畸变（桶型、枕型）和切向畸变，前者有k1、k2、k3、k4 4个系数，后者有p1、p2 2个系数。矫正公式略。<br>②：常用边缘检测算子，一阶算子：Sobel、Prewitt、Robert、canny；二阶算子：Laplacian，对噪声敏感。Sobel和Prewitt只是权值不同，效果上，Sobel要由于Prewitt，Robert是使用对角差分。其中canny是阶段性算法，分为3个阶段滤波、增强、检测，效果最好。</li>\n<li>NLP基础:<br>①：学习了Word Embedding，它包含了词与词之间的关系，将一个词用一定维度的向量来表示，作为网络的输入。<br>②：如何训练Word Embedding，通常采用context-target pairs的方法，Context的选定有：Last 4 words、4 words on left &amp; right、Last 1 word、Nearby 1 word，通过这种方式来构造监督学习的样本。现成的方案有基于CBOW 的 Word2Vec方法和基于Skip-Gram的Word2Vec方法，CBOW方案是使用周围的词来预测中间的词，Skip-Gram是使用中间的词来预测周围的词，Skip-Gram使用的要多一点。但Skip-Gram中使用softmax进行分类，针对大数据集，softmax的分母计算较耗时，所以出现了层级softmax（二叉树）、Negative Sampling（使用1个正样本，k个负样本来组成一次迭代的样本）。<br>③：GloVe：比较简单的Word Embedding方法，通过对一个公式求最小化的参数即可。<br>④：用RNN来做情感分类，首先下载已经训练好的Word Embedding，然后将分词的结果通过Word Embedding转换为词向量，再通过搭建好的RNN来进行做情感分类。</li>\n</ol>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"","more":"<ol>\n<li>图像基础复习：<br>①：相机模型参数，相机畸变系数及矫正;相机模型参数分为内参与外参，内参有焦距f、像素宽度dx和像素高度dy、相机主点u0、v0，外参有R和T。畸变分为径向畸变（桶型、枕型）和切向畸变，前者有k1、k2、k3、k4 4个系数，后者有p1、p2 2个系数。矫正公式略。<br>②：常用边缘检测算子，一阶算子：Sobel、Prewitt、Robert、canny；二阶算子：Laplacian，对噪声敏感。Sobel和Prewitt只是权值不同，效果上，Sobel要由于Prewitt，Robert是使用对角差分。其中canny是阶段性算法，分为3个阶段滤波、增强、检测，效果最好。</li>\n<li>NLP基础:<br>①：学习了Word Embedding，它包含了词与词之间的关系，将一个词用一定维度的向量来表示，作为网络的输入。<br>②：如何训练Word Embedding，通常采用context-target pairs的方法，Context的选定有：Last 4 words、4 words on left &amp; right、Last 1 word、Nearby 1 word，通过这种方式来构造监督学习的样本。现成的方案有基于CBOW 的 Word2Vec方法和基于Skip-Gram的Word2Vec方法，CBOW方案是使用周围的词来预测中间的词，Skip-Gram是使用中间的词来预测周围的词，Skip-Gram使用的要多一点。但Skip-Gram中使用softmax进行分类，针对大数据集，softmax的分母计算较耗时，所以出现了层级softmax（二叉树）、Negative Sampling（使用1个正样本，k个负样本来组成一次迭代的样本）。<br>③：GloVe：比较简单的Word Embedding方法，通过对一个公式求最小化的参数即可。<br>④：用RNN来做情感分类，首先下载已经训练好的Word Embedding，然后将分词的结果通过Word Embedding转换为词向量，再通过搭建好的RNN来进行做情感分类。</li>\n</ol>"},{"title":"YOLO & Attention","date":"2019-04-13T03:11:00.000Z","_content":"这周准备了3场笔试，所以学习新知识的时间不多。主要将Andrew Ng的深度学习课程的最后一节序列模型和注意力机制学完了，同时回顾了一下前面学习的YOLO算法，下面做一个总结。。\n\n<!-- more -->\n1. YOLO (v1）：\n\n- YOLO算法，首先将图像网格化，然后将目标按中心所在位置分配给所在格子，然后利用卷积网络的参数共享特性，只需要一次卷积就可以得出结果，加速计算，能做到实时。\n- 对每个格子都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h和confidence，在原文中作者取S=7，B=2.\n- 为了解决有很多检测框的问题，h和confidence，保留最大的，其他的删除。\n- 为了解决有多个目标出现在同一个grid里的情况，通过预先设置Anchor box模板，来解决，假设会有2个目标同时出现在同一个grid中，行人和汽车，就设置两个Anchor box1和Anchor box2，同时将输出y，这里y里面的元素就会有10个了。再对于每个类别，单独运行非极大值抑制，就可以得到最后的结果。\n- 如何选择anchor box：\n1、可以人工的指定Anchor box的形状，使其包含训练集中的大多数样本，个数一般5-10个左右。\n2、还有更好的做法，k-means算法，对形状进行聚类。\n\n2. Attention：\nAttention说白了是做一个向量加权，在NLP、语音和图像描述等方面上得到了应用。\n\n    2.1. Attention的定义：\n\n    - 给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。\n\n    - attention的重点就是这个集合values中的每个value的“权值”的计算方法。\n\n    - 有时候也把这种attention的机制叫做query的输出关注了（或者说叫考虑到了）原文的不同部分\n   \n   通过计算Attention Scores，然后利用softmax函数将其概率化。\n   \n    2.2. Attention的变体\n   \n    - soft attention、global attention、动态attention\n    - Hard attention\n    - local attention\n    - 静态attention\n    - 强制前向attention\n    - self attention\n    - key-value attention\n    - multi-head attention\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/第 3 周学习分享.md","raw":"---\ntitle: YOLO & Attention\ndate: 2019-04-13 11:11:00\ncategories:\n- 周总结\ntags: \n- YOLO\n- Attention \n- deep learning\n---\n这周准备了3场笔试，所以学习新知识的时间不多。主要将Andrew Ng的深度学习课程的最后一节序列模型和注意力机制学完了，同时回顾了一下前面学习的YOLO算法，下面做一个总结。。\n\n<!-- more -->\n1. YOLO (v1）：\n\n- YOLO算法，首先将图像网格化，然后将目标按中心所在位置分配给所在格子，然后利用卷积网络的参数共享特性，只需要一次卷积就可以得出结果，加速计算，能做到实时。\n- 对每个格子都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h和confidence，在原文中作者取S=7，B=2.\n- 为了解决有很多检测框的问题，h和confidence，保留最大的，其他的删除。\n- 为了解决有多个目标出现在同一个grid里的情况，通过预先设置Anchor box模板，来解决，假设会有2个目标同时出现在同一个grid中，行人和汽车，就设置两个Anchor box1和Anchor box2，同时将输出y，这里y里面的元素就会有10个了。再对于每个类别，单独运行非极大值抑制，就可以得到最后的结果。\n- 如何选择anchor box：\n1、可以人工的指定Anchor box的形状，使其包含训练集中的大多数样本，个数一般5-10个左右。\n2、还有更好的做法，k-means算法，对形状进行聚类。\n\n2. Attention：\nAttention说白了是做一个向量加权，在NLP、语音和图像描述等方面上得到了应用。\n\n    2.1. Attention的定义：\n\n    - 给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。\n\n    - attention的重点就是这个集合values中的每个value的“权值”的计算方法。\n\n    - 有时候也把这种attention的机制叫做query的输出关注了（或者说叫考虑到了）原文的不同部分\n   \n   通过计算Attention Scores，然后利用softmax函数将其概率化。\n   \n    2.2. Attention的变体\n   \n    - soft attention、global attention、动态attention\n    - Hard attention\n    - local attention\n    - 静态attention\n    - 强制前向attention\n    - self attention\n    - key-value attention\n    - multi-head attention\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"第 3 周学习分享","published":1,"updated":"2019-05-08T10:50:25.485Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq5000glnvvb8hbqsrw","content":"<p>这周准备了3场笔试，所以学习新知识的时间不多。主要将Andrew Ng的深度学习课程的最后一节序列模型和注意力机制学完了，同时回顾了一下前面学习的YOLO算法，下面做一个总结。。</p>\n<a id=\"more\"></a>\n<ol>\n<li>YOLO (v1）：</li>\n</ol>\n<ul>\n<li>YOLO算法，首先将图像网格化，然后将目标按中心所在位置分配给所在格子，然后利用卷积网络的参数共享特性，只需要一次卷积就可以得出结果，加速计算，能做到实时。</li>\n<li>对每个格子都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h和confidence，在原文中作者取S=7，B=2.</li>\n<li>为了解决有很多检测框的问题，h和confidence，保留最大的，其他的删除。</li>\n<li>为了解决有多个目标出现在同一个grid里的情况，通过预先设置Anchor box模板，来解决，假设会有2个目标同时出现在同一个grid中，行人和汽车，就设置两个Anchor box1和Anchor box2，同时将输出y，这里y里面的元素就会有10个了。再对于每个类别，单独运行非极大值抑制，就可以得到最后的结果。</li>\n<li>如何选择anchor box：<br>1、可以人工的指定Anchor box的形状，使其包含训练集中的大多数样本，个数一般5-10个左右。<br>2、还有更好的做法，k-means算法，对形状进行聚类。</li>\n</ul>\n<ol start=\"2\">\n<li><p>Attention：<br>Attention说白了是做一个向量加权，在NLP、语音和图像描述等方面上得到了应用。</p>\n<p> 2.1. Attention的定义：</p>\n<ul>\n<li><p>给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。</p>\n</li>\n<li><p>attention的重点就是这个集合values中的每个value的“权值”的计算方法。</p>\n</li>\n<li><p>有时候也把这种attention的机制叫做query的输出关注了（或者说叫考虑到了）原文的不同部分</p>\n</li>\n</ul>\n<p>通过计算Attention Scores，然后利用softmax函数将其概率化。</p>\n<p> 2.2. Attention的变体</p>\n<ul>\n<li>soft attention、global attention、动态attention</li>\n<li>Hard attention</li>\n<li>local attention</li>\n<li>静态attention</li>\n<li>强制前向attention</li>\n<li>self attention</li>\n<li>key-value attention</li>\n<li>multi-head attention</li>\n</ul>\n</li>\n</ol>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>这周准备了3场笔试，所以学习新知识的时间不多。主要将Andrew Ng的深度学习课程的最后一节序列模型和注意力机制学完了，同时回顾了一下前面学习的YOLO算法，下面做一个总结。。</p>","more":"<ol>\n<li>YOLO (v1）：</li>\n</ol>\n<ul>\n<li>YOLO算法，首先将图像网格化，然后将目标按中心所在位置分配给所在格子，然后利用卷积网络的参数共享特性，只需要一次卷积就可以得出结果，加速计算，能做到实时。</li>\n<li>对每个格子都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h和confidence，在原文中作者取S=7，B=2.</li>\n<li>为了解决有很多检测框的问题，h和confidence，保留最大的，其他的删除。</li>\n<li>为了解决有多个目标出现在同一个grid里的情况，通过预先设置Anchor box模板，来解决，假设会有2个目标同时出现在同一个grid中，行人和汽车，就设置两个Anchor box1和Anchor box2，同时将输出y，这里y里面的元素就会有10个了。再对于每个类别，单独运行非极大值抑制，就可以得到最后的结果。</li>\n<li>如何选择anchor box：<br>1、可以人工的指定Anchor box的形状，使其包含训练集中的大多数样本，个数一般5-10个左右。<br>2、还有更好的做法，k-means算法，对形状进行聚类。</li>\n</ul>\n<ol start=\"2\">\n<li><p>Attention：<br>Attention说白了是做一个向量加权，在NLP、语音和图像描述等方面上得到了应用。</p>\n<p> 2.1. Attention的定义：</p>\n<ul>\n<li><p>给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。</p>\n</li>\n<li><p>attention的重点就是这个集合values中的每个value的“权值”的计算方法。</p>\n</li>\n<li><p>有时候也把这种attention的机制叫做query的输出关注了（或者说叫考虑到了）原文的不同部分</p>\n</li>\n</ul>\n<p>通过计算Attention Scores，然后利用softmax函数将其概率化。</p>\n<p> 2.2. Attention的变体</p>\n<ul>\n<li>soft attention、global attention、动态attention</li>\n<li>Hard attention</li>\n<li>local attention</li>\n<li>静态attention</li>\n<li>强制前向attention</li>\n<li>self attention</li>\n<li>key-value attention</li>\n<li>multi-head attention</li>\n</ul>\n</li>\n</ol>"},{"title":"GN 论文总结","date":"2019-04-28T03:11:00.000Z","_content":"\nGN(Group Normalization)解决了在Batch Size太小时BN失效的问题。总结如下：\n\n<!-- more -->\n\n### 唐洋的学习心得\n介绍GN之前，先回顾下BN。\n\n##### 1. BN的优点\n\n- BN可以使网络中每层输入数据的分布相对稳定，加速模型的学习速度\n- BN可以使模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定\n- BN可以允许网络使用饱和性激活函数（sigmoid & tanh），缓解梯度消失问题\n- BN具有一定的正则化效果\n\n#### 2. BN的缺点\n先不管它应用在什么任务（像素级图片生成）或网络（RNN）上面会有缺陷。\n\n- 对于限制了Batch Size的任务来说，BN会失效\n- 对于训练时计算的均值和方差，在验证和测试的时候并不适用（如果训练、验证、测试集的分布不同）\n\n#### 3. GN\nGN [here](https://arxiv.org/pdf/1803.08494.pdf) 抛弃了对Batch Size大小的依赖，是BN的一种替代方法。\n\nGroup的思想来自于：AlexNet的group convolutions，MobileNet & Xception 的channel-wise convolutions, ShuffleNet的channle shuffle operation，SIFT、HOG的group wise 金字塔等。\n\n![GN示意图](https://github.com/facebookresearch/Detectron/blob/master/projects/GN/gn.jpg)\n\n**Normalization methods**. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels. In rightmost figure, which is a simple case of 2 groups (G = 2) each having 3 channels.\n\n**主要步骤**：首先将Channels划分为多个groups，再计算每个group内的均值μ和方差σ，以进行归一化。\n\n##### 3.1 如何划分groups？\n也就是我上周总结中提到的几何S如何确定。\n\n**首先在2D图像中，使用4D向量来表示,(N,C,H,W),N表示batch axis, C表示channel axis,H & W表示height & width axis**.\n\n- BN是在同一个C axis上进行，即计算μ和σ是在（N, H, W）axes.\n- LN (layer)是在同一个N axis上进行，即计算μ和σ是在(C, H, W) axes.\n- IN (Instance)是在同一个N axis和C axis上进行，即计算μ和σ是在(H,W) axes.\n- GN也是在同一个N axis 和C axis上进行，计算μ和σ是在(H,W) axes和C/G channels.\n\nGN计算公式是原文的公式1（也是其他几种Normalization的方法的计算公式，只是集合S的确定方式不同），其中μ和σ如原文公式2所示，确定集合S的公式如公式7所示。\n\nGN公式中(原文公式7)，G表示分组数量（默认32），C/G表示每组的通道数量，计算示意图如上图中的最右边，例子中G=2，每组的channels=3.\n\n##### 3.2 代码实现(tf)\n```\n\tdef GroupNorm(x, gamma, beta, G, eps=1e-5):\n\t\t# x: input features with shape [N, C, H, W]\n\t\t# gamma: beta: scale and offset, with shape [1, C, 1, 1]\n\t\t# G: number of groups for GN\n\t\tN, G, H, W = x.shape\n\t\tx = tf.reshape(x, [N, G, C//G, H, W])\n\t\t\n\t\t# [2,3,4]表示 G,H,W axes\n\t\tmean, var = tf.nn.moments(x, [2,3,4], keep_dim=True)\n\t\tx = (x - mean) / tf.sqrt(var + eps)\n\t\t\n\t\tx = tf.reshape(x, [N, C, H, W])\n\t\t\n\t\treturn x * gamma + beta \n```\n","source":"_posts/第 5 周学习分享.md","raw":"---\ntitle: GN 论文总结\ndate: 2019-04-28 11:11:00\ncategories:\n- 周总结\ntags: \n- GN\n- deep learning\n---\n\nGN(Group Normalization)解决了在Batch Size太小时BN失效的问题。总结如下：\n\n<!-- more -->\n\n### 唐洋的学习心得\n介绍GN之前，先回顾下BN。\n\n##### 1. BN的优点\n\n- BN可以使网络中每层输入数据的分布相对稳定，加速模型的学习速度\n- BN可以使模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定\n- BN可以允许网络使用饱和性激活函数（sigmoid & tanh），缓解梯度消失问题\n- BN具有一定的正则化效果\n\n#### 2. BN的缺点\n先不管它应用在什么任务（像素级图片生成）或网络（RNN）上面会有缺陷。\n\n- 对于限制了Batch Size的任务来说，BN会失效\n- 对于训练时计算的均值和方差，在验证和测试的时候并不适用（如果训练、验证、测试集的分布不同）\n\n#### 3. GN\nGN [here](https://arxiv.org/pdf/1803.08494.pdf) 抛弃了对Batch Size大小的依赖，是BN的一种替代方法。\n\nGroup的思想来自于：AlexNet的group convolutions，MobileNet & Xception 的channel-wise convolutions, ShuffleNet的channle shuffle operation，SIFT、HOG的group wise 金字塔等。\n\n![GN示意图](https://github.com/facebookresearch/Detectron/blob/master/projects/GN/gn.jpg)\n\n**Normalization methods**. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels. In rightmost figure, which is a simple case of 2 groups (G = 2) each having 3 channels.\n\n**主要步骤**：首先将Channels划分为多个groups，再计算每个group内的均值μ和方差σ，以进行归一化。\n\n##### 3.1 如何划分groups？\n也就是我上周总结中提到的几何S如何确定。\n\n**首先在2D图像中，使用4D向量来表示,(N,C,H,W),N表示batch axis, C表示channel axis,H & W表示height & width axis**.\n\n- BN是在同一个C axis上进行，即计算μ和σ是在（N, H, W）axes.\n- LN (layer)是在同一个N axis上进行，即计算μ和σ是在(C, H, W) axes.\n- IN (Instance)是在同一个N axis和C axis上进行，即计算μ和σ是在(H,W) axes.\n- GN也是在同一个N axis 和C axis上进行，计算μ和σ是在(H,W) axes和C/G channels.\n\nGN计算公式是原文的公式1（也是其他几种Normalization的方法的计算公式，只是集合S的确定方式不同），其中μ和σ如原文公式2所示，确定集合S的公式如公式7所示。\n\nGN公式中(原文公式7)，G表示分组数量（默认32），C/G表示每组的通道数量，计算示意图如上图中的最右边，例子中G=2，每组的channels=3.\n\n##### 3.2 代码实现(tf)\n```\n\tdef GroupNorm(x, gamma, beta, G, eps=1e-5):\n\t\t# x: input features with shape [N, C, H, W]\n\t\t# gamma: beta: scale and offset, with shape [1, C, 1, 1]\n\t\t# G: number of groups for GN\n\t\tN, G, H, W = x.shape\n\t\tx = tf.reshape(x, [N, G, C//G, H, W])\n\t\t\n\t\t# [2,3,4]表示 G,H,W axes\n\t\tmean, var = tf.nn.moments(x, [2,3,4], keep_dim=True)\n\t\tx = (x - mean) / tf.sqrt(var + eps)\n\t\t\n\t\tx = tf.reshape(x, [N, C, H, W])\n\t\t\n\t\treturn x * gamma + beta \n```\n","slug":"第 5 周学习分享","published":1,"updated":"2019-05-08T10:50:25.485Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq6000klnvvnkekv1pd","content":"<p>GN(Group Normalization)解决了在Batch Size太小时BN失效的问题。总结如下：</p>\n<a id=\"more\"></a>\n<h3 id=\"唐洋的学习心得\"><a href=\"#唐洋的学习心得\" class=\"headerlink\" title=\"唐洋的学习心得\"></a>唐洋的学习心得</h3><p>介绍GN之前，先回顾下BN。</p>\n<h5 id=\"1-BN的优点\"><a href=\"#1-BN的优点\" class=\"headerlink\" title=\"1. BN的优点\"></a>1. BN的优点</h5><ul>\n<li>BN可以使网络中每层输入数据的分布相对稳定，加速模型的学习速度</li>\n<li>BN可以使模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</li>\n<li>BN可以允许网络使用饱和性激活函数（sigmoid &amp; tanh），缓解梯度消失问题</li>\n<li>BN具有一定的正则化效果</li>\n</ul>\n<h4 id=\"2-BN的缺点\"><a href=\"#2-BN的缺点\" class=\"headerlink\" title=\"2. BN的缺点\"></a>2. BN的缺点</h4><p>先不管它应用在什么任务（像素级图片生成）或网络（RNN）上面会有缺陷。</p>\n<ul>\n<li>对于限制了Batch Size的任务来说，BN会失效</li>\n<li>对于训练时计算的均值和方差，在验证和测试的时候并不适用（如果训练、验证、测试集的分布不同）</li>\n</ul>\n<h4 id=\"3-GN\"><a href=\"#3-GN\" class=\"headerlink\" title=\"3. GN\"></a>3. GN</h4><p>GN <a href=\"https://arxiv.org/pdf/1803.08494.pdf\" target=\"_blank\" rel=\"noopener\">here</a> 抛弃了对Batch Size大小的依赖，是BN的一种替代方法。</p>\n<p>Group的思想来自于：AlexNet的group convolutions，MobileNet &amp; Xception 的channel-wise convolutions, ShuffleNet的channle shuffle operation，SIFT、HOG的group wise 金字塔等。</p>\n<p><img src=\"https://github.com/facebookresearch/Detectron/blob/master/projects/GN/gn.jpg\" alt=\"GN示意图\"></p>\n<p><strong>Normalization methods</strong>. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels. In rightmost figure, which is a simple case of 2 groups (G = 2) each having 3 channels.</p>\n<p><strong>主要步骤</strong>：首先将Channels划分为多个groups，再计算每个group内的均值μ和方差σ，以进行归一化。</p>\n<h5 id=\"3-1-如何划分groups？\"><a href=\"#3-1-如何划分groups？\" class=\"headerlink\" title=\"3.1 如何划分groups？\"></a>3.1 如何划分groups？</h5><p>也就是我上周总结中提到的几何S如何确定。</p>\n<p><strong>首先在2D图像中，使用4D向量来表示,(N,C,H,W),N表示batch axis, C表示channel axis,H &amp; W表示height &amp; width axis</strong>.</p>\n<ul>\n<li>BN是在同一个C axis上进行，即计算μ和σ是在（N, H, W）axes.</li>\n<li>LN (layer)是在同一个N axis上进行，即计算μ和σ是在(C, H, W) axes.</li>\n<li>IN (Instance)是在同一个N axis和C axis上进行，即计算μ和σ是在(H,W) axes.</li>\n<li>GN也是在同一个N axis 和C axis上进行，计算μ和σ是在(H,W) axes和C/G channels.</li>\n</ul>\n<p>GN计算公式是原文的公式1（也是其他几种Normalization的方法的计算公式，只是集合S的确定方式不同），其中μ和σ如原文公式2所示，确定集合S的公式如公式7所示。</p>\n<p>GN公式中(原文公式7)，G表示分组数量（默认32），C/G表示每组的通道数量，计算示意图如上图中的最右边，例子中G=2，每组的channels=3.</p>\n<h5 id=\"3-2-代码实现-tf\"><a href=\"#3-2-代码实现-tf\" class=\"headerlink\" title=\"3.2 代码实现(tf)\"></a>3.2 代码实现(tf)</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def GroupNorm(x, gamma, beta, G, eps=1e-5):</span><br><span class=\"line\">\t# x: input features with shape [N, C, H, W]</span><br><span class=\"line\">\t# gamma: beta: scale and offset, with shape [1, C, 1, 1]</span><br><span class=\"line\">\t# G: number of groups for GN</span><br><span class=\"line\">\tN, G, H, W = x.shape</span><br><span class=\"line\">\tx = tf.reshape(x, [N, G, C//G, H, W])</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t# [2,3,4]表示 G,H,W axes</span><br><span class=\"line\">\tmean, var = tf.nn.moments(x, [2,3,4], keep_dim=True)</span><br><span class=\"line\">\tx = (x - mean) / tf.sqrt(var + eps)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tx = tf.reshape(x, [N, C, H, W])</span><br><span class=\"line\">\t</span><br><span class=\"line\">\treturn x * gamma + beta</span><br></pre></td></tr></table></figure>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>GN(Group Normalization)解决了在Batch Size太小时BN失效的问题。总结如下：</p>","more":"<h3 id=\"唐洋的学习心得\"><a href=\"#唐洋的学习心得\" class=\"headerlink\" title=\"唐洋的学习心得\"></a>唐洋的学习心得</h3><p>介绍GN之前，先回顾下BN。</p>\n<h5 id=\"1-BN的优点\"><a href=\"#1-BN的优点\" class=\"headerlink\" title=\"1. BN的优点\"></a>1. BN的优点</h5><ul>\n<li>BN可以使网络中每层输入数据的分布相对稳定，加速模型的学习速度</li>\n<li>BN可以使模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</li>\n<li>BN可以允许网络使用饱和性激活函数（sigmoid &amp; tanh），缓解梯度消失问题</li>\n<li>BN具有一定的正则化效果</li>\n</ul>\n<h4 id=\"2-BN的缺点\"><a href=\"#2-BN的缺点\" class=\"headerlink\" title=\"2. BN的缺点\"></a>2. BN的缺点</h4><p>先不管它应用在什么任务（像素级图片生成）或网络（RNN）上面会有缺陷。</p>\n<ul>\n<li>对于限制了Batch Size的任务来说，BN会失效</li>\n<li>对于训练时计算的均值和方差，在验证和测试的时候并不适用（如果训练、验证、测试集的分布不同）</li>\n</ul>\n<h4 id=\"3-GN\"><a href=\"#3-GN\" class=\"headerlink\" title=\"3. GN\"></a>3. GN</h4><p>GN <a href=\"https://arxiv.org/pdf/1803.08494.pdf\" target=\"_blank\" rel=\"noopener\">here</a> 抛弃了对Batch Size大小的依赖，是BN的一种替代方法。</p>\n<p>Group的思想来自于：AlexNet的group convolutions，MobileNet &amp; Xception 的channel-wise convolutions, ShuffleNet的channle shuffle operation，SIFT、HOG的group wise 金字塔等。</p>\n<p><img src=\"https://github.com/facebookresearch/Detectron/blob/master/projects/GN/gn.jpg\" alt=\"GN示意图\"></p>\n<p><strong>Normalization methods</strong>. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels. In rightmost figure, which is a simple case of 2 groups (G = 2) each having 3 channels.</p>\n<p><strong>主要步骤</strong>：首先将Channels划分为多个groups，再计算每个group内的均值μ和方差σ，以进行归一化。</p>\n<h5 id=\"3-1-如何划分groups？\"><a href=\"#3-1-如何划分groups？\" class=\"headerlink\" title=\"3.1 如何划分groups？\"></a>3.1 如何划分groups？</h5><p>也就是我上周总结中提到的几何S如何确定。</p>\n<p><strong>首先在2D图像中，使用4D向量来表示,(N,C,H,W),N表示batch axis, C表示channel axis,H &amp; W表示height &amp; width axis</strong>.</p>\n<ul>\n<li>BN是在同一个C axis上进行，即计算μ和σ是在（N, H, W）axes.</li>\n<li>LN (layer)是在同一个N axis上进行，即计算μ和σ是在(C, H, W) axes.</li>\n<li>IN (Instance)是在同一个N axis和C axis上进行，即计算μ和σ是在(H,W) axes.</li>\n<li>GN也是在同一个N axis 和C axis上进行，计算μ和σ是在(H,W) axes和C/G channels.</li>\n</ul>\n<p>GN计算公式是原文的公式1（也是其他几种Normalization的方法的计算公式，只是集合S的确定方式不同），其中μ和σ如原文公式2所示，确定集合S的公式如公式7所示。</p>\n<p>GN公式中(原文公式7)，G表示分组数量（默认32），C/G表示每组的通道数量，计算示意图如上图中的最右边，例子中G=2，每组的channels=3.</p>\n<h5 id=\"3-2-代码实现-tf\"><a href=\"#3-2-代码实现-tf\" class=\"headerlink\" title=\"3.2 代码实现(tf)\"></a>3.2 代码实现(tf)</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def GroupNorm(x, gamma, beta, G, eps=1e-5):</span><br><span class=\"line\">\t# x: input features with shape [N, C, H, W]</span><br><span class=\"line\">\t# gamma: beta: scale and offset, with shape [1, C, 1, 1]</span><br><span class=\"line\">\t# G: number of groups for GN</span><br><span class=\"line\">\tN, G, H, W = x.shape</span><br><span class=\"line\">\tx = tf.reshape(x, [N, G, C//G, H, W])</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t# [2,3,4]表示 G,H,W axes</span><br><span class=\"line\">\tmean, var = tf.nn.moments(x, [2,3,4], keep_dim=True)</span><br><span class=\"line\">\tx = (x - mean) / tf.sqrt(var + eps)</span><br><span class=\"line\">\t</span><br><span class=\"line\">\tx = tf.reshape(x, [N, C, H, W])</span><br><span class=\"line\">\t</span><br><span class=\"line\">\treturn x * gamma + beta</span><br></pre></td></tr></table></figure>"},{"title":"学习基于深度学习的目标检测框架","date":"2019-05-08T09:15:00.000Z","_content":"总结R-CNN、SPP-NET、Fast R-CNN、Faster R-CNN、YOLO(v1 v2)、SSD [参考](https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fast-r-cnn)\n<!-- more -->\n\n### 提前的总结\n#### 方案1.候选区域 + 深度学习回归（two stage）\n[R-CNN](http://arxiv.org/abs/1311.2524)，2014（Selective Search + CNN + SVM），首先预选2000个框\n\n[SPP-Net](http://arxiv.org/abs/1406.4729)，2015（ROI Pooling），一次卷积，不限制输入尺寸\n\n[Fast R-CNN](http://arxiv.org/abs/1504.08083)，2015（Selective Search + CNN + ROI），结合R-CNN和SPP-Net的思路。将分类和回归都加入到网络中进行训练，做成了端到端。\n\n[Faster R-CNN](http://arxiv.org/abs/1506.01497)，2015（RPN + CNN + ROI），用RPN来求候选框，同时引入anchor机制来应对目标形变问题。\n\n#### 方案2. 深度学习回归（one stage）\n[YOLO v1](http://arxiv.org/abs/1506.02640)，2016（回归任务）\n\n[YOLO v2](https://arxiv.org/abs/1612.08242)，2017，引入Faster R-CNN的anchor机制\n\n[SSD](http://arxiv.org/abs/1512.02325)，2016，结合YOLO的回归和Faster R-CNN的anchor\n\n---\n\n### 细节讨论\n\n#### 1.R-CNN\n1. 候选区域：利用颜色、纹理、边缘信息，使用selective search提取一定（2000）的框（在保持较高的recall情况下）。对于每个框的区域，需要修正区域大小，以适合于CNN的输入，并提取特征。\n2. SVM分类：训练一个二分类SVM，来判断当前候选框里物体的类别。\n3. 回归：使用回归来修正候选框的位置。\n4. **问题1**：对2000个框都要进行CNN，框与框有重叠，时间复杂度高，在重叠区域的计算是不必要的，因此可以提速。\n5. **问题2**：框的尺寸不一，需要resize，但是resize就会出现失真的问题。肯定会影响精度。\n\n#### 2.SPP-Net\n针对R-CNN的两个问题，SPP-Net就被提出来了。\n\n1. ROI Pooling：在普通的CNN结构中加入了ROI Pooling，其中的pooling filter可根据输入调整大小，使得输出是一个固定维度的向量，输入可以是任意的尺寸。然后给到全连接FC层。\n2. 对原图只做一次卷积，首先得到整张图的feature map，再找到每个候选框在feature map上所对应的区域，将这个区域的特征输入到ROI Pooling，完成特征提取的工作。\n\n#### 3.Fast R-CNN\n在R-CNN的基础上结合了SPP-Net的思路，相比原来R-CNN，使用一次卷积；在最后一次卷积后添加了ROI Pooling；将边框回归直接加入到了CNN网络中，因此做成了端到端网络。\n\nR-CNN分为三个阶段，Fast R-CNN使用softmax来代替SVM，边框回归也加入到网络中，因此成为了端到端的网络。**这可以为后面改进阶段性的算法作为一个重要的提示**\n\nR-CNN的方法：2000个候选框,resize -> CNN提取特征 -> 分类+回归\n\nFast R-CNN方法:原始图片 -> CNN -> ROI Pooling -> 分类+回归\n\n相比R-CNN训练速度提升了8.8x， 测试速度提升了146x\n\n**问题** Selective Search寻找候选框非常耗时，\n\n#### 4.Faster R-CNN\n引入RPN网络代替selective search，使用RPN来搜索候选框，同时引入anchor box应对目标形状的变化问题。\n\n1. RPN：将RPN放在最后一个卷积层后面，所以RPN是作用在feature map上的，RPN包含分类和边框回归两个任务，分类器判断框属于的类别，边框回归进一步的优化/精细候选框。\n\n---\n\n#### 5.YOLO v1\n*two stage*的方案不能满足实时性要求，所以one stage的出来了。\n\n1. 首先将图像划分为7x7的网格\n2. 对于每个网格，都预测2个边框（包括边框是目标的置信度和类别概率），最终可以得到7x7x2个边框，再经过NMS，即可得到最终的回归框。\n3. **问题1**：只是用7x7的网格，使得回归的精度不够高。而且当每个格子中包含多个物体（>2）时，不能被检测出来。\n4. **问题2**：输入图像输入resize到224*224\n\n#### 6.YOLO v2\n引入了Faster R-CNN的anchor box机制，并使用卷积层代替YOLO v1的全连接层，所以对输入图像不需要resize了。\n\n1. 改进anchor box：Faster R-CNN中需要首选anchor box，而v2中采用k-means在许多框中进行聚类，以产生合适的框，同时使用过IOU作为k-means的距离计算。\n2. 引入skip layer：借鉴ResNet的思想，使得网络更深。\n\n#### 7.SSD\nSSD结合了YOLO回归的思想和Faster R-CNN的anchor box机制，YOLO中预测某个位置使用的全图特征，SSD预测某个位置使用的这个位置周围的特征。\n\n同时SSD的anchor是在多个feature map上进行的，这样就可以做的多尺度。\n","source":"_posts/第 7 周学习分享.md","raw":"---\ntitle: 学习基于深度学习的目标检测框架\ndate: 2019-5-8 17:15:00\ncategories:\n- 周总结\ntags: \n- 目标检测\n- R-CNN系列\n- YOLO\n- SSD\n- deep learning\n---\n总结R-CNN、SPP-NET、Fast R-CNN、Faster R-CNN、YOLO(v1 v2)、SSD [参考](https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fast-r-cnn)\n<!-- more -->\n\n### 提前的总结\n#### 方案1.候选区域 + 深度学习回归（two stage）\n[R-CNN](http://arxiv.org/abs/1311.2524)，2014（Selective Search + CNN + SVM），首先预选2000个框\n\n[SPP-Net](http://arxiv.org/abs/1406.4729)，2015（ROI Pooling），一次卷积，不限制输入尺寸\n\n[Fast R-CNN](http://arxiv.org/abs/1504.08083)，2015（Selective Search + CNN + ROI），结合R-CNN和SPP-Net的思路。将分类和回归都加入到网络中进行训练，做成了端到端。\n\n[Faster R-CNN](http://arxiv.org/abs/1506.01497)，2015（RPN + CNN + ROI），用RPN来求候选框，同时引入anchor机制来应对目标形变问题。\n\n#### 方案2. 深度学习回归（one stage）\n[YOLO v1](http://arxiv.org/abs/1506.02640)，2016（回归任务）\n\n[YOLO v2](https://arxiv.org/abs/1612.08242)，2017，引入Faster R-CNN的anchor机制\n\n[SSD](http://arxiv.org/abs/1512.02325)，2016，结合YOLO的回归和Faster R-CNN的anchor\n\n---\n\n### 细节讨论\n\n#### 1.R-CNN\n1. 候选区域：利用颜色、纹理、边缘信息，使用selective search提取一定（2000）的框（在保持较高的recall情况下）。对于每个框的区域，需要修正区域大小，以适合于CNN的输入，并提取特征。\n2. SVM分类：训练一个二分类SVM，来判断当前候选框里物体的类别。\n3. 回归：使用回归来修正候选框的位置。\n4. **问题1**：对2000个框都要进行CNN，框与框有重叠，时间复杂度高，在重叠区域的计算是不必要的，因此可以提速。\n5. **问题2**：框的尺寸不一，需要resize，但是resize就会出现失真的问题。肯定会影响精度。\n\n#### 2.SPP-Net\n针对R-CNN的两个问题，SPP-Net就被提出来了。\n\n1. ROI Pooling：在普通的CNN结构中加入了ROI Pooling，其中的pooling filter可根据输入调整大小，使得输出是一个固定维度的向量，输入可以是任意的尺寸。然后给到全连接FC层。\n2. 对原图只做一次卷积，首先得到整张图的feature map，再找到每个候选框在feature map上所对应的区域，将这个区域的特征输入到ROI Pooling，完成特征提取的工作。\n\n#### 3.Fast R-CNN\n在R-CNN的基础上结合了SPP-Net的思路，相比原来R-CNN，使用一次卷积；在最后一次卷积后添加了ROI Pooling；将边框回归直接加入到了CNN网络中，因此做成了端到端网络。\n\nR-CNN分为三个阶段，Fast R-CNN使用softmax来代替SVM，边框回归也加入到网络中，因此成为了端到端的网络。**这可以为后面改进阶段性的算法作为一个重要的提示**\n\nR-CNN的方法：2000个候选框,resize -> CNN提取特征 -> 分类+回归\n\nFast R-CNN方法:原始图片 -> CNN -> ROI Pooling -> 分类+回归\n\n相比R-CNN训练速度提升了8.8x， 测试速度提升了146x\n\n**问题** Selective Search寻找候选框非常耗时，\n\n#### 4.Faster R-CNN\n引入RPN网络代替selective search，使用RPN来搜索候选框，同时引入anchor box应对目标形状的变化问题。\n\n1. RPN：将RPN放在最后一个卷积层后面，所以RPN是作用在feature map上的，RPN包含分类和边框回归两个任务，分类器判断框属于的类别，边框回归进一步的优化/精细候选框。\n\n---\n\n#### 5.YOLO v1\n*two stage*的方案不能满足实时性要求，所以one stage的出来了。\n\n1. 首先将图像划分为7x7的网格\n2. 对于每个网格，都预测2个边框（包括边框是目标的置信度和类别概率），最终可以得到7x7x2个边框，再经过NMS，即可得到最终的回归框。\n3. **问题1**：只是用7x7的网格，使得回归的精度不够高。而且当每个格子中包含多个物体（>2）时，不能被检测出来。\n4. **问题2**：输入图像输入resize到224*224\n\n#### 6.YOLO v2\n引入了Faster R-CNN的anchor box机制，并使用卷积层代替YOLO v1的全连接层，所以对输入图像不需要resize了。\n\n1. 改进anchor box：Faster R-CNN中需要首选anchor box，而v2中采用k-means在许多框中进行聚类，以产生合适的框，同时使用过IOU作为k-means的距离计算。\n2. 引入skip layer：借鉴ResNet的思想，使得网络更深。\n\n#### 7.SSD\nSSD结合了YOLO回归的思想和Faster R-CNN的anchor box机制，YOLO中预测某个位置使用的全图特征，SSD预测某个位置使用的这个位置周围的特征。\n\n同时SSD的anchor是在多个feature map上进行的，这样就可以做的多尺度。\n","slug":"第 7 周学习分享","published":1,"updated":"2019-05-08T10:50:25.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq7000llnvvwcqv1poz","content":"<p>总结R-CNN、SPP-NET、Fast R-CNN、Faster R-CNN、YOLO(v1 v2)、SSD <a href=\"https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fast-r-cnn\" target=\"_blank\" rel=\"noopener\">参考</a><br><a id=\"more\"></a></p>\n<h3 id=\"提前的总结\"><a href=\"#提前的总结\" class=\"headerlink\" title=\"提前的总结\"></a>提前的总结</h3><h4 id=\"方案1-候选区域-深度学习回归（two-stage）\"><a href=\"#方案1-候选区域-深度学习回归（two-stage）\" class=\"headerlink\" title=\"方案1.候选区域 + 深度学习回归（two stage）\"></a>方案1.候选区域 + 深度学习回归（two stage）</h4><p><a href=\"http://arxiv.org/abs/1311.2524\" target=\"_blank\" rel=\"noopener\">R-CNN</a>，2014（Selective Search + CNN + SVM），首先预选2000个框</p>\n<p><a href=\"http://arxiv.org/abs/1406.4729\" target=\"_blank\" rel=\"noopener\">SPP-Net</a>，2015（ROI Pooling），一次卷积，不限制输入尺寸</p>\n<p><a href=\"http://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a>，2015（Selective Search + CNN + ROI），结合R-CNN和SPP-Net的思路。将分类和回归都加入到网络中进行训练，做成了端到端。</p>\n<p><a href=\"http://arxiv.org/abs/1506.01497\" target=\"_blank\" rel=\"noopener\">Faster R-CNN</a>，2015（RPN + CNN + ROI），用RPN来求候选框，同时引入anchor机制来应对目标形变问题。</p>\n<h4 id=\"方案2-深度学习回归（one-stage）\"><a href=\"#方案2-深度学习回归（one-stage）\" class=\"headerlink\" title=\"方案2. 深度学习回归（one stage）\"></a>方案2. 深度学习回归（one stage）</h4><p><a href=\"http://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"noopener\">YOLO v1</a>，2016（回归任务）</p>\n<p><a href=\"https://arxiv.org/abs/1612.08242\" target=\"_blank\" rel=\"noopener\">YOLO v2</a>，2017，引入Faster R-CNN的anchor机制</p>\n<p><a href=\"http://arxiv.org/abs/1512.02325\" target=\"_blank\" rel=\"noopener\">SSD</a>，2016，结合YOLO的回归和Faster R-CNN的anchor</p>\n<hr>\n<h3 id=\"细节讨论\"><a href=\"#细节讨论\" class=\"headerlink\" title=\"细节讨论\"></a>细节讨论</h3><h4 id=\"1-R-CNN\"><a href=\"#1-R-CNN\" class=\"headerlink\" title=\"1.R-CNN\"></a>1.R-CNN</h4><ol>\n<li>候选区域：利用颜色、纹理、边缘信息，使用selective search提取一定（2000）的框（在保持较高的recall情况下）。对于每个框的区域，需要修正区域大小，以适合于CNN的输入，并提取特征。</li>\n<li>SVM分类：训练一个二分类SVM，来判断当前候选框里物体的类别。</li>\n<li>回归：使用回归来修正候选框的位置。</li>\n<li><strong>问题1</strong>：对2000个框都要进行CNN，框与框有重叠，时间复杂度高，在重叠区域的计算是不必要的，因此可以提速。</li>\n<li><strong>问题2</strong>：框的尺寸不一，需要resize，但是resize就会出现失真的问题。肯定会影响精度。</li>\n</ol>\n<h4 id=\"2-SPP-Net\"><a href=\"#2-SPP-Net\" class=\"headerlink\" title=\"2.SPP-Net\"></a>2.SPP-Net</h4><p>针对R-CNN的两个问题，SPP-Net就被提出来了。</p>\n<ol>\n<li>ROI Pooling：在普通的CNN结构中加入了ROI Pooling，其中的pooling filter可根据输入调整大小，使得输出是一个固定维度的向量，输入可以是任意的尺寸。然后给到全连接FC层。</li>\n<li>对原图只做一次卷积，首先得到整张图的feature map，再找到每个候选框在feature map上所对应的区域，将这个区域的特征输入到ROI Pooling，完成特征提取的工作。</li>\n</ol>\n<h4 id=\"3-Fast-R-CNN\"><a href=\"#3-Fast-R-CNN\" class=\"headerlink\" title=\"3.Fast R-CNN\"></a>3.Fast R-CNN</h4><p>在R-CNN的基础上结合了SPP-Net的思路，相比原来R-CNN，使用一次卷积；在最后一次卷积后添加了ROI Pooling；将边框回归直接加入到了CNN网络中，因此做成了端到端网络。</p>\n<p>R-CNN分为三个阶段，Fast R-CNN使用softmax来代替SVM，边框回归也加入到网络中，因此成为了端到端的网络。<strong>这可以为后面改进阶段性的算法作为一个重要的提示</strong></p>\n<p>R-CNN的方法：2000个候选框,resize -&gt; CNN提取特征 -&gt; 分类+回归</p>\n<p>Fast R-CNN方法:原始图片 -&gt; CNN -&gt; ROI Pooling -&gt; 分类+回归</p>\n<p>相比R-CNN训练速度提升了8.8x， 测试速度提升了146x</p>\n<p><strong>问题</strong> Selective Search寻找候选框非常耗时，</p>\n<h4 id=\"4-Faster-R-CNN\"><a href=\"#4-Faster-R-CNN\" class=\"headerlink\" title=\"4.Faster R-CNN\"></a>4.Faster R-CNN</h4><p>引入RPN网络代替selective search，使用RPN来搜索候选框，同时引入anchor box应对目标形状的变化问题。</p>\n<ol>\n<li>RPN：将RPN放在最后一个卷积层后面，所以RPN是作用在feature map上的，RPN包含分类和边框回归两个任务，分类器判断框属于的类别，边框回归进一步的优化/精细候选框。</li>\n</ol>\n<hr>\n<h4 id=\"5-YOLO-v1\"><a href=\"#5-YOLO-v1\" class=\"headerlink\" title=\"5.YOLO v1\"></a>5.YOLO v1</h4><p><em>two stage</em>的方案不能满足实时性要求，所以one stage的出来了。</p>\n<ol>\n<li>首先将图像划分为7x7的网格</li>\n<li>对于每个网格，都预测2个边框（包括边框是目标的置信度和类别概率），最终可以得到7x7x2个边框，再经过NMS，即可得到最终的回归框。</li>\n<li><strong>问题1</strong>：只是用7x7的网格，使得回归的精度不够高。而且当每个格子中包含多个物体（&gt;2）时，不能被检测出来。</li>\n<li><strong>问题2</strong>：输入图像输入resize到224*224</li>\n</ol>\n<h4 id=\"6-YOLO-v2\"><a href=\"#6-YOLO-v2\" class=\"headerlink\" title=\"6.YOLO v2\"></a>6.YOLO v2</h4><p>引入了Faster R-CNN的anchor box机制，并使用卷积层代替YOLO v1的全连接层，所以对输入图像不需要resize了。</p>\n<ol>\n<li>改进anchor box：Faster R-CNN中需要首选anchor box，而v2中采用k-means在许多框中进行聚类，以产生合适的框，同时使用过IOU作为k-means的距离计算。</li>\n<li>引入skip layer：借鉴ResNet的思想，使得网络更深。</li>\n</ol>\n<h4 id=\"7-SSD\"><a href=\"#7-SSD\" class=\"headerlink\" title=\"7.SSD\"></a>7.SSD</h4><p>SSD结合了YOLO回归的思想和Faster R-CNN的anchor box机制，YOLO中预测某个位置使用的全图特征，SSD预测某个位置使用的这个位置周围的特征。</p>\n<p>同时SSD的anchor是在多个feature map上进行的，这样就可以做的多尺度。</p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>总结R-CNN、SPP-NET、Fast R-CNN、Faster R-CNN、YOLO(v1 v2)、SSD <a href=\"https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fast-r-cnn\" target=\"_blank\" rel=\"noopener\">参考</a><br>","more":"</p>\n<h3 id=\"提前的总结\"><a href=\"#提前的总结\" class=\"headerlink\" title=\"提前的总结\"></a>提前的总结</h3><h4 id=\"方案1-候选区域-深度学习回归（two-stage）\"><a href=\"#方案1-候选区域-深度学习回归（two-stage）\" class=\"headerlink\" title=\"方案1.候选区域 + 深度学习回归（two stage）\"></a>方案1.候选区域 + 深度学习回归（two stage）</h4><p><a href=\"http://arxiv.org/abs/1311.2524\" target=\"_blank\" rel=\"noopener\">R-CNN</a>，2014（Selective Search + CNN + SVM），首先预选2000个框</p>\n<p><a href=\"http://arxiv.org/abs/1406.4729\" target=\"_blank\" rel=\"noopener\">SPP-Net</a>，2015（ROI Pooling），一次卷积，不限制输入尺寸</p>\n<p><a href=\"http://arxiv.org/abs/1504.08083\" target=\"_blank\" rel=\"noopener\">Fast R-CNN</a>，2015（Selective Search + CNN + ROI），结合R-CNN和SPP-Net的思路。将分类和回归都加入到网络中进行训练，做成了端到端。</p>\n<p><a href=\"http://arxiv.org/abs/1506.01497\" target=\"_blank\" rel=\"noopener\">Faster R-CNN</a>，2015（RPN + CNN + ROI），用RPN来求候选框，同时引入anchor机制来应对目标形变问题。</p>\n<h4 id=\"方案2-深度学习回归（one-stage）\"><a href=\"#方案2-深度学习回归（one-stage）\" class=\"headerlink\" title=\"方案2. 深度学习回归（one stage）\"></a>方案2. 深度学习回归（one stage）</h4><p><a href=\"http://arxiv.org/abs/1506.02640\" target=\"_blank\" rel=\"noopener\">YOLO v1</a>，2016（回归任务）</p>\n<p><a href=\"https://arxiv.org/abs/1612.08242\" target=\"_blank\" rel=\"noopener\">YOLO v2</a>，2017，引入Faster R-CNN的anchor机制</p>\n<p><a href=\"http://arxiv.org/abs/1512.02325\" target=\"_blank\" rel=\"noopener\">SSD</a>，2016，结合YOLO的回归和Faster R-CNN的anchor</p>\n<hr>\n<h3 id=\"细节讨论\"><a href=\"#细节讨论\" class=\"headerlink\" title=\"细节讨论\"></a>细节讨论</h3><h4 id=\"1-R-CNN\"><a href=\"#1-R-CNN\" class=\"headerlink\" title=\"1.R-CNN\"></a>1.R-CNN</h4><ol>\n<li>候选区域：利用颜色、纹理、边缘信息，使用selective search提取一定（2000）的框（在保持较高的recall情况下）。对于每个框的区域，需要修正区域大小，以适合于CNN的输入，并提取特征。</li>\n<li>SVM分类：训练一个二分类SVM，来判断当前候选框里物体的类别。</li>\n<li>回归：使用回归来修正候选框的位置。</li>\n<li><strong>问题1</strong>：对2000个框都要进行CNN，框与框有重叠，时间复杂度高，在重叠区域的计算是不必要的，因此可以提速。</li>\n<li><strong>问题2</strong>：框的尺寸不一，需要resize，但是resize就会出现失真的问题。肯定会影响精度。</li>\n</ol>\n<h4 id=\"2-SPP-Net\"><a href=\"#2-SPP-Net\" class=\"headerlink\" title=\"2.SPP-Net\"></a>2.SPP-Net</h4><p>针对R-CNN的两个问题，SPP-Net就被提出来了。</p>\n<ol>\n<li>ROI Pooling：在普通的CNN结构中加入了ROI Pooling，其中的pooling filter可根据输入调整大小，使得输出是一个固定维度的向量，输入可以是任意的尺寸。然后给到全连接FC层。</li>\n<li>对原图只做一次卷积，首先得到整张图的feature map，再找到每个候选框在feature map上所对应的区域，将这个区域的特征输入到ROI Pooling，完成特征提取的工作。</li>\n</ol>\n<h4 id=\"3-Fast-R-CNN\"><a href=\"#3-Fast-R-CNN\" class=\"headerlink\" title=\"3.Fast R-CNN\"></a>3.Fast R-CNN</h4><p>在R-CNN的基础上结合了SPP-Net的思路，相比原来R-CNN，使用一次卷积；在最后一次卷积后添加了ROI Pooling；将边框回归直接加入到了CNN网络中，因此做成了端到端网络。</p>\n<p>R-CNN分为三个阶段，Fast R-CNN使用softmax来代替SVM，边框回归也加入到网络中，因此成为了端到端的网络。<strong>这可以为后面改进阶段性的算法作为一个重要的提示</strong></p>\n<p>R-CNN的方法：2000个候选框,resize -&gt; CNN提取特征 -&gt; 分类+回归</p>\n<p>Fast R-CNN方法:原始图片 -&gt; CNN -&gt; ROI Pooling -&gt; 分类+回归</p>\n<p>相比R-CNN训练速度提升了8.8x， 测试速度提升了146x</p>\n<p><strong>问题</strong> Selective Search寻找候选框非常耗时，</p>\n<h4 id=\"4-Faster-R-CNN\"><a href=\"#4-Faster-R-CNN\" class=\"headerlink\" title=\"4.Faster R-CNN\"></a>4.Faster R-CNN</h4><p>引入RPN网络代替selective search，使用RPN来搜索候选框，同时引入anchor box应对目标形状的变化问题。</p>\n<ol>\n<li>RPN：将RPN放在最后一个卷积层后面，所以RPN是作用在feature map上的，RPN包含分类和边框回归两个任务，分类器判断框属于的类别，边框回归进一步的优化/精细候选框。</li>\n</ol>\n<hr>\n<h4 id=\"5-YOLO-v1\"><a href=\"#5-YOLO-v1\" class=\"headerlink\" title=\"5.YOLO v1\"></a>5.YOLO v1</h4><p><em>two stage</em>的方案不能满足实时性要求，所以one stage的出来了。</p>\n<ol>\n<li>首先将图像划分为7x7的网格</li>\n<li>对于每个网格，都预测2个边框（包括边框是目标的置信度和类别概率），最终可以得到7x7x2个边框，再经过NMS，即可得到最终的回归框。</li>\n<li><strong>问题1</strong>：只是用7x7的网格，使得回归的精度不够高。而且当每个格子中包含多个物体（&gt;2）时，不能被检测出来。</li>\n<li><strong>问题2</strong>：输入图像输入resize到224*224</li>\n</ol>\n<h4 id=\"6-YOLO-v2\"><a href=\"#6-YOLO-v2\" class=\"headerlink\" title=\"6.YOLO v2\"></a>6.YOLO v2</h4><p>引入了Faster R-CNN的anchor box机制，并使用卷积层代替YOLO v1的全连接层，所以对输入图像不需要resize了。</p>\n<ol>\n<li>改进anchor box：Faster R-CNN中需要首选anchor box，而v2中采用k-means在许多框中进行聚类，以产生合适的框，同时使用过IOU作为k-means的距离计算。</li>\n<li>引入skip layer：借鉴ResNet的思想，使得网络更深。</li>\n</ol>\n<h4 id=\"7-SSD\"><a href=\"#7-SSD\" class=\"headerlink\" title=\"7.SSD\"></a>7.SSD</h4><p>SSD结合了YOLO回归的思想和Faster R-CNN的anchor box机制，YOLO中预测某个位置使用的全图特征，SSD预测某个位置使用的这个位置周围的特征。</p>\n<p>同时SSD的anchor是在多个feature map上进行的，这样就可以做的多尺度。</p>"},{"title":"粗糙表面计算机模拟(matlab+3dsMax仿真)","date":"2018-03-23T10:29:30.000Z","_content":"本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.\n采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。\n<!-- more -->\n# 代码 #\n  \n\tclear\n\tclc\n\t\n\tN=128;%生成大小\n\tdelta=0.05;%表面均方根粗糙度\n\tbetax=30;%x方向的相关长度\n\tbetay=30;%y方向的相关长度\n\tC=1;%功率谱密度\n\t\n\tL=0.05;\n\tdx=L/N;dy=dx;\n\tNN=-N/2:N/2-1;\n\t[Nx,Ny]=meshgrid(NN,NN);\n\ttaux=dx.*Nx;tauy=dy.*Ny;\n\t\n\t%%生成具有指定自相关函数的粗糙表面\n\teta=randn(N,N);%高斯分布白噪声\n\tA=fft2(eta);%傅里叶变换\n\tR=zeros(N,N);\n\tR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\n\tGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\n\tH=(Gz/C).^0.5;%传递函数\n\tZ=H.*A;%表面高度的傅里叶变换\n\tz=ifft2(Z);%表面高度分布\n\tz = abs(z) * 1800;\n\tfigure(1);\n\tmesh(z);\n\t% surf2stl('surf_roughness.stl',1,1,z) % 生成模型\n\ttitle('rough surface');\n\taxis square\n\t\n# 实验结果 #\nmatlab仿真结果128*128\n\n![matlab仿真结果128*128](https://mic-jasontang.github.io/imgs/surf_roughness_128.png)\n\nmatlab仿真结果256*256\n\n![matlab仿真结果256*256](https://mic-jasontang.github.io/imgs/surf_roughness_256.png)\n# 仿真结果 #\n采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。\n\n3dMax仿真结果\n\n![3dMax仿真结果](https://mic-jasontang.github.io/imgs/50mm-106-68.png)\n\n3dMax仿真之后进行试验的结果\n\n![3dMax仿真之后进行试验的结果](https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png)\n# 参考文献 #\n[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.\n","source":"_posts/粗糙表面计算机模拟.md","raw":"---\ntitle: 粗糙表面计算机模拟(matlab+3dsMax仿真)\ndate: 2018-03-23 18:29:30\ncategories:\n- 研磨&粗糙度\n- 研磨表面仿真\ntags: \n- 表面粗糙度\n- matlab模拟粗糙度\n- 3dsMax仿真\n---\n本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.\n采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。\n<!-- more -->\n# 代码 #\n  \n\tclear\n\tclc\n\t\n\tN=128;%生成大小\n\tdelta=0.05;%表面均方根粗糙度\n\tbetax=30;%x方向的相关长度\n\tbetay=30;%y方向的相关长度\n\tC=1;%功率谱密度\n\t\n\tL=0.05;\n\tdx=L/N;dy=dx;\n\tNN=-N/2:N/2-1;\n\t[Nx,Ny]=meshgrid(NN,NN);\n\ttaux=dx.*Nx;tauy=dy.*Ny;\n\t\n\t%%生成具有指定自相关函数的粗糙表面\n\teta=randn(N,N);%高斯分布白噪声\n\tA=fft2(eta);%傅里叶变换\n\tR=zeros(N,N);\n\tR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\n\tGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\n\tH=(Gz/C).^0.5;%传递函数\n\tZ=H.*A;%表面高度的傅里叶变换\n\tz=ifft2(Z);%表面高度分布\n\tz = abs(z) * 1800;\n\tfigure(1);\n\tmesh(z);\n\t% surf2stl('surf_roughness.stl',1,1,z) % 生成模型\n\ttitle('rough surface');\n\taxis square\n\t\n# 实验结果 #\nmatlab仿真结果128*128\n\n![matlab仿真结果128*128](https://mic-jasontang.github.io/imgs/surf_roughness_128.png)\n\nmatlab仿真结果256*256\n\n![matlab仿真结果256*256](https://mic-jasontang.github.io/imgs/surf_roughness_256.png)\n# 仿真结果 #\n采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。\n\n3dMax仿真结果\n\n![3dMax仿真结果](https://mic-jasontang.github.io/imgs/50mm-106-68.png)\n\n3dMax仿真之后进行试验的结果\n\n![3dMax仿真之后进行试验的结果](https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png)\n# 参考文献 #\n[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.\n","slug":"粗糙表面计算机模拟","published":1,"updated":"2019-05-08T10:50:25.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq8000olnvv3lbk6r4h","content":"<p>本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.<br>采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。<br><a id=\"more\"></a></p>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><pre><code>clear\nclc\n\nN=128;%生成大小\ndelta=0.05;%表面均方根粗糙度\nbetax=30;%x方向的相关长度\nbetay=30;%y方向的相关长度\nC=1;%功率谱密度\n\nL=0.05;\ndx=L/N;dy=dx;\nNN=-N/2:N/2-1;\n[Nx,Ny]=meshgrid(NN,NN);\ntaux=dx.*Nx;tauy=dy.*Ny;\n\n%%生成具有指定自相关函数的粗糙表面\neta=randn(N,N);%高斯分布白噪声\nA=fft2(eta);%傅里叶变换\nR=zeros(N,N);\nR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\nGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\nH=(Gz/C).^0.5;%传递函数\nZ=H.*A;%表面高度的傅里叶变换\nz=ifft2(Z);%表面高度分布\nz = abs(z) * 1800;\nfigure(1);\nmesh(z);\n% surf2stl(&apos;surf_roughness.stl&apos;,1,1,z) % 生成模型\ntitle(&apos;rough surface&apos;);\naxis square\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>matlab仿真结果128*128</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_128.png\" alt=\"matlab仿真结果128*128\"></p>\n<p>matlab仿真结果256*256</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_256.png\" alt=\"matlab仿真结果256*256\"></p>\n<h1 id=\"仿真结果\"><a href=\"#仿真结果\" class=\"headerlink\" title=\"仿真结果\"></a>仿真结果</h1><p>采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。</p>\n<p>3dMax仿真结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68.png\" alt=\"3dMax仿真结果\"></p>\n<p>3dMax仿真之后进行试验的结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png\" alt=\"3dMax仿真之后进行试验的结果\"></p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.</p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>本文主要使用matlab对零件表面的粗糙度进行模拟[1]，然后生成模型，导入到3dsMax中进行仿真.<br>采用matlab实现具有高斯分布粗糙表面的模拟，参考胡元中的论文。<br>","more":"</p>\n<h1 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h1><pre><code>clear\nclc\n\nN=128;%生成大小\ndelta=0.05;%表面均方根粗糙度\nbetax=30;%x方向的相关长度\nbetay=30;%y方向的相关长度\nC=1;%功率谱密度\n\nL=0.05;\ndx=L/N;dy=dx;\nNN=-N/2:N/2-1;\n[Nx,Ny]=meshgrid(NN,NN);\ntaux=dx.*Nx;tauy=dy.*Ny;\n\n%%生成具有指定自相关函数的粗糙表面\neta=randn(N,N);%高斯分布白噪声\nA=fft2(eta);%傅里叶变换\nR=zeros(N,N);\nR=delta^2*exp(-2.3*((taux/betax).^2+(tauy/betay).^2).^0.5);%自相关函数\nGz=1/(2*pi^2).*fft2(R);%功率谱密度函数\nH=(Gz/C).^0.5;%传递函数\nZ=H.*A;%表面高度的傅里叶变换\nz=ifft2(Z);%表面高度分布\nz = abs(z) * 1800;\nfigure(1);\nmesh(z);\n% surf2stl(&apos;surf_roughness.stl&apos;,1,1,z) % 生成模型\ntitle(&apos;rough surface&apos;);\naxis square\n</code></pre><h1 id=\"实验结果\"><a href=\"#实验结果\" class=\"headerlink\" title=\"实验结果\"></a>实验结果</h1><p>matlab仿真结果128*128</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_128.png\" alt=\"matlab仿真结果128*128\"></p>\n<p>matlab仿真结果256*256</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/surf_roughness_256.png\" alt=\"matlab仿真结果256*256\"></p>\n<h1 id=\"仿真结果\"><a href=\"#仿真结果\" class=\"headerlink\" title=\"仿真结果\"></a>仿真结果</h1><p>采用目标聚光灯和目标相机，相机采用50mm焦距拍摄，金属材质，高光级别106，光泽度68，模拟效果还算理想。</p>\n<p>3dMax仿真结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68.png\" alt=\"3dMax仿真结果\"></p>\n<p>3dMax仿真之后进行试验的结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/50mm-106-68-ans.png\" alt=\"3dMax仿真之后进行试验的结果\"></p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p>[1]陈辉,胡元中,王慧,王文中.粗糙表面计算机模拟[J].润滑与密封,2006(10):52-55+59.</p>"},{"title":"FCN 论文总结","date":"2019-05-04T03:00:46.000Z","_content":"FCN，于2015年被Jonathan提出，是应用在Semantic segmentation中，下面总结与CNN有何不同，以及它的优缺点。\n<!-- more -->\n\n2015年Jonathan发表了[《Fully Convolutional Networks for Semantic Segmentation》](https://arxiv.org/pdf/1411.4038.pdf)将FCN应用在了语义分割中。\n\n##### 1. CNN与FCN有何不同？\n##### 1.1 网络结构上的不同\nCNN通常在卷积层之后会接上若干个全连接层，将卷积层产生的特征映射为固定长度的特征向量。如经典的AlexNet，最后输出1000维的特征向量，再用Softmax做分类。CNN的输入尺寸一般是固定的。\n\nFCN，所有层都是卷积层，可以接受任意尺寸的输入图像，采用反卷积对FCN中最后一个卷积层的特征图**上采样**，使它恢复到输入图像相同的尺寸，最后逐个像素计算softmax分类的损失，相当于每一个像素对应一个训练样本，如图1所示。\n![FCN](https://images2015.cnblogs.com/blog/829125/201701/829125-20170104183245769-878631707.png)\n\n##### 1.2 在分割任务上的不同\n基于CNN的分割方法，为了对一个像素进行分类，需要使用该像素周围的像素区域作为CNN的输入。这样做有几个缺点：a.存储开销大，存储空间依赖卷积核的大小和卷积的次数。b.计算效率低，因为卷积会有很多重复计算。c.CNN提取局部特征，受限于卷积核的大小限制。\n\n而在FCN中，是进行的像素级任务，可以对每个像素进行分类。（**我觉得主要解决的还是上面的缺点c**）\n\n##### 2. 全连接层与卷积层\n全连接层与卷积层是可以相互转换的。利用1*1卷积即可。\n\n#### 3. 如何保持输出与输入尺寸相同？\n保留每个池化层的feature map,将最后一个池化层之后的结果（heatMap）进行上采样（双线性插值），然后依次使用上层的卷积核对上采样结果进行反卷积，最终就得到了与输入尺寸相同的图像。\n\n#### 4.缺点\n缺点显而易见，进行的是像素级任务，那自然就抛弃了像素之间的关系——空间一致性。\n\n上采样的结果比较模糊，不能注意到图像的细节。\n\n","source":"_posts/第 6 周学习分享.md","raw":"---\ntitle: FCN 论文总结\ndate: 2019-05-04 11:00:46\ncategories:\n- 周总结\ntags: \n- FCN\n- deep learning\n---\nFCN，于2015年被Jonathan提出，是应用在Semantic segmentation中，下面总结与CNN有何不同，以及它的优缺点。\n<!-- more -->\n\n2015年Jonathan发表了[《Fully Convolutional Networks for Semantic Segmentation》](https://arxiv.org/pdf/1411.4038.pdf)将FCN应用在了语义分割中。\n\n##### 1. CNN与FCN有何不同？\n##### 1.1 网络结构上的不同\nCNN通常在卷积层之后会接上若干个全连接层，将卷积层产生的特征映射为固定长度的特征向量。如经典的AlexNet，最后输出1000维的特征向量，再用Softmax做分类。CNN的输入尺寸一般是固定的。\n\nFCN，所有层都是卷积层，可以接受任意尺寸的输入图像，采用反卷积对FCN中最后一个卷积层的特征图**上采样**，使它恢复到输入图像相同的尺寸，最后逐个像素计算softmax分类的损失，相当于每一个像素对应一个训练样本，如图1所示。\n![FCN](https://images2015.cnblogs.com/blog/829125/201701/829125-20170104183245769-878631707.png)\n\n##### 1.2 在分割任务上的不同\n基于CNN的分割方法，为了对一个像素进行分类，需要使用该像素周围的像素区域作为CNN的输入。这样做有几个缺点：a.存储开销大，存储空间依赖卷积核的大小和卷积的次数。b.计算效率低，因为卷积会有很多重复计算。c.CNN提取局部特征，受限于卷积核的大小限制。\n\n而在FCN中，是进行的像素级任务，可以对每个像素进行分类。（**我觉得主要解决的还是上面的缺点c**）\n\n##### 2. 全连接层与卷积层\n全连接层与卷积层是可以相互转换的。利用1*1卷积即可。\n\n#### 3. 如何保持输出与输入尺寸相同？\n保留每个池化层的feature map,将最后一个池化层之后的结果（heatMap）进行上采样（双线性插值），然后依次使用上层的卷积核对上采样结果进行反卷积，最终就得到了与输入尺寸相同的图像。\n\n#### 4.缺点\n缺点显而易见，进行的是像素级任务，那自然就抛弃了像素之间的关系——空间一致性。\n\n上采样的结果比较模糊，不能注意到图像的细节。\n\n","slug":"第 6 周学习分享","published":1,"updated":"2019-05-08T10:50:25.486Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqq8000plnvvg04xjvzz","content":"<p>FCN，于2015年被Jonathan提出，是应用在Semantic segmentation中，下面总结与CNN有何不同，以及它的优缺点。<br><a id=\"more\"></a></p>\n<p>2015年Jonathan发表了<a href=\"https://arxiv.org/pdf/1411.4038.pdf\" target=\"_blank\" rel=\"noopener\">《Fully Convolutional Networks for Semantic Segmentation》</a>将FCN应用在了语义分割中。</p>\n<h5 id=\"1-CNN与FCN有何不同？\"><a href=\"#1-CNN与FCN有何不同？\" class=\"headerlink\" title=\"1. CNN与FCN有何不同？\"></a>1. CNN与FCN有何不同？</h5><h5 id=\"1-1-网络结构上的不同\"><a href=\"#1-1-网络结构上的不同\" class=\"headerlink\" title=\"1.1 网络结构上的不同\"></a>1.1 网络结构上的不同</h5><p>CNN通常在卷积层之后会接上若干个全连接层，将卷积层产生的特征映射为固定长度的特征向量。如经典的AlexNet，最后输出1000维的特征向量，再用Softmax做分类。CNN的输入尺寸一般是固定的。</p>\n<p>FCN，所有层都是卷积层，可以接受任意尺寸的输入图像，采用反卷积对FCN中最后一个卷积层的特征图<strong>上采样</strong>，使它恢复到输入图像相同的尺寸，最后逐个像素计算softmax分类的损失，相当于每一个像素对应一个训练样本，如图1所示。<br><img src=\"https://images2015.cnblogs.com/blog/829125/201701/829125-20170104183245769-878631707.png\" alt=\"FCN\"></p>\n<h5 id=\"1-2-在分割任务上的不同\"><a href=\"#1-2-在分割任务上的不同\" class=\"headerlink\" title=\"1.2 在分割任务上的不同\"></a>1.2 在分割任务上的不同</h5><p>基于CNN的分割方法，为了对一个像素进行分类，需要使用该像素周围的像素区域作为CNN的输入。这样做有几个缺点：a.存储开销大，存储空间依赖卷积核的大小和卷积的次数。b.计算效率低，因为卷积会有很多重复计算。c.CNN提取局部特征，受限于卷积核的大小限制。</p>\n<p>而在FCN中，是进行的像素级任务，可以对每个像素进行分类。（<strong>我觉得主要解决的还是上面的缺点c</strong>）</p>\n<h5 id=\"2-全连接层与卷积层\"><a href=\"#2-全连接层与卷积层\" class=\"headerlink\" title=\"2. 全连接层与卷积层\"></a>2. 全连接层与卷积层</h5><p>全连接层与卷积层是可以相互转换的。利用1*1卷积即可。</p>\n<h4 id=\"3-如何保持输出与输入尺寸相同？\"><a href=\"#3-如何保持输出与输入尺寸相同？\" class=\"headerlink\" title=\"3. 如何保持输出与输入尺寸相同？\"></a>3. 如何保持输出与输入尺寸相同？</h4><p>保留每个池化层的feature map,将最后一个池化层之后的结果（heatMap）进行上采样（双线性插值），然后依次使用上层的卷积核对上采样结果进行反卷积，最终就得到了与输入尺寸相同的图像。</p>\n<h4 id=\"4-缺点\"><a href=\"#4-缺点\" class=\"headerlink\" title=\"4.缺点\"></a>4.缺点</h4><p>缺点显而易见，进行的是像素级任务，那自然就抛弃了像素之间的关系——空间一致性。</p>\n<p>上采样的结果比较模糊，不能注意到图像的细节。</p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>FCN，于2015年被Jonathan提出，是应用在Semantic segmentation中，下面总结与CNN有何不同，以及它的优缺点。<br>","more":"</p>\n<p>2015年Jonathan发表了<a href=\"https://arxiv.org/pdf/1411.4038.pdf\" target=\"_blank\" rel=\"noopener\">《Fully Convolutional Networks for Semantic Segmentation》</a>将FCN应用在了语义分割中。</p>\n<h5 id=\"1-CNN与FCN有何不同？\"><a href=\"#1-CNN与FCN有何不同？\" class=\"headerlink\" title=\"1. CNN与FCN有何不同？\"></a>1. CNN与FCN有何不同？</h5><h5 id=\"1-1-网络结构上的不同\"><a href=\"#1-1-网络结构上的不同\" class=\"headerlink\" title=\"1.1 网络结构上的不同\"></a>1.1 网络结构上的不同</h5><p>CNN通常在卷积层之后会接上若干个全连接层，将卷积层产生的特征映射为固定长度的特征向量。如经典的AlexNet，最后输出1000维的特征向量，再用Softmax做分类。CNN的输入尺寸一般是固定的。</p>\n<p>FCN，所有层都是卷积层，可以接受任意尺寸的输入图像，采用反卷积对FCN中最后一个卷积层的特征图<strong>上采样</strong>，使它恢复到输入图像相同的尺寸，最后逐个像素计算softmax分类的损失，相当于每一个像素对应一个训练样本，如图1所示。<br><img src=\"https://images2015.cnblogs.com/blog/829125/201701/829125-20170104183245769-878631707.png\" alt=\"FCN\"></p>\n<h5 id=\"1-2-在分割任务上的不同\"><a href=\"#1-2-在分割任务上的不同\" class=\"headerlink\" title=\"1.2 在分割任务上的不同\"></a>1.2 在分割任务上的不同</h5><p>基于CNN的分割方法，为了对一个像素进行分类，需要使用该像素周围的像素区域作为CNN的输入。这样做有几个缺点：a.存储开销大，存储空间依赖卷积核的大小和卷积的次数。b.计算效率低，因为卷积会有很多重复计算。c.CNN提取局部特征，受限于卷积核的大小限制。</p>\n<p>而在FCN中，是进行的像素级任务，可以对每个像素进行分类。（<strong>我觉得主要解决的还是上面的缺点c</strong>）</p>\n<h5 id=\"2-全连接层与卷积层\"><a href=\"#2-全连接层与卷积层\" class=\"headerlink\" title=\"2. 全连接层与卷积层\"></a>2. 全连接层与卷积层</h5><p>全连接层与卷积层是可以相互转换的。利用1*1卷积即可。</p>\n<h4 id=\"3-如何保持输出与输入尺寸相同？\"><a href=\"#3-如何保持输出与输入尺寸相同？\" class=\"headerlink\" title=\"3. 如何保持输出与输入尺寸相同？\"></a>3. 如何保持输出与输入尺寸相同？</h4><p>保留每个池化层的feature map,将最后一个池化层之后的结果（heatMap）进行上采样（双线性插值），然后依次使用上层的卷积核对上采样结果进行反卷积，最终就得到了与输入尺寸相同的图像。</p>\n<h4 id=\"4-缺点\"><a href=\"#4-缺点\" class=\"headerlink\" title=\"4.缺点\"></a>4.缺点</h4><p>缺点显而易见，进行的是像素级任务，那自然就抛弃了像素之间的关系——空间一致性。</p>\n<p>上采样的结果比较模糊，不能注意到图像的细节。</p>"},{"title":"Mnist手写数字体识别(tensorflow)","date":"2018-03-20T13:35:05.000Z","_content":"# Tensorflow #\n\n\n> 首先，简单的说下，tensorflow的基本架构。\n>使用 TensorFlow, 你必须明白 TensorFlow:\n\n- 使用图 (graph) 来表示计算任务.\n- 在被称之为 会话 (Session) 的上下文 (context) 中执行图.\n- 使用 tensor 表示数据.\n- 通过 变量 (Variable) 维护状态.\n- 使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.\n\n<!--more -->\n\n# Tensor #\n\n> TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].\n\n> 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.\n\n> Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。\n\n# Graph #\n\n\n> TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.\n\n\n\n> 例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。\n\n![](https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif)\n> 构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.\n\n\n> TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.\n\n# Session #\n> 当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。\n\n\n\n> tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.\n\n\n# 数据载体 #\n> Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：\n   \n\t# 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\n\tstate = tf.Variable(0, name=\"counter\")\n\n\t# 创建一个常量，其值为1，没有指定数据类型（dtype）\n\tone = tf.constant(1)\n\n\n\n> 针对上面的变量和常量，看看Tensorflow里面的函数定义：\n>\n    class Variable(object):　\n\tdef __init__(self,\n\t\tinitial_value=None,\n\t\ttrainable=True,\n\t\tcollections=None,\n\t\tvalidate_shape=True,\n\t\tcaching_device=None,\n\t\tname=None,\n\t\tvariable_def=None,\n\t\tdtype=None,\n\t\texpected_shape=None,\n\t\timport_scope=None)：\n\n>\n\tdef constant(value, dtype=None, shape=None, name=\"Const\", verify_shape=False)：\n\n> 从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。\n\n> 还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。\n>\n\tinput1 = tf.placeholder(tf.types.float32)\n\tinput2 = tf.placeholder(tf.types.float32)\n\toutput = tf.mul(input1, input2)\n>\t\n\twith tf.Session() as sess:\n\t  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n>\t\n\t# 输出:\n\t# [array([ 14.], dtype=float32)]\n\n> 占位符的定义原型，也是一个函数：\n>\n\tdef placeholder(dtype, shape=None, name=None)：\n\n\n\n> 到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：\n\n\n----------\n\n> main.py驱动程序\n \n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 20:41\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : main.py\n\t# @ToDo    : 驱动程序\n\t\n\timport _thread\n\t\n\tfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\t\n\t\n\tif __name__ == '__main__':\n\t    _thread.start_new_thread(mnist_train.main, (None,))\n\t    _thread.start_new_thread(mnist_eval.main, (None,))\n\t\n\t    # 这个不能删除，当做主线程\n\t    while 1:\n\t        pass\n\n> mnist_inference.py计算前向传播的过程及定义了神经网络的参数\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/20 19:43\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_inference.py\n\t# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\t\n\t\n\timport tensorflow as tf\n\t\n\t# 定义神经网络结构相关的参数\n\tINPUT_NODE = 784\n\tOUTPUT_NODE = 10\n\tLAYER1_NODE = 500\n\t\n\t\n\t# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\n\tdef get_weight_variable(shape, regularizer):\n\t    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t\n\t    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t    # 自定义集合\n\t    if regularizer:\n\t        tf.add_to_collection(\"losses\", regularizer(weights))\n\t    return weights\n\t\n\t\n\t# 前向传播过程\n\tdef inference(input_tensor, regularizer):\n\t    # 声明第一层神经网络的变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer1\"):\n\t        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\t\n\t    # 声明第二层圣经网络变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer2\"):\n\t        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer2 = tf.matmul(layer1, weights) + biases\n\t    # 返回最后前向传播的结果\n\t    return layer2\n\n> mnist_train.py定义了神经网络的训练过程\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:08\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_train.py\n\t# @ToDo    : 定义了神经网络的训练过程\n\t\n\timport os\n\t\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\t\n\t# 配置神经网络的参数\n\tBATCH_SIZE = 100\n\tLEARNING_REATE_BASE = 0.8\n\tLEARNING_RATE_DECAY = 0.99\n\tREGULARAZTION_RATE = 0.0001\n\tTRAING_STEPS = 2000\n\tMOVING_AVERAGE_DECAY = 0.99\n\t# 模型保存的路径和文件名\n\tMODEL_SAVE_PATH = \"./model/\"\n\tMODEL_NAME = \"model.ckpt\"\n\t\n\t\n\tdef train(mnist):\n\t    # 定义输入输出placeholder\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n\t    y = mnist_inference.inference(x, regularizer)\n\t    global_step = tf.Variable(0, trainable=False)\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n\t    variables_average_op = variable_averages.apply(tf.trainable_variables())\n\t    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n\t    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\t    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n\t    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n\t                                              global_step,\n\t                                              mnist.train.num_examples / BATCH_SIZE,\n\t                                              LEARNING_RATE_DECAY)\n\t    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\t\n\t    with tf.control_dependencies([train_step, variables_average_op]):\n\t        train_op = tf.no_op(name=\"train\")\n\t\n\t    # 初始化持久化类\n\t    saver = tf.train.Saver()\n\t    with tf.Session() as sess:\n\t        tf.global_variables_initializer().run()\n\t\n\t        for i in range(TRAING_STEPS):\n\t            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n\t            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\t\n\t            if i % 1000 == 0:\n\t                print(\"After %d training step(s), loss on training batch is %g.\" % (i, loss_value))\n\t\n\t                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    train(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n> mnist_eval.py测试过程\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:32\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_eval.py\n\t# @ToDo    : 测试过程\n\t\n\t\n\timport time\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\timport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\t\n\t# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\n\tEVAL_INTERVAL_SECS = 10\n\t\n\t\n\tdef evaluate(mnist):\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    validate_feed = {x: mnist.validation.images,\n\t                     y_: mnist.validation.labels}\n\t\n\t    y = mnist_inference.inference(x, None)\n\t\n\t    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n\t    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n\t    variables_to_restore = variable_averages.variables_to_restore()\n\t    saver = tf.train.Saver(variables_to_restore)\n\t\n\t    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n\t    stop_count = 0\n\t    while True:\n\t        with tf.Session() as sess:\n\t            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n\t            # 停止条件 #\n\t            stop_count += EVAL_INTERVAL_SECS\n\t            if stop_count == mnist_train.TRAING_STEPS:\n\t                return\n\t            # 停止条件 #\n\t            if ckpt and ckpt.model_checkpoint_path:\n\t                saver.restore(sess, ckpt.model_checkpoint_path)\n\t                # 通过文件名得到模型保存时迭代的轮数\n\t                # 输出./model/model.ckpt-29001\n\t                print(ckpt.model_checkpoint_path)\n\t                global_step = ckpt.model_checkpoint_path.split(\"/\")[-1].split(\"-\")[-1]\n\t                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n\t                print(\"After %s training step(s), validation accuracy is %g\" % (global_step, accuracy_score))\n\t            else:\n\t                print(\"No checkpoint file found\")\n\t                return\n\t        time.sleep(EVAL_INTERVAL_SECS)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    evaluate(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n# 参考文章 #\n[https://www.cnblogs.com/shihuc/p/6648130.html](https://www.cnblogs.com/shihuc/p/6648130.html \"Tensorflow之基于MNIST手写识别的入门介绍\")\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Mnist手写数字识别-tensorflow.md","raw":"---\ntitle: Mnist手写数字体识别(tensorflow)\ndate: 2018-03-20 21:35:05\ncategories:\n- tensorflow学习\n- tensorflow-Demo\ntags: \n- Mnist\n- tensorflow\n---\n# Tensorflow #\n\n\n> 首先，简单的说下，tensorflow的基本架构。\n>使用 TensorFlow, 你必须明白 TensorFlow:\n\n- 使用图 (graph) 来表示计算任务.\n- 在被称之为 会话 (Session) 的上下文 (context) 中执行图.\n- 使用 tensor 表示数据.\n- 通过 变量 (Variable) 维护状态.\n- 使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.\n\n<!--more -->\n\n# Tensor #\n\n> TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].\n\n> 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.\n\n> Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。\n\n# Graph #\n\n\n> TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.\n\n\n\n> 例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。\n\n![](https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif)\n> 构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.\n\n\n> TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.\n\n# Session #\n> 当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。\n\n\n\n> tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.\n\n\n# 数据载体 #\n> Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：\n   \n\t# 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\n\tstate = tf.Variable(0, name=\"counter\")\n\n\t# 创建一个常量，其值为1，没有指定数据类型（dtype）\n\tone = tf.constant(1)\n\n\n\n> 针对上面的变量和常量，看看Tensorflow里面的函数定义：\n>\n    class Variable(object):　\n\tdef __init__(self,\n\t\tinitial_value=None,\n\t\ttrainable=True,\n\t\tcollections=None,\n\t\tvalidate_shape=True,\n\t\tcaching_device=None,\n\t\tname=None,\n\t\tvariable_def=None,\n\t\tdtype=None,\n\t\texpected_shape=None,\n\t\timport_scope=None)：\n\n>\n\tdef constant(value, dtype=None, shape=None, name=\"Const\", verify_shape=False)：\n\n> 从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。\n\n> 还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。\n>\n\tinput1 = tf.placeholder(tf.types.float32)\n\tinput2 = tf.placeholder(tf.types.float32)\n\toutput = tf.mul(input1, input2)\n>\t\n\twith tf.Session() as sess:\n\t  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n>\t\n\t# 输出:\n\t# [array([ 14.], dtype=float32)]\n\n> 占位符的定义原型，也是一个函数：\n>\n\tdef placeholder(dtype, shape=None, name=None)：\n\n\n\n> 到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：\n\n\n----------\n\n> main.py驱动程序\n \n    #!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 20:41\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : main.py\n\t# @ToDo    : 驱动程序\n\t\n\timport _thread\n\t\n\tfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\t\n\t\n\tif __name__ == '__main__':\n\t    _thread.start_new_thread(mnist_train.main, (None,))\n\t    _thread.start_new_thread(mnist_eval.main, (None,))\n\t\n\t    # 这个不能删除，当做主线程\n\t    while 1:\n\t        pass\n\n> mnist_inference.py计算前向传播的过程及定义了神经网络的参数\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/20 19:43\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_inference.py\n\t# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\t\n\t\n\timport tensorflow as tf\n\t\n\t# 定义神经网络结构相关的参数\n\tINPUT_NODE = 784\n\tOUTPUT_NODE = 10\n\tLAYER1_NODE = 500\n\t\n\t\n\t# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\n\tdef get_weight_variable(shape, regularizer):\n\t    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t\n\t    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t    # 自定义集合\n\t    if regularizer:\n\t        tf.add_to_collection(\"losses\", regularizer(weights))\n\t    return weights\n\t\n\t\n\t# 前向传播过程\n\tdef inference(input_tensor, regularizer):\n\t    # 声明第一层神经网络的变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer1\"):\n\t        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\t\n\t    # 声明第二层圣经网络变量并完成前向传播过程\n\t    with tf.variable_scope(\"layer2\"):\n\t        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n\t        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n\t        layer2 = tf.matmul(layer1, weights) + biases\n\t    # 返回最后前向传播的结果\n\t    return layer2\n\n> mnist_train.py定义了神经网络的训练过程\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:08\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_train.py\n\t# @ToDo    : 定义了神经网络的训练过程\n\t\n\timport os\n\t\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\t\n\t# 配置神经网络的参数\n\tBATCH_SIZE = 100\n\tLEARNING_REATE_BASE = 0.8\n\tLEARNING_RATE_DECAY = 0.99\n\tREGULARAZTION_RATE = 0.0001\n\tTRAING_STEPS = 2000\n\tMOVING_AVERAGE_DECAY = 0.99\n\t# 模型保存的路径和文件名\n\tMODEL_SAVE_PATH = \"./model/\"\n\tMODEL_NAME = \"model.ckpt\"\n\t\n\t\n\tdef train(mnist):\n\t    # 定义输入输出placeholder\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n\t    y = mnist_inference.inference(x, regularizer)\n\t    global_step = tf.Variable(0, trainable=False)\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n\t    variables_average_op = variable_averages.apply(tf.trainable_variables())\n\t    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n\t    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\t    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n\t    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n\t                                              global_step,\n\t                                              mnist.train.num_examples / BATCH_SIZE,\n\t                                              LEARNING_RATE_DECAY)\n\t    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\t\n\t    with tf.control_dependencies([train_step, variables_average_op]):\n\t        train_op = tf.no_op(name=\"train\")\n\t\n\t    # 初始化持久化类\n\t    saver = tf.train.Saver()\n\t    with tf.Session() as sess:\n\t        tf.global_variables_initializer().run()\n\t\n\t        for i in range(TRAING_STEPS):\n\t            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n\t            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\t\n\t            if i % 1000 == 0:\n\t                print(\"After %d training step(s), loss on training batch is %g.\" % (i, loss_value))\n\t\n\t                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    train(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n> mnist_eval.py测试过程\n \n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:32\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_eval.py\n\t# @ToDo    : 测试过程\n\t\n\t\n\timport time\n\timport tensorflow as tf\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\timport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\t\n\t# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\n\tEVAL_INTERVAL_SECS = 10\n\t\n\t\n\tdef evaluate(mnist):\n\t    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    validate_feed = {x: mnist.validation.images,\n\t                     y_: mnist.validation.labels}\n\t\n\t    y = mnist_inference.inference(x, None)\n\t\n\t    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n\t    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n\t    variables_to_restore = variable_averages.variables_to_restore()\n\t    saver = tf.train.Saver(variables_to_restore)\n\t\n\t    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n\t    stop_count = 0\n\t    while True:\n\t        with tf.Session() as sess:\n\t            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n\t            # 停止条件 #\n\t            stop_count += EVAL_INTERVAL_SECS\n\t            if stop_count == mnist_train.TRAING_STEPS:\n\t                return\n\t            # 停止条件 #\n\t            if ckpt and ckpt.model_checkpoint_path:\n\t                saver.restore(sess, ckpt.model_checkpoint_path)\n\t                # 通过文件名得到模型保存时迭代的轮数\n\t                # 输出./model/model.ckpt-29001\n\t                print(ckpt.model_checkpoint_path)\n\t                global_step = ckpt.model_checkpoint_path.split(\"/\")[-1].split(\"-\")[-1]\n\t                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n\t                print(\"After %s training step(s), validation accuracy is %g\" % (global_step, accuracy_score))\n\t            else:\n\t                print(\"No checkpoint file found\")\n\t                return\n\t        time.sleep(EVAL_INTERVAL_SECS)\n\t\n\t\n\tdef main(argv=None):\n\t    mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n\t    evaluate(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\n# 参考文章 #\n[https://www.cnblogs.com/shihuc/p/6648130.html](https://www.cnblogs.com/shihuc/p/6648130.html \"Tensorflow之基于MNIST手写识别的入门介绍\")\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Mnist手写数字识别-tensorflow","published":1,"updated":"2019-05-08T10:50:25.481Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqr8003ylnvvonpwbtz5","content":"<h1 id=\"Tensorflow\"><a href=\"#Tensorflow\" class=\"headerlink\" title=\"Tensorflow\"></a>Tensorflow</h1><blockquote>\n<p>首先，简单的说下，tensorflow的基本架构。<br>使用 TensorFlow, 你必须明白 TensorFlow:</p>\n</blockquote>\n<ul>\n<li>使用图 (graph) 来表示计算任务.</li>\n<li>在被称之为 会话 (Session) 的上下文 (context) 中执行图.</li>\n<li>使用 tensor 表示数据.</li>\n<li>通过 变量 (Variable) 维护状态.</li>\n<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>\n</ul>\n<a id=\"more\"></a>\n<h1 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h1><blockquote>\n<p>TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].</p>\n</blockquote>\n<blockquote>\n<p>一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.</p>\n</blockquote>\n<blockquote>\n<p>Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。</p>\n</blockquote>\n<h1 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h1><blockquote>\n<p>TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.</p>\n</blockquote>\n<blockquote>\n<p>例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。</p>\n</blockquote>\n<p><img src=\"https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif\" alt></p>\n<blockquote>\n<p>构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.</p>\n</blockquote>\n<blockquote>\n<p>TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.</p>\n</blockquote>\n<h1 id=\"Session\"><a href=\"#Session\" class=\"headerlink\" title=\"Session\"></a>Session</h1><blockquote>\n<p>当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。</p>\n</blockquote>\n<blockquote>\n<p>tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.</p>\n</blockquote>\n<h1 id=\"数据载体\"><a href=\"#数据载体\" class=\"headerlink\" title=\"数据载体\"></a>数据载体</h1><blockquote>\n<p>Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：</p>\n</blockquote>\n<pre><code># 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\nstate = tf.Variable(0, name=&quot;counter&quot;)\n\n# 创建一个常量，其值为1，没有指定数据类型（dtype）\none = tf.constant(1)\n</code></pre><blockquote>\n<p>针对上面的变量和常量，看看Tensorflow里面的函数定义：</p>\n</blockquote>\n<pre><code>class Variable(object):　\ndef __init__(self,\n    initial_value=None,\n    trainable=True,\n    collections=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    expected_shape=None,\n    import_scope=None)：\n</code></pre><blockquote>\n</blockquote>\n<pre><code>def constant(value, dtype=None, shape=None, name=&quot;Const&quot;, verify_shape=False)：\n</code></pre><blockquote>\n<p>从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。</p>\n</blockquote>\n<blockquote>\n<p>还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。</p>\n</blockquote>\n<pre><code>input1 = tf.placeholder(tf.types.float32)\ninput2 = tf.placeholder(tf.types.float32)\noutput = tf.mul(input1, input2)\n</code></pre><blockquote>\n<pre><code>with tf.Session() as sess:\n  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n</code></pre></blockquote>\n<pre><code># 输出:\n# [array([ 14.], dtype=float32)]\n</code></pre><blockquote>\n<p>占位符的定义原型，也是一个函数：</p>\n</blockquote>\n<pre><code>def placeholder(dtype, shape=None, name=None)：\n</code></pre><blockquote>\n<p>到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：</p>\n</blockquote>\n<hr>\n<blockquote>\n<p>main.py驱动程序</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 20:41\n# @Author  : Jasontang\n# @Site    : \n# @File    : main.py\n# @ToDo    : 驱动程序\n\nimport _thread\n\nfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\n\nif __name__ == &apos;__main__&apos;:\n    _thread.start_new_thread(mnist_train.main, (None,))\n    _thread.start_new_thread(mnist_eval.main, (None,))\n\n    # 这个不能删除，当做主线程\n    while 1:\n        pass\n</code></pre><blockquote>\n<p>mnist_inference.py计算前向传播的过程及定义了神经网络的参数</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/20 19:43\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_inference.py\n# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\n\nimport tensorflow as tf\n\n# 定义神经网络结构相关的参数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\n\n# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(&quot;weights&quot;, shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\n    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n    # 自定义集合\n    if regularizer:\n        tf.add_to_collection(&quot;losses&quot;, regularizer(weights))\n    return weights\n\n\n# 前向传播过程\ndef inference(input_tensor, regularizer):\n    # 声明第一层神经网络的变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer1&quot;):\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    # 声明第二层圣经网络变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer2&quot;):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n    # 返回最后前向传播的结果\n    return layer2\n</code></pre><blockquote>\n<p>mnist_train.py定义了神经网络的训练过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:08\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_train.py\n# @ToDo    : 定义了神经网络的训练过程\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\n# 配置神经网络的参数\nBATCH_SIZE = 100\nLEARNING_REATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAING_STEPS = 2000\nMOVING_AVERAGE_DECAY = 0.99\n# 模型保存的路径和文件名\nMODEL_SAVE_PATH = &quot;./model/&quot;\nMODEL_NAME = &quot;model.ckpt&quot;\n\n\ndef train(mnist):\n    # 定义输入输出placeholder\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_average_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(&quot;losses&quot;))\n    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                              global_step,\n                                              mnist.train.num_examples / BATCH_SIZE,\n                                              LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\n    with tf.control_dependencies([train_step, variables_average_op]):\n        train_op = tf.no_op(name=&quot;train&quot;)\n\n    # 初始化持久化类\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\n            if i % 1000 == 0:\n                print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (i, loss_value))\n\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    train(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><blockquote>\n<p>mnist_eval.py测试过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:32\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_eval.py\n# @ToDo    : 测试过程\n\n\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\nimport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\n# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\nEVAL_INTERVAL_SECS = 10\n\n\ndef evaluate(mnist):\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    validate_feed = {x: mnist.validation.images,\n                     y_: mnist.validation.labels}\n\n    y = mnist_inference.inference(x, None)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n    stop_count = 0\n    while True:\n        with tf.Session() as sess:\n            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n            # 停止条件 #\n            stop_count += EVAL_INTERVAL_SECS\n            if stop_count == mnist_train.TRAING_STEPS:\n                return\n            # 停止条件 #\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                # 通过文件名得到模型保存时迭代的轮数\n                # 输出./model/model.ckpt-29001\n                print(ckpt.model_checkpoint_path)\n                global_step = ckpt.model_checkpoint_path.split(&quot;/&quot;)[-1].split(&quot;-&quot;)[-1]\n                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n                print(&quot;After %s training step(s), validation accuracy is %g&quot; % (global_step, accuracy_score))\n            else:\n                print(&quot;No checkpoint file found&quot;)\n                return\n        time.sleep(EVAL_INTERVAL_SECS)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    evaluate(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><h1 id=\"参考文章\"><a href=\"#参考文章\" class=\"headerlink\" title=\"参考文章\"></a>参考文章</h1><p><a href=\"https://www.cnblogs.com/shihuc/p/6648130.html\" title=\"Tensorflow之基于MNIST手写识别的入门介绍\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/shihuc/p/6648130.html</a></p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<h1 id=\"Tensorflow\"><a href=\"#Tensorflow\" class=\"headerlink\" title=\"Tensorflow\"></a>Tensorflow</h1><blockquote>\n<p>首先，简单的说下，tensorflow的基本架构。<br>使用 TensorFlow, 你必须明白 TensorFlow:</p>\n</blockquote>\n<ul>\n<li>使用图 (graph) 来表示计算任务.</li>\n<li>在被称之为 会话 (Session) 的上下文 (context) 中执行图.</li>\n<li>使用 tensor 表示数据.</li>\n<li>通过 变量 (Variable) 维护状态.</li>\n<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>\n</ul>","more":"<h1 id=\"Tensor\"><a href=\"#Tensor\" class=\"headerlink\" title=\"Tensor\"></a>Tensor</h1><blockquote>\n<p>TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels].</p>\n</blockquote>\n<blockquote>\n<p>一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例.</p>\n</blockquote>\n<blockquote>\n<p>Tensor是tensorflow中非常重要且非常基础的概念，可以说数据的呈现形式都是用tensor表示的。输入输出都是tensor，tensor的中文含义，就是张量，可以简单的理解为线性代数里面的向量或者矩阵。</p>\n</blockquote>\n<h1 id=\"Graph\"><a href=\"#Graph\" class=\"headerlink\" title=\"Graph\"></a>Graph</h1><blockquote>\n<p>TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op.</p>\n</blockquote>\n<blockquote>\n<p>例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. 下面这个图，就是一个比较形象的说明，图中的每一个节点，就是一个op，各个op透过tensor数据流向形成边的连接，构成了一个图。</p>\n</blockquote>\n<p><img src=\"https://images2015.cnblogs.com/blog/844237/201703/844237-20170330093311608-2056024255.gif\" alt></p>\n<blockquote>\n<p>构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 常量 (Constant). 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入.</p>\n</blockquote>\n<blockquote>\n<p>TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.</p>\n</blockquote>\n<h1 id=\"Session\"><a href=\"#Session\" class=\"headerlink\" title=\"Session\"></a>Session</h1><blockquote>\n<p>当图构建好后，需要创建一个Session来运行构建好的图，来实现逻辑，创建session的时候，若无任何参数，tensorflow将启用默认的session。session.run(xxx)是比较典型的使用方案, session运行结束后，返回值是一个tensor。</p>\n</blockquote>\n<blockquote>\n<p>tensorflow中的session，有两大类，一种就是普通的session，即tensorflow.Session(),还有一种是交互式session，即tensorflow.InteractiveSession(). 使用Tensor.eval() 和Operation.run()方法代替Session.run(). 这样可以避免使用一个变量来持有会话, 为程序架构的设计添加了灵活性.</p>\n</blockquote>\n<h1 id=\"数据载体\"><a href=\"#数据载体\" class=\"headerlink\" title=\"数据载体\"></a>数据载体</h1><blockquote>\n<p>Tensorflow体系下，变量（Variable）是用来维护图计算过程中的中间状态信息，是一种常见高频使用的数据载体，还有一种特殊的数据载体，那就是常量（Constant），主要是用作图处理过程的输入量。这些数据载体，也都是以Tensor的形式体现。变量定义和常量定义上，比较好理解：</p>\n</blockquote>\n<pre><code># 创建一个变量, 初始化为标量0.没有指定数据类型（dtype）\nstate = tf.Variable(0, name=&quot;counter&quot;)\n\n# 创建一个常量，其值为1，没有指定数据类型（dtype）\none = tf.constant(1)\n</code></pre><blockquote>\n<p>针对上面的变量和常量，看看Tensorflow里面的函数定义：</p>\n</blockquote>\n<pre><code>class Variable(object):　\ndef __init__(self,\n    initial_value=None,\n    trainable=True,\n    collections=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    expected_shape=None,\n    import_scope=None)：\n</code></pre><blockquote>\n</blockquote>\n<pre><code>def constant(value, dtype=None, shape=None, name=&quot;Const&quot;, verify_shape=False)：\n</code></pre><blockquote>\n<p>从上面的源码可以看出，定义变量，其实就是定义了一个Variable的实例，而定义常量，其实就是调用了一下常量函数，创建了一个常量Tensor。</p>\n</blockquote>\n<blockquote>\n<p>还有一个很重要的概念，那就是占位符placeholder，这个在Tensorflow中进行Feed数据灌入时，很有用。所谓的数据灌入，指的是在创建Tensorflow的图时，节点的输入部分，就是一个placeholder，后续在执行session操作的前，将实际数据Feed到图中，进行执行即可。</p>\n</blockquote>\n<pre><code>input1 = tf.placeholder(tf.types.float32)\ninput2 = tf.placeholder(tf.types.float32)\noutput = tf.mul(input1, input2)\n</code></pre><blockquote>\n<pre><code>with tf.Session() as sess:\n  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})\n</code></pre></blockquote>\n<pre><code># 输出:\n# [array([ 14.], dtype=float32)]\n</code></pre><blockquote>\n<p>占位符的定义原型，也是一个函数：</p>\n</blockquote>\n<pre><code>def placeholder(dtype, shape=None, name=None)：\n</code></pre><blockquote>\n<p>到此，Tensorflow的入门级的基本知识介绍完了。下面，将结合一个MNIST的手写识别的例子，从代码上简单分析一下，源代码分成4个文件：</p>\n</blockquote>\n<hr>\n<blockquote>\n<p>main.py驱动程序</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 20:41\n# @Author  : Jasontang\n# @Site    : \n# @File    : main.py\n# @ToDo    : 驱动程序\n\nimport _thread\n\nfrom neural_network_learning.hand_writting_refactor import mnist_train, mnist_eval\n\n\nif __name__ == &apos;__main__&apos;:\n    _thread.start_new_thread(mnist_train.main, (None,))\n    _thread.start_new_thread(mnist_eval.main, (None,))\n\n    # 这个不能删除，当做主线程\n    while 1:\n        pass\n</code></pre><blockquote>\n<p>mnist_inference.py计算前向传播的过程及定义了神经网络的参数</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/20 19:43\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_inference.py\n# @ToDo    : 定义了前向传播的过程及神经网络的参数\n\n\nimport tensorflow as tf\n\n# 定义神经网络结构相关的参数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\nLAYER1_NODE = 500\n\n\n# 训练时会创建这些变量，测试时会通过保存的模型加载这些变量的取值\ndef get_weight_variable(shape, regularizer):\n    weights = tf.get_variable(&quot;weights&quot;, shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n\n    # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n    # 自定义集合\n    if regularizer:\n        tf.add_to_collection(&quot;losses&quot;, regularizer(weights))\n    return weights\n\n\n# 前向传播过程\ndef inference(input_tensor, regularizer):\n    # 声明第一层神经网络的变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer1&quot;):\n        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n\n    # 声明第二层圣经网络变量并完成前向传播过程\n    with tf.variable_scope(&quot;layer2&quot;):\n        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n        biases = tf.get_variable(&quot;biases&quot;, [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n        layer2 = tf.matmul(layer1, weights) + biases\n    # 返回最后前向传播的结果\n    return layer2\n</code></pre><blockquote>\n<p>mnist_train.py定义了神经网络的训练过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:08\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_train.py\n# @ToDo    : 定义了神经网络的训练过程\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\n\n# 配置神经网络的参数\nBATCH_SIZE = 100\nLEARNING_REATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAING_STEPS = 2000\nMOVING_AVERAGE_DECAY = 0.99\n# 模型保存的路径和文件名\nMODEL_SAVE_PATH = &quot;./model/&quot;\nMODEL_NAME = &quot;model.ckpt&quot;\n\n\ndef train(mnist):\n    # 定义输入输出placeholder\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_average_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(&quot;losses&quot;))\n    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                              global_step,\n                                              mnist.train.num_examples / BATCH_SIZE,\n                                              LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\n    with tf.control_dependencies([train_step, variables_average_op]):\n        train_op = tf.no_op(name=&quot;train&quot;)\n\n    # 初始化持久化类\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys})\n\n            if i % 1000 == 0:\n                print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (i, loss_value))\n\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    train(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><blockquote>\n<p>mnist_eval.py测试过程</p>\n</blockquote>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:32\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_eval.py\n# @ToDo    : 测试过程\n\n\nimport time\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.hand_writting_refactor.mnist_inference as mnist_inference\nimport neural_network_learning.hand_writting_refactor.mnist_train as mnist_train\n\n# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\nEVAL_INTERVAL_SECS = 10\n\n\ndef evaluate(mnist):\n    x = tf.placeholder(tf.float32, [None, mnist_inference.INPUT_NODE], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    validate_feed = {x: mnist.validation.images,\n                     y_: mnist.validation.labels}\n\n    y = mnist_inference.inference(x, None)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n    stop_count = 0\n    while True:\n        with tf.Session() as sess:\n            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n            # 停止条件 #\n            stop_count += EVAL_INTERVAL_SECS\n            if stop_count == mnist_train.TRAING_STEPS:\n                return\n            # 停止条件 #\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                # 通过文件名得到模型保存时迭代的轮数\n                # 输出./model/model.ckpt-29001\n                print(ckpt.model_checkpoint_path)\n                global_step = ckpt.model_checkpoint_path.split(&quot;/&quot;)[-1].split(&quot;-&quot;)[-1]\n                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n                print(&quot;After %s training step(s), validation accuracy is %g&quot; % (global_step, accuracy_score))\n            else:\n                print(&quot;No checkpoint file found&quot;)\n                return\n        time.sleep(EVAL_INTERVAL_SECS)\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(&quot;../MNIST_data&quot;, one_hot=True)\n    evaluate(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><h1 id=\"参考文章\"><a href=\"#参考文章\" class=\"headerlink\" title=\"参考文章\"></a>参考文章</h1><p><a href=\"https://www.cnblogs.com/shihuc/p/6648130.html\" title=\"Tensorflow之基于MNIST手写识别的入门介绍\" target=\"_blank\" rel=\"noopener\">https://www.cnblogs.com/shihuc/p/6648130.html</a></p>"},{"title":"深度学习中的Normalization模型","date":"2019-04-20T03:20:02.000Z","_content":"\n主要学习了BN、Layer Normalization、Instance Normalization及Group Normalization,具体如下：\n<!-- more -->\n\n1. Normalization到底是在做什么？\n\n\t规范化就是希望对x进行一定的变换（规范化函数），使得规范化后的数值满足一定的特性。\n\n\t至于深度学习中的Normalization，因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，一类是对第L层每个神经元的激活值或者说对于第L+1层网络神经元的输入值进行Normalization操作，比如BatchNorm/LayerNorm/InstanceNorm/GroupNorm等方法都属于这一类；另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如Weight Norm就属于这一类。广义上讲，一般机器学习里看到的损失函数里面加入的对参数的的L1/L2等正则项，本质上也属于这第二类规范化操作。L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果，而L2正则的规范化目标是有效减小原始参数值的大小。有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。\n\n2. Normalization加入的位置？\n\n\t第一种是原始BN论文提出的，放在激活函数之前；另外一种是后续研究提出的，放在激活函数之后，不少研究表明将BN放在激活函数之后效果更好。\n\n\t### 公式为：**a<sub>i</sub><sup>norm</sup>=γ<sub>i</sub> * (a<sub>i</sub> - μ)/δ<sub>i</sub> + β<sub>i</sub>** \n\t其中,μ和δ分别为神经元集合S的均值和标准差。\n\n\t### 不同Normalization方法确定S集合的方式不同。\n\n3. Batch Normalization 如何做？\n\n\t3.1. 前向神经网络中的BN\n\t\n\t对于前向神经网络来说，BatchNorm在计算隐层某个神经元k激活的规范值的时候，对应的神经元集合S范围是如何划定呢？因为对于Mini-Batch训练方法来说，根据Loss更新梯度使用Batch中所有实例来做，所以对于神经元k来说，假设某个Batch包含n个训练实例，那么每个训练实例在神经元k都会产生一个激活值，也就是说Batch中n个训练实例分别通过同一个神经元k的时候产生了n个激活值，BatchNorm的集合S选择入围的神经元就是这n个同一个神经元被Batch不同训练实例激发的激活值。划定集合S的范围后，采用上述公式即可完成规范化操作。\n\t\n\t3.2 CNN网络中的BN\n\t\n\t那么在卷积层中，如果要对通道激活二维平面中某个激活值进行Normalization操作，怎么确定集合S的范围呢？类似于前向神经网络中的BatchNorm计算过程，对于Mini-Batch训练方法来说，反向传播更新梯度使用Batch中所有实例的梯度方向来进行，所以对于CNN某个卷积层对应的输出通道k来说，假设某个Batch包含n个训练实例，那么每个训练实例在这个通道k都会产生一个二维激活平面，也就是说Batch中n个训练实例分别通过同一个卷积核的输出通道k的时候产生了n个激活平面。假设激活平面长为5，宽为4，则激活平面包含20个激活值，n个不同实例的激活平面共包含20\\*n个激活值。那么BatchNorm的集合S的范围就是由这20*n个同一个通道被Batch不同训练实例激发的激活平面中包含的所有激活值构成。划定集合S的范围后，激活平面中任意一个激活值都需进行Normalization操作，采用上述公式即可完成规范化操作。这样即完成CNN卷积层的BatchNorm转换过程。\n\t\n\t从理论上讲，类似的BatchNorm操作也可以应用在RNN上，事实上也有不少研究做了尝试，但是各种实验证明其实这么做没什么用。\n\t\n\t3.3 Batch Norm的缺点\n\t- 如果Batch Size太小，则BN效果明显下降。\n\t- 对于有些像素级图片生成任务来说，BN效果不佳。\n\t- RNN等动态网络使用BN效果不佳且使用起来不方便。\n\t- 训练时和推理时统计量不一致\n\t\n\t上面所列BN的四个缺点，表面看是四个问题，其实深入思考，都指向了幕后同一个黑手，这个隐藏在暗处的黑手是谁呢？就是BN要求计算统计量的时候必须在同一个Mini-Batch内的实例之间进行统计，因此形成了Batch内实例之间的相互依赖和影响的关系。如何从根本上解决这些问题？一个自然的想法是：把对Batch的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖Batch内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。在BN后的几乎所有改进模型都是在这个指导思想下进行的。\n\n4. Layer Normalization 如何做？\n\t\n\t为了能够在只有当前一个训练实例的情形下，也能找到一个合理的统计范围，一个最直接的想法是：前向神经网络(如MLP)的同一隐层自己包含了若干神经元；同理，CNN中同一个卷积层包含k个输出通道，每个通道包含m\\*n个神经元，整个通道包含了k\\*m*n个神经元；类似的，RNN的每个时间步的隐层也包含了若干神经元。那么我们完全可以直接用同层隐层神经元的响应值作为集合S的范围来求均值和方差。这就是Layer Normalization的基本思想。\n\n\tBN在RNN中用起来很不方便，而Layer Normalization这种在同隐层内计算统计量的模式就比较符合RNN这种动态网络，目前在RNN中貌似也只有LayerNorm相对有效，但Layer Normalization目前看好像也只适合应用在RNN场景下，在CNN等环境下效果是不如BatchNorm或者GroupNorm等模型的。\n5. Instance Normalization 如何做？\n\t\n\tLayer Normalization在抛开对Mini-Batch的依赖目标下，为了能够统计均值方差，很自然地把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？对于CNN明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。对于图中某个卷积层来说，每个输出通道内的神经元会作为集合S来统计均值方差。\n\t\n\t对于RNN或者MLP，如果在同一个隐层类似CNN这样缩小范围，那么就只剩下单独一个神经元，输出也是单值而非CNN的二维平面，这意味着没有形成集合S，所以RNN和MLP是无法进行Instance Normalization操作的，这个很好理解。\n6. Group Normalization 如何做？\n\t\n\t从Layer Normalization和Instance Normalization可以看出，这是两种极端情况，Layer Normalization是将同层所有神经元作为统计范围，而Instance Normalization则是CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。那么，有没有介于两者之间的统计范围呢？通道分组是CNN常用的模型优化技巧，所以自然而然会想到对CNN中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计。这就是Group Normalization的核心思想，是Facebook何凯明研究组2017年提出的改进模型。\n\t\n7. BN为何有效？\n\n\t原始的BN论文给出的解释是BN可以解决神经网络训练过程中的ICS（Internal Covariate Shift）问题，所谓ICS问题，指的是由于深度网络由很多隐层构成，在训练过程中由于底层网络参数不断变化，导致上层隐层神经元激活值的分布逐渐发生很大的变化和偏移，而这非常不利于有效稳定地训练神经网络。**不过后来经过学者研究发现BN和ICS并没有什么关系**\n\t\n\tBN与损失曲面有关，在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面形态异常复杂，大量空间坑坑洼洼相当不平整，也有很多空间是由平坦的大量充满鞍点的曲面构成，训练过程就是利用SGD在这个复杂平面上一步一步游走，期望找到全局最小值，也就是曲面里最深的那个坑。所以在SGD寻优时，在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。\n\t\n\t研究表明，BN真正的用处在于：通过上文所述的Normalization操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多(平滑程度用L-Lipschitz函数来评估)。\n\t\n8. **总结**\n\n\t所有Normalization模型都采取了类似的步骤和过程，将神经元的激活值重整为均值为0方差为1的新数值，最大的不同在于计算统计量的神经元集合S的划分方法上。BN采用了同一个神经元，但是来自于Mini-Batch中不同训练实例导致的不同激活作为统计范围。而为了克服Mini-Batch带来的弊端，后续改进方法抛弃了Mini-Batch的思路，只用当前训练实例引发的激活来划分集合S的统计范围，概括而言，LayerNorm采用同隐层的所有神经元；InstanceNorm采用CNN中卷积层的单个通道作为统计范围，而GroupNorm则折衷两者，采用卷积层的通道分组，在划分为同一个分组的通道内来作为通道范围。\n\t\n\t**使用场景:** 对于RNN的神经网络结构来说，目前只有LayerNorm是相对有效的；如果是GAN等图片生成或图片内容改写类型的任务，可以优先尝试InstanceNorm；如果使用场景约束BatchSize必须设置很小，无疑此时考虑使用GroupNorm；而其它任务情形应该优先考虑使用BatchNorm。\n","source":"_posts/第 4 周学习分享.md","raw":"---\ntitle: 深度学习中的Normalization模型\ndate: 2019-04-20 11:20:02\ncategories:\n- 周总结\ntags: \n- Normalization\n- BN\n- GN\n- deep learning\n---\n\n主要学习了BN、Layer Normalization、Instance Normalization及Group Normalization,具体如下：\n<!-- more -->\n\n1. Normalization到底是在做什么？\n\n\t规范化就是希望对x进行一定的变换（规范化函数），使得规范化后的数值满足一定的特性。\n\n\t至于深度学习中的Normalization，因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，一类是对第L层每个神经元的激活值或者说对于第L+1层网络神经元的输入值进行Normalization操作，比如BatchNorm/LayerNorm/InstanceNorm/GroupNorm等方法都属于这一类；另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如Weight Norm就属于这一类。广义上讲，一般机器学习里看到的损失函数里面加入的对参数的的L1/L2等正则项，本质上也属于这第二类规范化操作。L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果，而L2正则的规范化目标是有效减小原始参数值的大小。有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。\n\n2. Normalization加入的位置？\n\n\t第一种是原始BN论文提出的，放在激活函数之前；另外一种是后续研究提出的，放在激活函数之后，不少研究表明将BN放在激活函数之后效果更好。\n\n\t### 公式为：**a<sub>i</sub><sup>norm</sup>=γ<sub>i</sub> * (a<sub>i</sub> - μ)/δ<sub>i</sub> + β<sub>i</sub>** \n\t其中,μ和δ分别为神经元集合S的均值和标准差。\n\n\t### 不同Normalization方法确定S集合的方式不同。\n\n3. Batch Normalization 如何做？\n\n\t3.1. 前向神经网络中的BN\n\t\n\t对于前向神经网络来说，BatchNorm在计算隐层某个神经元k激活的规范值的时候，对应的神经元集合S范围是如何划定呢？因为对于Mini-Batch训练方法来说，根据Loss更新梯度使用Batch中所有实例来做，所以对于神经元k来说，假设某个Batch包含n个训练实例，那么每个训练实例在神经元k都会产生一个激活值，也就是说Batch中n个训练实例分别通过同一个神经元k的时候产生了n个激活值，BatchNorm的集合S选择入围的神经元就是这n个同一个神经元被Batch不同训练实例激发的激活值。划定集合S的范围后，采用上述公式即可完成规范化操作。\n\t\n\t3.2 CNN网络中的BN\n\t\n\t那么在卷积层中，如果要对通道激活二维平面中某个激活值进行Normalization操作，怎么确定集合S的范围呢？类似于前向神经网络中的BatchNorm计算过程，对于Mini-Batch训练方法来说，反向传播更新梯度使用Batch中所有实例的梯度方向来进行，所以对于CNN某个卷积层对应的输出通道k来说，假设某个Batch包含n个训练实例，那么每个训练实例在这个通道k都会产生一个二维激活平面，也就是说Batch中n个训练实例分别通过同一个卷积核的输出通道k的时候产生了n个激活平面。假设激活平面长为5，宽为4，则激活平面包含20个激活值，n个不同实例的激活平面共包含20\\*n个激活值。那么BatchNorm的集合S的范围就是由这20*n个同一个通道被Batch不同训练实例激发的激活平面中包含的所有激活值构成。划定集合S的范围后，激活平面中任意一个激活值都需进行Normalization操作，采用上述公式即可完成规范化操作。这样即完成CNN卷积层的BatchNorm转换过程。\n\t\n\t从理论上讲，类似的BatchNorm操作也可以应用在RNN上，事实上也有不少研究做了尝试，但是各种实验证明其实这么做没什么用。\n\t\n\t3.3 Batch Norm的缺点\n\t- 如果Batch Size太小，则BN效果明显下降。\n\t- 对于有些像素级图片生成任务来说，BN效果不佳。\n\t- RNN等动态网络使用BN效果不佳且使用起来不方便。\n\t- 训练时和推理时统计量不一致\n\t\n\t上面所列BN的四个缺点，表面看是四个问题，其实深入思考，都指向了幕后同一个黑手，这个隐藏在暗处的黑手是谁呢？就是BN要求计算统计量的时候必须在同一个Mini-Batch内的实例之间进行统计，因此形成了Batch内实例之间的相互依赖和影响的关系。如何从根本上解决这些问题？一个自然的想法是：把对Batch的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖Batch内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。在BN后的几乎所有改进模型都是在这个指导思想下进行的。\n\n4. Layer Normalization 如何做？\n\t\n\t为了能够在只有当前一个训练实例的情形下，也能找到一个合理的统计范围，一个最直接的想法是：前向神经网络(如MLP)的同一隐层自己包含了若干神经元；同理，CNN中同一个卷积层包含k个输出通道，每个通道包含m\\*n个神经元，整个通道包含了k\\*m*n个神经元；类似的，RNN的每个时间步的隐层也包含了若干神经元。那么我们完全可以直接用同层隐层神经元的响应值作为集合S的范围来求均值和方差。这就是Layer Normalization的基本思想。\n\n\tBN在RNN中用起来很不方便，而Layer Normalization这种在同隐层内计算统计量的模式就比较符合RNN这种动态网络，目前在RNN中貌似也只有LayerNorm相对有效，但Layer Normalization目前看好像也只适合应用在RNN场景下，在CNN等环境下效果是不如BatchNorm或者GroupNorm等模型的。\n5. Instance Normalization 如何做？\n\t\n\tLayer Normalization在抛开对Mini-Batch的依赖目标下，为了能够统计均值方差，很自然地把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？对于CNN明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。对于图中某个卷积层来说，每个输出通道内的神经元会作为集合S来统计均值方差。\n\t\n\t对于RNN或者MLP，如果在同一个隐层类似CNN这样缩小范围，那么就只剩下单独一个神经元，输出也是单值而非CNN的二维平面，这意味着没有形成集合S，所以RNN和MLP是无法进行Instance Normalization操作的，这个很好理解。\n6. Group Normalization 如何做？\n\t\n\t从Layer Normalization和Instance Normalization可以看出，这是两种极端情况，Layer Normalization是将同层所有神经元作为统计范围，而Instance Normalization则是CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。那么，有没有介于两者之间的统计范围呢？通道分组是CNN常用的模型优化技巧，所以自然而然会想到对CNN中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计。这就是Group Normalization的核心思想，是Facebook何凯明研究组2017年提出的改进模型。\n\t\n7. BN为何有效？\n\n\t原始的BN论文给出的解释是BN可以解决神经网络训练过程中的ICS（Internal Covariate Shift）问题，所谓ICS问题，指的是由于深度网络由很多隐层构成，在训练过程中由于底层网络参数不断变化，导致上层隐层神经元激活值的分布逐渐发生很大的变化和偏移，而这非常不利于有效稳定地训练神经网络。**不过后来经过学者研究发现BN和ICS并没有什么关系**\n\t\n\tBN与损失曲面有关，在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面形态异常复杂，大量空间坑坑洼洼相当不平整，也有很多空间是由平坦的大量充满鞍点的曲面构成，训练过程就是利用SGD在这个复杂平面上一步一步游走，期望找到全局最小值，也就是曲面里最深的那个坑。所以在SGD寻优时，在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。\n\t\n\t研究表明，BN真正的用处在于：通过上文所述的Normalization操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多(平滑程度用L-Lipschitz函数来评估)。\n\t\n8. **总结**\n\n\t所有Normalization模型都采取了类似的步骤和过程，将神经元的激活值重整为均值为0方差为1的新数值，最大的不同在于计算统计量的神经元集合S的划分方法上。BN采用了同一个神经元，但是来自于Mini-Batch中不同训练实例导致的不同激活作为统计范围。而为了克服Mini-Batch带来的弊端，后续改进方法抛弃了Mini-Batch的思路，只用当前训练实例引发的激活来划分集合S的统计范围，概括而言，LayerNorm采用同隐层的所有神经元；InstanceNorm采用CNN中卷积层的单个通道作为统计范围，而GroupNorm则折衷两者，采用卷积层的通道分组，在划分为同一个分组的通道内来作为通道范围。\n\t\n\t**使用场景:** 对于RNN的神经网络结构来说，目前只有LayerNorm是相对有效的；如果是GAN等图片生成或图片内容改写类型的任务，可以优先尝试InstanceNorm；如果使用场景约束BatchSize必须设置很小，无疑此时考虑使用GroupNorm；而其它任务情形应该优先考虑使用BatchNorm。\n","slug":"第 4 周学习分享","published":1,"updated":"2019-05-08T10:50:25.485Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqr90040lnvvx9p6qppg","content":"<p>主要学习了BN、Layer Normalization、Instance Normalization及Group Normalization,具体如下：<br><a id=\"more\"></a></p>\n<ol>\n<li><p>Normalization到底是在做什么？</p>\n<p> 规范化就是希望对x进行一定的变换（规范化函数），使得规范化后的数值满足一定的特性。</p>\n<p> 至于深度学习中的Normalization，因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，一类是对第L层每个神经元的激活值或者说对于第L+1层网络神经元的输入值进行Normalization操作，比如BatchNorm/LayerNorm/InstanceNorm/GroupNorm等方法都属于这一类；另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如Weight Norm就属于这一类。广义上讲，一般机器学习里看到的损失函数里面加入的对参数的的L1/L2等正则项，本质上也属于这第二类规范化操作。L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果，而L2正则的规范化目标是有效减小原始参数值的大小。有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。</p>\n</li>\n<li><p>Normalization加入的位置？</p>\n<p> 第一种是原始BN论文提出的，放在激活函数之前；另外一种是后续研究提出的，放在激活函数之后，不少研究表明将BN放在激活函数之后效果更好。</p>\n<h3 id=\"公式为：ainorm-γi-ai-μ-δi-βi\"><a href=\"#公式为：ainorm-γi-ai-μ-δi-βi\" class=\"headerlink\" title=\"公式为：ainorm=γi * (ai - μ)/δi + βi\"></a>公式为：<strong>a<sub>i</sub><sup>norm</sup>=γ<sub>i</sub> * (a<sub>i</sub> - μ)/δ<sub>i</sub> + β<sub>i</sub></strong></h3><p> 其中,μ和δ分别为神经元集合S的均值和标准差。</p>\n<h3 id=\"不同Normalization方法确定S集合的方式不同。\"><a href=\"#不同Normalization方法确定S集合的方式不同。\" class=\"headerlink\" title=\"不同Normalization方法确定S集合的方式不同。\"></a>不同Normalization方法确定S集合的方式不同。</h3></li>\n<li><p>Batch Normalization 如何做？</p>\n<p> 3.1. 前向神经网络中的BN</p>\n<p> 对于前向神经网络来说，BatchNorm在计算隐层某个神经元k激活的规范值的时候，对应的神经元集合S范围是如何划定呢？因为对于Mini-Batch训练方法来说，根据Loss更新梯度使用Batch中所有实例来做，所以对于神经元k来说，假设某个Batch包含n个训练实例，那么每个训练实例在神经元k都会产生一个激活值，也就是说Batch中n个训练实例分别通过同一个神经元k的时候产生了n个激活值，BatchNorm的集合S选择入围的神经元就是这n个同一个神经元被Batch不同训练实例激发的激活值。划定集合S的范围后，采用上述公式即可完成规范化操作。</p>\n<p> 3.2 CNN网络中的BN</p>\n<p> 那么在卷积层中，如果要对通道激活二维平面中某个激活值进行Normalization操作，怎么确定集合S的范围呢？类似于前向神经网络中的BatchNorm计算过程，对于Mini-Batch训练方法来说，反向传播更新梯度使用Batch中所有实例的梯度方向来进行，所以对于CNN某个卷积层对应的输出通道k来说，假设某个Batch包含n个训练实例，那么每个训练实例在这个通道k都会产生一个二维激活平面，也就是说Batch中n个训练实例分别通过同一个卷积核的输出通道k的时候产生了n个激活平面。假设激活平面长为5，宽为4，则激活平面包含20个激活值，n个不同实例的激活平面共包含20*n个激活值。那么BatchNorm的集合S的范围就是由这20*n个同一个通道被Batch不同训练实例激发的激活平面中包含的所有激活值构成。划定集合S的范围后，激活平面中任意一个激活值都需进行Normalization操作，采用上述公式即可完成规范化操作。这样即完成CNN卷积层的BatchNorm转换过程。</p>\n<p> 从理论上讲，类似的BatchNorm操作也可以应用在RNN上，事实上也有不少研究做了尝试，但是各种实验证明其实这么做没什么用。</p>\n<p> 3.3 Batch Norm的缺点</p>\n<ul>\n<li>如果Batch Size太小，则BN效果明显下降。</li>\n<li>对于有些像素级图片生成任务来说，BN效果不佳。</li>\n<li>RNN等动态网络使用BN效果不佳且使用起来不方便。</li>\n<li><p>训练时和推理时统计量不一致</p>\n<p>上面所列BN的四个缺点，表面看是四个问题，其实深入思考，都指向了幕后同一个黑手，这个隐藏在暗处的黑手是谁呢？就是BN要求计算统计量的时候必须在同一个Mini-Batch内的实例之间进行统计，因此形成了Batch内实例之间的相互依赖和影响的关系。如何从根本上解决这些问题？一个自然的想法是：把对Batch的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖Batch内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。在BN后的几乎所有改进模型都是在这个指导思想下进行的。</p>\n</li>\n</ul>\n</li>\n<li><p>Layer Normalization 如何做？</p>\n<p> 为了能够在只有当前一个训练实例的情形下，也能找到一个合理的统计范围，一个最直接的想法是：前向神经网络(如MLP)的同一隐层自己包含了若干神经元；同理，CNN中同一个卷积层包含k个输出通道，每个通道包含m*n个神经元，整个通道包含了k*m*n个神经元；类似的，RNN的每个时间步的隐层也包含了若干神经元。那么我们完全可以直接用同层隐层神经元的响应值作为集合S的范围来求均值和方差。这就是Layer Normalization的基本思想。</p>\n<p> BN在RNN中用起来很不方便，而Layer Normalization这种在同隐层内计算统计量的模式就比较符合RNN这种动态网络，目前在RNN中貌似也只有LayerNorm相对有效，但Layer Normalization目前看好像也只适合应用在RNN场景下，在CNN等环境下效果是不如BatchNorm或者GroupNorm等模型的。</p>\n</li>\n<li><p>Instance Normalization 如何做？</p>\n<p> Layer Normalization在抛开对Mini-Batch的依赖目标下，为了能够统计均值方差，很自然地把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？对于CNN明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。对于图中某个卷积层来说，每个输出通道内的神经元会作为集合S来统计均值方差。</p>\n<p> 对于RNN或者MLP，如果在同一个隐层类似CNN这样缩小范围，那么就只剩下单独一个神经元，输出也是单值而非CNN的二维平面，这意味着没有形成集合S，所以RNN和MLP是无法进行Instance Normalization操作的，这个很好理解。</p>\n</li>\n<li><p>Group Normalization 如何做？</p>\n<p> 从Layer Normalization和Instance Normalization可以看出，这是两种极端情况，Layer Normalization是将同层所有神经元作为统计范围，而Instance Normalization则是CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。那么，有没有介于两者之间的统计范围呢？通道分组是CNN常用的模型优化技巧，所以自然而然会想到对CNN中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计。这就是Group Normalization的核心思想，是Facebook何凯明研究组2017年提出的改进模型。</p>\n</li>\n<li><p>BN为何有效？</p>\n<p> 原始的BN论文给出的解释是BN可以解决神经网络训练过程中的ICS（Internal Covariate Shift）问题，所谓ICS问题，指的是由于深度网络由很多隐层构成，在训练过程中由于底层网络参数不断变化，导致上层隐层神经元激活值的分布逐渐发生很大的变化和偏移，而这非常不利于有效稳定地训练神经网络。<strong>不过后来经过学者研究发现BN和ICS并没有什么关系</strong></p>\n<p> BN与损失曲面有关，在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面形态异常复杂，大量空间坑坑洼洼相当不平整，也有很多空间是由平坦的大量充满鞍点的曲面构成，训练过程就是利用SGD在这个复杂平面上一步一步游走，期望找到全局最小值，也就是曲面里最深的那个坑。所以在SGD寻优时，在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。</p>\n<p> 研究表明，BN真正的用处在于：通过上文所述的Normalization操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多(平滑程度用L-Lipschitz函数来评估)。</p>\n</li>\n<li><p><strong>总结</strong></p>\n<p> 所有Normalization模型都采取了类似的步骤和过程，将神经元的激活值重整为均值为0方差为1的新数值，最大的不同在于计算统计量的神经元集合S的划分方法上。BN采用了同一个神经元，但是来自于Mini-Batch中不同训练实例导致的不同激活作为统计范围。而为了克服Mini-Batch带来的弊端，后续改进方法抛弃了Mini-Batch的思路，只用当前训练实例引发的激活来划分集合S的统计范围，概括而言，LayerNorm采用同隐层的所有神经元；InstanceNorm采用CNN中卷积层的单个通道作为统计范围，而GroupNorm则折衷两者，采用卷积层的通道分组，在划分为同一个分组的通道内来作为通道范围。</p>\n<p> <strong>使用场景:</strong> 对于RNN的神经网络结构来说，目前只有LayerNorm是相对有效的；如果是GAN等图片生成或图片内容改写类型的任务，可以优先尝试InstanceNorm；如果使用场景约束BatchSize必须设置很小，无疑此时考虑使用GroupNorm；而其它任务情形应该优先考虑使用BatchNorm。</p>\n</li>\n</ol>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<p>主要学习了BN、Layer Normalization、Instance Normalization及Group Normalization,具体如下：<br>","more":"</p>\n<ol>\n<li><p>Normalization到底是在做什么？</p>\n<p> 规范化就是希望对x进行一定的变换（规范化函数），使得规范化后的数值满足一定的特性。</p>\n<p> 至于深度学习中的Normalization，因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，一类是对第L层每个神经元的激活值或者说对于第L+1层网络神经元的输入值进行Normalization操作，比如BatchNorm/LayerNorm/InstanceNorm/GroupNorm等方法都属于这一类；另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如Weight Norm就属于这一类。广义上讲，一般机器学习里看到的损失函数里面加入的对参数的的L1/L2等正则项，本质上也属于这第二类规范化操作。L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果，而L2正则的规范化目标是有效减小原始参数值的大小。有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。</p>\n</li>\n<li><p>Normalization加入的位置？</p>\n<p> 第一种是原始BN论文提出的，放在激活函数之前；另外一种是后续研究提出的，放在激活函数之后，不少研究表明将BN放在激活函数之后效果更好。</p>\n<h3 id=\"公式为：ainorm-γi-ai-μ-δi-βi\"><a href=\"#公式为：ainorm-γi-ai-μ-δi-βi\" class=\"headerlink\" title=\"公式为：ainorm=γi * (ai - μ)/δi + βi\"></a>公式为：<strong>a<sub>i</sub><sup>norm</sup>=γ<sub>i</sub> * (a<sub>i</sub> - μ)/δ<sub>i</sub> + β<sub>i</sub></strong></h3><p> 其中,μ和δ分别为神经元集合S的均值和标准差。</p>\n<h3 id=\"不同Normalization方法确定S集合的方式不同。\"><a href=\"#不同Normalization方法确定S集合的方式不同。\" class=\"headerlink\" title=\"不同Normalization方法确定S集合的方式不同。\"></a>不同Normalization方法确定S集合的方式不同。</h3></li>\n<li><p>Batch Normalization 如何做？</p>\n<p> 3.1. 前向神经网络中的BN</p>\n<p> 对于前向神经网络来说，BatchNorm在计算隐层某个神经元k激活的规范值的时候，对应的神经元集合S范围是如何划定呢？因为对于Mini-Batch训练方法来说，根据Loss更新梯度使用Batch中所有实例来做，所以对于神经元k来说，假设某个Batch包含n个训练实例，那么每个训练实例在神经元k都会产生一个激活值，也就是说Batch中n个训练实例分别通过同一个神经元k的时候产生了n个激活值，BatchNorm的集合S选择入围的神经元就是这n个同一个神经元被Batch不同训练实例激发的激活值。划定集合S的范围后，采用上述公式即可完成规范化操作。</p>\n<p> 3.2 CNN网络中的BN</p>\n<p> 那么在卷积层中，如果要对通道激活二维平面中某个激活值进行Normalization操作，怎么确定集合S的范围呢？类似于前向神经网络中的BatchNorm计算过程，对于Mini-Batch训练方法来说，反向传播更新梯度使用Batch中所有实例的梯度方向来进行，所以对于CNN某个卷积层对应的输出通道k来说，假设某个Batch包含n个训练实例，那么每个训练实例在这个通道k都会产生一个二维激活平面，也就是说Batch中n个训练实例分别通过同一个卷积核的输出通道k的时候产生了n个激活平面。假设激活平面长为5，宽为4，则激活平面包含20个激活值，n个不同实例的激活平面共包含20*n个激活值。那么BatchNorm的集合S的范围就是由这20*n个同一个通道被Batch不同训练实例激发的激活平面中包含的所有激活值构成。划定集合S的范围后，激活平面中任意一个激活值都需进行Normalization操作，采用上述公式即可完成规范化操作。这样即完成CNN卷积层的BatchNorm转换过程。</p>\n<p> 从理论上讲，类似的BatchNorm操作也可以应用在RNN上，事实上也有不少研究做了尝试，但是各种实验证明其实这么做没什么用。</p>\n<p> 3.3 Batch Norm的缺点</p>\n<ul>\n<li>如果Batch Size太小，则BN效果明显下降。</li>\n<li>对于有些像素级图片生成任务来说，BN效果不佳。</li>\n<li>RNN等动态网络使用BN效果不佳且使用起来不方便。</li>\n<li><p>训练时和推理时统计量不一致</p>\n<p>上面所列BN的四个缺点，表面看是四个问题，其实深入思考，都指向了幕后同一个黑手，这个隐藏在暗处的黑手是谁呢？就是BN要求计算统计量的时候必须在同一个Mini-Batch内的实例之间进行统计，因此形成了Batch内实例之间的相互依赖和影响的关系。如何从根本上解决这些问题？一个自然的想法是：把对Batch的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖Batch内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。在BN后的几乎所有改进模型都是在这个指导思想下进行的。</p>\n</li>\n</ul>\n</li>\n<li><p>Layer Normalization 如何做？</p>\n<p> 为了能够在只有当前一个训练实例的情形下，也能找到一个合理的统计范围，一个最直接的想法是：前向神经网络(如MLP)的同一隐层自己包含了若干神经元；同理，CNN中同一个卷积层包含k个输出通道，每个通道包含m*n个神经元，整个通道包含了k*m*n个神经元；类似的，RNN的每个时间步的隐层也包含了若干神经元。那么我们完全可以直接用同层隐层神经元的响应值作为集合S的范围来求均值和方差。这就是Layer Normalization的基本思想。</p>\n<p> BN在RNN中用起来很不方便，而Layer Normalization这种在同隐层内计算统计量的模式就比较符合RNN这种动态网络，目前在RNN中貌似也只有LayerNorm相对有效，但Layer Normalization目前看好像也只适合应用在RNN场景下，在CNN等环境下效果是不如BatchNorm或者GroupNorm等模型的。</p>\n</li>\n<li><p>Instance Normalization 如何做？</p>\n<p> Layer Normalization在抛开对Mini-Batch的依赖目标下，为了能够统计均值方差，很自然地把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？对于CNN明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。对于图中某个卷积层来说，每个输出通道内的神经元会作为集合S来统计均值方差。</p>\n<p> 对于RNN或者MLP，如果在同一个隐层类似CNN这样缩小范围，那么就只剩下单独一个神经元，输出也是单值而非CNN的二维平面，这意味着没有形成集合S，所以RNN和MLP是无法进行Instance Normalization操作的，这个很好理解。</p>\n</li>\n<li><p>Group Normalization 如何做？</p>\n<p> 从Layer Normalization和Instance Normalization可以看出，这是两种极端情况，Layer Normalization是将同层所有神经元作为统计范围，而Instance Normalization则是CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。那么，有没有介于两者之间的统计范围呢？通道分组是CNN常用的模型优化技巧，所以自然而然会想到对CNN中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计。这就是Group Normalization的核心思想，是Facebook何凯明研究组2017年提出的改进模型。</p>\n</li>\n<li><p>BN为何有效？</p>\n<p> 原始的BN论文给出的解释是BN可以解决神经网络训练过程中的ICS（Internal Covariate Shift）问题，所谓ICS问题，指的是由于深度网络由很多隐层构成，在训练过程中由于底层网络参数不断变化，导致上层隐层神经元激活值的分布逐渐发生很大的变化和偏移，而这非常不利于有效稳定地训练神经网络。<strong>不过后来经过学者研究发现BN和ICS并没有什么关系</strong></p>\n<p> BN与损失曲面有关，在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面形态异常复杂，大量空间坑坑洼洼相当不平整，也有很多空间是由平坦的大量充满鞍点的曲面构成，训练过程就是利用SGD在这个复杂平面上一步一步游走，期望找到全局最小值，也就是曲面里最深的那个坑。所以在SGD寻优时，在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。</p>\n<p> 研究表明，BN真正的用处在于：通过上文所述的Normalization操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多(平滑程度用L-Lipschitz函数来评估)。</p>\n</li>\n<li><p><strong>总结</strong></p>\n<p> 所有Normalization模型都采取了类似的步骤和过程，将神经元的激活值重整为均值为0方差为1的新数值，最大的不同在于计算统计量的神经元集合S的划分方法上。BN采用了同一个神经元，但是来自于Mini-Batch中不同训练实例导致的不同激活作为统计范围。而为了克服Mini-Batch带来的弊端，后续改进方法抛弃了Mini-Batch的思路，只用当前训练实例引发的激活来划分集合S的统计范围，概括而言，LayerNorm采用同隐层的所有神经元；InstanceNorm采用CNN中卷积层的单个通道作为统计范围，而GroupNorm则折衷两者，采用卷积层的通道分组，在划分为同一个分组的通道内来作为通道范围。</p>\n<p> <strong>使用场景:</strong> 对于RNN的神经网络结构来说，目前只有LayerNorm是相对有效的；如果是GAN等图片生成或图片内容改写类型的任务，可以优先尝试InstanceNorm；如果使用场景约束BatchSize必须设置很小，无疑此时考虑使用GroupNorm；而其它任务情形应该优先考虑使用BatchNorm。</p>\n</li>\n</ol>"},{"title":"深度学习中的卷积和池化","date":"2018-06-21T13:05:19.000Z","_content":"\n# 1. Convolution  #\n> 卷积是什么？\n> \n> 卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。\n\n<!-- more -->\n\n- 卷积动画演示\n\t- 卷积核\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv-kernel.png)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_no_padding.gif)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_padding.gif)\n> 在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。\n> \n> - 卷积的目的\n> \n> 卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。\n\n----------\n**卷积分为许多种，下面将会一一介绍。**\n\n- 符号约定\n\n> i: 输入大小表示为i x i\n> \n> k: 卷积核大小表示为k x k\n> \n> s: 步长\n> \n> p: 填充\n>\n> o: 输出表示为o*o\n## 1.1 unit strides ##\n卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。\n### 1.1.1  No zero padding, unit strides ###\n\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.png)\n\n无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1_2.png)\n\n动画演示\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.gif)\n\n### 1.1.2 Zero padding, unit strides ###\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2_2.png)\n\n动画演示\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.gif)\n\n#### 1.1.2.1 Zero padding, unit strides - Half(Same) padding ####\n这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.png)\n\n有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3_2.png)\n\n动画演示\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.gif)\n\n#### 1.1.2.2 Zero padding, unit strides - Full padding ####\n卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4_2.png)\n\n动画演示\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.gif)\n\n## 1.2 Non-unit strides ##\n接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。\n### 1.2.1 No zero padding, non-unit strides ###\n\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.png)\n\n无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5_5.png)\n\n其中向下取整是为了避免(i-k)/s是小数的情况。\n\n动画演示\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.gif)\n\n### 1.2.2 Zero padding, non-unit strides ###\n\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.png)\n\n有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6_2.png)\n\n其中向下取整是为了避免(i+2p-k)/s是小数的情况。\n\n动画演示\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.gif)\n\n## 1.3 Convolution as a matrix operation ##\n卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：\n\n1.1.1中的图被表示为如下形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png)\n\n矩阵表示的形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix.png)\n\n我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。\n# 2. Pooling  #\n\n> 池化操作是什么？\n>\n> 池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。\n\n> 池化操作是什么？\n>\n> 池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。\n\n池化分为平均值池化和最大值池化，下面会一一介绍。\n## 2.1 Average Pooling ##\n- 平均值池化可以被表示为\n\n![figure1.5](https://mic-jasontang.github.io/imgs/figure1.5.png)\n\n- 平均值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.5.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n\n## 2.2 Max Pooling ##\n- 最大值池化可以被表示为\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.png)\n\n- 最大值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n# 3. 3D-Conv  #\n3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。\n\n![figure1.6](https://mic-jasontang.github.io/imgs/conv_3d.jpg)\n\n输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。\n\n为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/ \"博客原地址\")\n\n\n<iframe \n    width=\"100%\" \n    height=\"100%\" \n    src=\"http://cs231n.github.io/assets/conv-demo/index.html\"\n点击查看动画[http://cs231n.github.io/assets/conv-demo/index.html](http://cs231n.github.io/assets/conv-demo/index.html)\n\n    width=\"750\" \n    height=\"720\" \n    src=\"https://cs231n.github.io/assets/conv-demo/index.html\"\n    frameborder=\"0\" \n    allowfullscreen>\n</iframe>\n\n# 4. LeNet-5  #\n\n这里介绍下LeNet-5模型，为了理解前面讲述的各种模型\n\n# Bibliography #\n\n这里介绍下LeNet-5模型，为了理解前面讲述的各种卷积和2种池化，下面具体介绍LeNet-5的每个层。\n\n- C1:\n\t- input: 32x32x1\n\t- conv: 5x5x1\n\t- padding: No Zero\n\t- strides: 1\n\t- output: 28x28x6\n\t- parmas: 5x5x1x6+6 = 156\n\n\t\n> 第一层，卷积层\n> \n> 这一层的输入就是原始的图像像素32*32*1。第一个卷积层过滤器尺寸为5*5，深度为6，不使用全0填充，步长为1。所以这一层的输出：28*28*6，卷积层共有5*5*1*6+6=156个参数。\n\n- S2:\n\t- input: 28x28x6\n\t- pool: 2x2\n\t- padding: No Zero\n\t- strides: 2\n\t- output: 14x14x6\n\t\n> 第二层，池化层\n> \n> 这一层的输入为第一层的输出，是一个28*28*6的节点矩阵。本层采用的过滤器大小为2*2，长和宽的步长均为2，所以本层的输出矩阵大小为14*14*6。\n\n- C3:\n\t- input: 14x14x6\n\t- conv: 5x5x16\n\t- padding: No Zero\n\t- strides: 1\n\t- output: 10x10x16\n\t- params: 5x5x6x16+16=2416\n\t\n> 第三层，卷积层\n> \n> 本层的输入矩阵大小为14*14*6，使用的过滤器大小为5*5，深度为16.本层不使用全0填充，步长为1。本层的输出矩阵大小为10*10*16。本层有5*5*6*16+16=2416个参数。\n\n- S4:\n\t- input: 10x10x16\n\t- pool: 2x2\n\t- padding: No Zero\n\t- strides: 2\n\t- output: 5x5x16\n\t\n> 第四层，池化层\n> \n> 本层的输入矩阵大小10*10*16。本层采用的过滤器大小为2*2，长和宽的步长均为2，所以本层的输出矩阵大小为5*5*16。\n\n- C5:\n\t- input: 5x5x16\n\t- conv: 5x5\n\t- padding: No Zero\n\t- strides: 1\n\t- output: 120\n\t- params: 5x5x16x120 + 120=48120\n\n> 第五层，全连接层(卷积层)\n> \n> 本层的输入矩阵大小为5*5*16，在LeNet-5论文中将这一层成为**卷积层**，但是因为过滤器的大小就是5*5，所以和全连接层没有区别。如果将5*5*16矩阵中的节点拉成一个向量，那么这一层和全连接层就一样了。本层的输出节点个数为120，总共有5*5*16*120+120=48120个参数。\n\n- F6:\n\t- input: 120\n\t- output: 84\n\t- params: 120x84+84 = 10164\n\n\n> 第六层，全连接层\n> \n> 本层的输入节点个数为120个，输出节点个数为84个，总共参数为120*84+84=10164个。\n\n- Output:\n\t- input: 84\n\t- output: 10\n\t- parmas: 84x10 + 10 = 850\n\n\n> 第七层，全连接层\n> \n> 本层的输入节点个数为84个，输出节点个数为10个，总共参数为84*10+10=850\n\n# Code #\n\n- lenet_train.py\n\n训练代码\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:08\n\t# @update  : 2018年4月19日21:38:34\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : lenet_train.py\n\t# @ToDo    : 使用LeNet-5模型。定义了神经网络的训练过程\n\t\n\timport os\n\t\n\timport tensorflow as tf\n\timport numpy as np\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\n\t\n\t# 配置神经网络的参数\n\tBATCH_SIZE = 100\n\t# LEARNING_REATE_BASE = 0.8  # 0.8的学习率导致准确率不高。明显看出不收敛，准确率跟瞎猜差不多。\n\tLEARNING_REATE_BASE = 0.01  # 降低学习率\n\tLEARNING_RATE_DECAY = 0.99\n\tREGULARAZTION_RATE = 0.0001\n\tTRAING_STEPS = 30000\n\tMOVING_AVERAGE_DECAY = 0.99\n\t# 模型保存的路径和文件名\n\tMODEL_SAVE_PATH = \"./model/\"\n\tMODEL_NAME = \"model.ckpt\"\n\t\n\t\n\tdef train(mnist):\n    # 定义输入输出placeholder\n    x = tf.placeholder(tf.float32,\n                       [BATCH_SIZE,\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.NUM_CHANNELS], name=\"input-x\")\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, True, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_average_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n    # loss = tf.get_collection(\"losses\") 返回一个列表\n    # loss_add = tf.add_n(loss) 将列表元素进行相加\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n\n    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                              global_step,\n                                              mnist.train.num_examples / BATCH_SIZE,\n                                              LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\n    with tf.control_dependencies([train_step, variables_average_op]):\n        train_op = tf.no_op(name=\"train\")\n\n    # 初始化持久化类\n    saver = tf.train.Saver()\n    with tf.device(\"/gpu:0\"):\n        session_conf = tf.ConfigProto(allow_soft_placement=True)\n        with tf.Session(config=session_conf) as sess:\n            tf.global_variables_initializer().run()\n\n            for i in range(TRAING_STEPS):\n                xs, ys = mnist.train.next_batch(BATCH_SIZE)\n                reshaped_xs = np.reshape(xs, [BATCH_SIZE,\n                                              mnist_inference.IMAGE_SIZE,\n                                              mnist_inference.IMAGE_SIZE,\n                                              mnist_inference.NUM_CHANNELS])\n                # print(type(xs))\n                # print(type(reshaped_xs))\n                _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n\n                if i % 1000 == 0:\n                    print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n\n                    saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\n\tdef main(argv=None):\n\t\t# 存放目录为当前工程目录下的MNIST_data目录\n\t    mnist = input_data.read_data_sets(\"./MNIST_data\", one_hot=True)\n\t    train(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\t    \n\n- lenet_inference\n\n计算代码\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/20 19:43\n\t# @update  : 2018年4月19日21:38:34\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : lent_inference.py\n\t# @ToDo    : 使用LeNet-5模型。定义了前向传播的过程及神经网络的参数\n\t\n\t\n\timport tensorflow as tf\n\t\n\t# 定义神经网络结构相关的参数\n\tINPUT_NODE = 784\n\tOUTPUT_NODE = 10\n\t\n\tIMAGE_SIZE = 28\n\tNUM_CHANNELS = 1\n\tNUM_LABELS = 10\n\t\n\t# 第一层卷积层的尺寸和深度\n\tCONV1_DEEP = 32\n\tCONV1_SIZE = 5\n\t# 第二层卷积层的尺寸和深度\n\tCONV2_DEEP = 64\n\tCONV2_SIZE = 5\n\t# 全连接层的结点个数\n\tFC_SIZE = 512\n\t\n\t# 卷积神经网络的前向传播过程\n\t# 添加一个新的参数train，用于区分训练过程和测试过程。\n\t# 在这个程序中将用到dropout方法，dropout可以进一步提升模型可靠性并防止\n\t# 过拟合。dropout过程只在训练时使用。\n\tdef inference(input_tensor, train, regularizer, dropout=0.5):\n\t    # 声明第一层神经网络的变量并完成前向传播过程\n\t    # 和标准的LeNet-5模型不大一样，这里定义的卷积层输入为28*28*1的原始MNIST图片像素。\n\t    # 因为卷积层使用了全0填充，所以输出为28*28*32的矩阵。\n\t    with tf.variable_scope(\"layer1-conv1\"):\n\t        conv1_weights = tf.get_variable(\"weights\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n\t                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        conv1_biases = tf.get_variable(\"biases\", [CONV1_DEEP], initializer=tf.constant_initializer(0.1))\n\t        # 使用变长为5，深度为32的过滤器，过滤器移动的步长为1，且使用全0填充。\n\t        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n\t        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n\t\n\t    # 实现第二层池化层的前向传播过程，这里选择最大池化层，池化层过滤器的变长为2\n\t    # 使用全0填充且移动的步长为2，这一层的输入是上一层的输出，也就是28*28*32的矩阵。\n\t    # 输出为14*14*32的矩阵\n\t    with tf.variable_scope(\"layer2-pool1\"):\n\t        pool1 = tf.nn.max_pool(relu1,\n\t                               ksize=[1, 2, 2, 1],\n\t                               strides=[1, 2, 2, 1],\n\t                               padding=\"SAME\")\n\t\n\t    # 声明第三层卷积层的变量并实现前向传播过程，这一层的输入为14*14*32的矩阵。\n\t    # 输出为14*14*64的矩阵。\n\t    with tf.variable_scope(\"layer3-conv2\"):\n\t        conv2_weights = tf.get_variable(\"weight\",\n\t                                        [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n\t                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.1))\n\t\n\t        # 使用边长为5，深度为64的过滤器，过滤器移动的步长为1，且使用全0填充。\n\t        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n\t        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n\t\n\t    # 实现第四层池化层的前向传播过程。这一层和第二层的记过是一样的，这一层的输入为14*14*64的矩阵，\n\t    # 输出位7*7*64的矩阵。\n\t    with tf.name_scope(\"layer4-pool2\"):\n\t        pool2 = tf.nn.max_pool(relu2,\n\t                               ksize=[1, 2, 2, 1],\n\t                               strides=[1, 2, 2, 1],\n\t                               padding=\"SAME\")\n\t\n\t    # 将第四层池化层的输出转化为第五层全连接层的输入格式。\n\t    # 第四层的输出为7*7*64的矩阵，然后第五层全连接层需要的输入格式为向量\n\t    # 所以在这里需要将这个7*7*64的矩阵拉直成一个向量。pool2.get_shape函数可以得到\n\t    # 第四层输出矩阵的维度而不需要手工计算。注意因为每一层神经网络的输入输出都为一个batch的矩阵，\n\t    # 所以这里得到的维度也包含了一个batch的数据的个数。\n\t    pool_shape = pool2.get_shape().as_list()\n\t\n\t    # 计算将矩阵拉直成向量之后的长度，这个长度就是矩阵长宽及深度的乘积。\n\t    # 这里pool_shape[0]为一个batch中数据的个数\n\t    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n\t\n\t    # 通过tf.reshape函数将第四层的输出变成一个batch的向量\n\t    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n\t\n\t    # 声明第五层全连接层的变量并实现前向传播过程，这一层的输入是拉直之后的一组向量，\n\t    # 向量长度为3136，输出是一组长度为512的向量。这一层和之前在重构MNIST数据集的代码基本一致，\n\t    # 唯一的区别就是引入了dropout的概念。dropout在训练时会随机将部分结点的输出改为0。\n\t    # dropout可以避免过拟合问题，从而使得模型在测试数据上的效果更好。\n\t    # dropout一般只在全连接层而不是卷积层或者池化层使用。\n\t    with tf.variable_scope(\"layer5-fc1\"):\n\t        fc1_weights = tf.get_variable(\"weight\",\n\t                                      [nodes, FC_SIZE],\n\t                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        # 只要全连接层的权重需要加入正则化\n\t        if regularizer is not None:\n\t            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t            tf.add_to_collection(\"losses\", regularizer(fc1_weights))\n\t        fc1_biases = tf.get_variable(\"bias\",\n\t                                     [FC_SIZE],\n\t                                     initializer=tf.constant_initializer(0.1))\n\t        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n\t        if train:\n\t            fc1 = tf.nn.dropout(fc1, dropout)\n\t\n\t    # 声明第六层全连接层的变量并实现前向传播过程。\n\t    # 这一层的输入为一组长度为512的向量，输出为一组长度为10的向量。\n\t    # 这一层的输出通过Softmax之后就得到了最后的分类结果。\n\t    with tf.variable_scope(\"layer6-fc2\"):\n\t        fc2_weights = tf.get_variable(\"weight\",\n\t                                      [FC_SIZE, NUM_LABELS],\n\t                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        if regularizer is not None:\n\t            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t            tf.add_to_collection(\"losses\", regularizer(fc2_weights))\n\t        fc2_biases = tf.get_variable(\"bias\",\n\t                                     [NUM_LABELS],\n\t                                     initializer=tf.constant_initializer(0.1))\n\t        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n\t\n\t    # 返回第六层的输出\n\t    return logit\n\n\n- lenet_eval.py\n\n测试代码\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018年4月19日21:38:34\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_eval.py\n\t# @ToDo    : 测试过程\n\t\n\t\n\timport time\n\timport tensorflow as tf\n\timport numpy as np\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\n\timport neural_network_learning.cnn.lenet.mnist_train as mnist_train\n\t\n\t# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\n\tEVAL_INTERVAL_SECS = 10\n\t\n\t\n\tdef evaluate(mnist):\n\t    x = tf.placeholder(tf.float32,\n\t                       [mnist.validation.images.shape[0],\n\t                        mnist_inference.IMAGE_SIZE,\n\t                        mnist_inference.IMAGE_SIZE,\n\t                        mnist_inference.NUM_CHANNELS], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    y = mnist_inference.inference(x, False, None)\n\t\n\t    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n\t    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n\t    variables_to_restore = variable_averages.variables_to_restore()\n\t    saver = tf.train.Saver(variables_to_restore)\n\t\n\t    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n\t    stop_count = 0\n\t    while True:\n\t        with tf.Session() as sess:\n\t            # tf.train.get_checkpoint_state函数会通过checkpoint文件自动找刀目录中最新模型的文件名\n\t            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n\t            # 停止条件 #\n\t            stop_count += 1\n\t            if stop_count > 5:\n\t                return\n\t            # 停止条件 #\n\t            if ckpt and ckpt.model_checkpoint_path:\n\t                # 加载模型\n\t                saver.restore(sess, ckpt.model_checkpoint_path)\n\t                # 通过文件名得到模型保存时迭代的轮数\n\t                # 输出./model/model.ckpt-29001\n\t                print(ckpt.model_checkpoint_path)\n\t                global_step = ckpt.model_checkpoint_path.split(\"/\")[-1].split(\"-\")[-1]\n\t                validate_feed = {x: mnist.validation.images,\n\t                                 y_: mnist.validation.labels}\n\t\n\t                # print(validate_feed[x])\n\t                reshaped_x = np.reshape(validate_feed[x],\n\t                                        [validate_feed[x].shape[0],\n\t                                         mnist_inference.IMAGE_SIZE,\n\t                                         mnist_inference.IMAGE_SIZE,\n\t                                         mnist_inference.NUM_CHANNELS])\n\t                validate_feed[x] = reshaped_x\n\t                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n\t                print(\"After %s training step(s), validation accuracy is %g\" % (global_step, accuracy_score))\n\t            else:\n\t                print(\"No checkpoint file found\")\n\t                return\n\t        time.sleep(EVAL_INTERVAL_SECS)\n\t\n\t\n\tdef main(argv=None):\n\t\t# 存放目录为当前工程目录下的MNIST_data目录\n\t    mnist = input_data.read_data_sets(\"./MNIST_data\", one_hot=True)\n\t    evaluate(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\nMNIST数据集\n\n![AI Live](https://mic-jasontang.github.io/imgs/mnist_data.png)\n\n测试结果\n\n![AI Live](https://mic-jasontang.github.io/imgs/mnist_result.png)\n\n最终正确率可以达到99%\n\n# Bibliography #\n\n1. A guide to convolution arithmetic for deep learning[https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf \"A guide to convolution arithmetic for deep learning\")\n2. TensforFlow 实战Google深度学习框架\n\n我参与举办了一个小团体，主要是技术分享，这篇是第三期的分享内容。下面是我们的公众号:\n\n![AI Live](https://mic-jasontang.github.io/imgs/AI_Live.jpg)\n\n![Join Us](https://mic-jasontang.github.io/imgs/join_us.jpg)\n\n","source":"_posts/深度学习中的卷积和池化.md","raw":"title: 深度学习中的卷积和池化\ndate: 2018-06-21 21:05:19\ncategories:\n- 深度学习\n- 深度学习基础\ntags: \n- 卷积\n- 池化\n- tensorflow\n- LeNet-5\n\n---\n\n# 1. Convolution  #\n> 卷积是什么？\n> \n> 卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。\n\n<!-- more -->\n\n- 卷积动画演示\n\t- 卷积核\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv-kernel.png)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_no_padding.gif)\n\n![卷积动画演示](https://mic-jasontang.github.io/imgs/conv_padding.gif)\n> 在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。\n> \n> - 卷积的目的\n> \n> 卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。\n\n----------\n**卷积分为许多种，下面将会一一介绍。**\n\n- 符号约定\n\n> i: 输入大小表示为i x i\n> \n> k: 卷积核大小表示为k x k\n> \n> s: 步长\n> \n> p: 填充\n>\n> o: 输出表示为o*o\n## 1.1 unit strides ##\n卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。\n### 1.1.1  No zero padding, unit strides ###\n\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.png)\n\n无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1_2.png)\n\n动画演示\n![figure2.1](https://mic-jasontang.github.io/imgs/figure2.1.gif)\n\n### 1.1.2 Zero padding, unit strides ###\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2_2.png)\n\n动画演示\n![figure2.2](https://mic-jasontang.github.io/imgs/figure2.2.gif)\n\n#### 1.1.2.1 Zero padding, unit strides - Half(Same) padding ####\n这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.png)\n\n有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3_2.png)\n\n动画演示\n![figure2.3](https://mic-jasontang.github.io/imgs/figure2.3.gif)\n\n#### 1.1.2.2 Zero padding, unit strides - Full padding ####\n卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.png)\n\n有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4_2.png)\n\n动画演示\n![figure2.4](https://mic-jasontang.github.io/imgs/figure2.4.gif)\n\n## 1.2 Non-unit strides ##\n接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。\n### 1.2.1 No zero padding, non-unit strides ###\n\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.png)\n\n无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：\n\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5_5.png)\n\n其中向下取整是为了避免(i-k)/s是小数的情况。\n\n动画演示\n![figure2.5](https://mic-jasontang.github.io/imgs/figure2.5.gif)\n\n### 1.2.2 Zero padding, non-unit strides ###\n\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.png)\n\n有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6_2.png)\n\n其中向下取整是为了避免(i+2p-k)/s是小数的情况。\n\n动画演示\n![figure2.6](https://mic-jasontang.github.io/imgs/figure2.6.gif)\n\n## 1.3 Convolution as a matrix operation ##\n卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：\n\n1.1.1中的图被表示为如下形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png)\n\n矩阵表示的形式\n\n![figure2.6](https://mic-jasontang.github.io/imgs/conv_as_matrix.png)\n\n我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。\n# 2. Pooling  #\n\n> 池化操作是什么？\n>\n> 池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。\n\n> 池化操作是什么？\n>\n> 池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。\n\n池化分为平均值池化和最大值池化，下面会一一介绍。\n## 2.1 Average Pooling ##\n- 平均值池化可以被表示为\n\n![figure1.5](https://mic-jasontang.github.io/imgs/figure1.5.png)\n\n- 平均值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.5.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n\n## 2.2 Max Pooling ##\n- 最大值池化可以被表示为\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.png)\n\n- 最大值池化的动画演示\n\n![figure1.6](https://mic-jasontang.github.io/imgs/figure1.6.gif)\n\n可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。\n# 3. 3D-Conv  #\n3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。\n\n![figure1.6](https://mic-jasontang.github.io/imgs/conv_3d.jpg)\n\n输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。\n\n为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/ \"博客原地址\")\n\n\n<iframe \n    width=\"100%\" \n    height=\"100%\" \n    src=\"http://cs231n.github.io/assets/conv-demo/index.html\"\n点击查看动画[http://cs231n.github.io/assets/conv-demo/index.html](http://cs231n.github.io/assets/conv-demo/index.html)\n\n    width=\"750\" \n    height=\"720\" \n    src=\"https://cs231n.github.io/assets/conv-demo/index.html\"\n    frameborder=\"0\" \n    allowfullscreen>\n</iframe>\n\n# 4. LeNet-5  #\n\n这里介绍下LeNet-5模型，为了理解前面讲述的各种模型\n\n# Bibliography #\n\n这里介绍下LeNet-5模型，为了理解前面讲述的各种卷积和2种池化，下面具体介绍LeNet-5的每个层。\n\n- C1:\n\t- input: 32x32x1\n\t- conv: 5x5x1\n\t- padding: No Zero\n\t- strides: 1\n\t- output: 28x28x6\n\t- parmas: 5x5x1x6+6 = 156\n\n\t\n> 第一层，卷积层\n> \n> 这一层的输入就是原始的图像像素32*32*1。第一个卷积层过滤器尺寸为5*5，深度为6，不使用全0填充，步长为1。所以这一层的输出：28*28*6，卷积层共有5*5*1*6+6=156个参数。\n\n- S2:\n\t- input: 28x28x6\n\t- pool: 2x2\n\t- padding: No Zero\n\t- strides: 2\n\t- output: 14x14x6\n\t\n> 第二层，池化层\n> \n> 这一层的输入为第一层的输出，是一个28*28*6的节点矩阵。本层采用的过滤器大小为2*2，长和宽的步长均为2，所以本层的输出矩阵大小为14*14*6。\n\n- C3:\n\t- input: 14x14x6\n\t- conv: 5x5x16\n\t- padding: No Zero\n\t- strides: 1\n\t- output: 10x10x16\n\t- params: 5x5x6x16+16=2416\n\t\n> 第三层，卷积层\n> \n> 本层的输入矩阵大小为14*14*6，使用的过滤器大小为5*5，深度为16.本层不使用全0填充，步长为1。本层的输出矩阵大小为10*10*16。本层有5*5*6*16+16=2416个参数。\n\n- S4:\n\t- input: 10x10x16\n\t- pool: 2x2\n\t- padding: No Zero\n\t- strides: 2\n\t- output: 5x5x16\n\t\n> 第四层，池化层\n> \n> 本层的输入矩阵大小10*10*16。本层采用的过滤器大小为2*2，长和宽的步长均为2，所以本层的输出矩阵大小为5*5*16。\n\n- C5:\n\t- input: 5x5x16\n\t- conv: 5x5\n\t- padding: No Zero\n\t- strides: 1\n\t- output: 120\n\t- params: 5x5x16x120 + 120=48120\n\n> 第五层，全连接层(卷积层)\n> \n> 本层的输入矩阵大小为5*5*16，在LeNet-5论文中将这一层成为**卷积层**，但是因为过滤器的大小就是5*5，所以和全连接层没有区别。如果将5*5*16矩阵中的节点拉成一个向量，那么这一层和全连接层就一样了。本层的输出节点个数为120，总共有5*5*16*120+120=48120个参数。\n\n- F6:\n\t- input: 120\n\t- output: 84\n\t- params: 120x84+84 = 10164\n\n\n> 第六层，全连接层\n> \n> 本层的输入节点个数为120个，输出节点个数为84个，总共参数为120*84+84=10164个。\n\n- Output:\n\t- input: 84\n\t- output: 10\n\t- parmas: 84x10 + 10 = 850\n\n\n> 第七层，全连接层\n> \n> 本层的输入节点个数为84个，输出节点个数为10个，总共参数为84*10+10=850\n\n# Code #\n\n- lenet_train.py\n\n训练代码\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/21 16:08\n\t# @update  : 2018年4月19日21:38:34\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : lenet_train.py\n\t# @ToDo    : 使用LeNet-5模型。定义了神经网络的训练过程\n\t\n\timport os\n\t\n\timport tensorflow as tf\n\timport numpy as np\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\n\t\n\t# 配置神经网络的参数\n\tBATCH_SIZE = 100\n\t# LEARNING_REATE_BASE = 0.8  # 0.8的学习率导致准确率不高。明显看出不收敛，准确率跟瞎猜差不多。\n\tLEARNING_REATE_BASE = 0.01  # 降低学习率\n\tLEARNING_RATE_DECAY = 0.99\n\tREGULARAZTION_RATE = 0.0001\n\tTRAING_STEPS = 30000\n\tMOVING_AVERAGE_DECAY = 0.99\n\t# 模型保存的路径和文件名\n\tMODEL_SAVE_PATH = \"./model/\"\n\tMODEL_NAME = \"model.ckpt\"\n\t\n\t\n\tdef train(mnist):\n    # 定义输入输出placeholder\n    x = tf.placeholder(tf.float32,\n                       [BATCH_SIZE,\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.NUM_CHANNELS], name=\"input-x\")\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\n    y = mnist_inference.inference(x, True, regularizer)\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n    variables_average_op = variable_averages.apply(tf.trainable_variables())\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n    # loss = tf.get_collection(\"losses\") 返回一个列表\n    # loss_add = tf.add_n(loss) 将列表元素进行相加\n    loss = cross_entropy_mean + tf.add_n(tf.get_collection(\"losses\"))\n\n    learing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                              global_step,\n                                              mnist.train.num_examples / BATCH_SIZE,\n                                              LEARNING_RATE_DECAY)\n    train_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\n    with tf.control_dependencies([train_step, variables_average_op]):\n        train_op = tf.no_op(name=\"train\")\n\n    # 初始化持久化类\n    saver = tf.train.Saver()\n    with tf.device(\"/gpu:0\"):\n        session_conf = tf.ConfigProto(allow_soft_placement=True)\n        with tf.Session(config=session_conf) as sess:\n            tf.global_variables_initializer().run()\n\n            for i in range(TRAING_STEPS):\n                xs, ys = mnist.train.next_batch(BATCH_SIZE)\n                reshaped_xs = np.reshape(xs, [BATCH_SIZE,\n                                              mnist_inference.IMAGE_SIZE,\n                                              mnist_inference.IMAGE_SIZE,\n                                              mnist_inference.NUM_CHANNELS])\n                # print(type(xs))\n                # print(type(reshaped_xs))\n                _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n\n                if i % 1000 == 0:\n                    print(\"After %d training step(s), loss on training batch is %g.\" % (step, loss_value))\n\n                    saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\n\tdef main(argv=None):\n\t\t# 存放目录为当前工程目录下的MNIST_data目录\n\t    mnist = input_data.read_data_sets(\"./MNIST_data\", one_hot=True)\n\t    train(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\t    \n\n- lenet_inference\n\n计算代码\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018/2/20 19:43\n\t# @update  : 2018年4月19日21:38:34\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : lent_inference.py\n\t# @ToDo    : 使用LeNet-5模型。定义了前向传播的过程及神经网络的参数\n\t\n\t\n\timport tensorflow as tf\n\t\n\t# 定义神经网络结构相关的参数\n\tINPUT_NODE = 784\n\tOUTPUT_NODE = 10\n\t\n\tIMAGE_SIZE = 28\n\tNUM_CHANNELS = 1\n\tNUM_LABELS = 10\n\t\n\t# 第一层卷积层的尺寸和深度\n\tCONV1_DEEP = 32\n\tCONV1_SIZE = 5\n\t# 第二层卷积层的尺寸和深度\n\tCONV2_DEEP = 64\n\tCONV2_SIZE = 5\n\t# 全连接层的结点个数\n\tFC_SIZE = 512\n\t\n\t# 卷积神经网络的前向传播过程\n\t# 添加一个新的参数train，用于区分训练过程和测试过程。\n\t# 在这个程序中将用到dropout方法，dropout可以进一步提升模型可靠性并防止\n\t# 过拟合。dropout过程只在训练时使用。\n\tdef inference(input_tensor, train, regularizer, dropout=0.5):\n\t    # 声明第一层神经网络的变量并完成前向传播过程\n\t    # 和标准的LeNet-5模型不大一样，这里定义的卷积层输入为28*28*1的原始MNIST图片像素。\n\t    # 因为卷积层使用了全0填充，所以输出为28*28*32的矩阵。\n\t    with tf.variable_scope(\"layer1-conv1\"):\n\t        conv1_weights = tf.get_variable(\"weights\", [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n\t                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        conv1_biases = tf.get_variable(\"biases\", [CONV1_DEEP], initializer=tf.constant_initializer(0.1))\n\t        # 使用变长为5，深度为32的过滤器，过滤器移动的步长为1，且使用全0填充。\n\t        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n\t        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n\t\n\t    # 实现第二层池化层的前向传播过程，这里选择最大池化层，池化层过滤器的变长为2\n\t    # 使用全0填充且移动的步长为2，这一层的输入是上一层的输出，也就是28*28*32的矩阵。\n\t    # 输出为14*14*32的矩阵\n\t    with tf.variable_scope(\"layer2-pool1\"):\n\t        pool1 = tf.nn.max_pool(relu1,\n\t                               ksize=[1, 2, 2, 1],\n\t                               strides=[1, 2, 2, 1],\n\t                               padding=\"SAME\")\n\t\n\t    # 声明第三层卷积层的变量并实现前向传播过程，这一层的输入为14*14*32的矩阵。\n\t    # 输出为14*14*64的矩阵。\n\t    with tf.variable_scope(\"layer3-conv2\"):\n\t        conv2_weights = tf.get_variable(\"weight\",\n\t                                        [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n\t                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        conv2_biases = tf.get_variable(\"bias\", [CONV2_DEEP], initializer=tf.constant_initializer(0.1))\n\t\n\t        # 使用边长为5，深度为64的过滤器，过滤器移动的步长为1，且使用全0填充。\n\t        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n\t        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n\t\n\t    # 实现第四层池化层的前向传播过程。这一层和第二层的记过是一样的，这一层的输入为14*14*64的矩阵，\n\t    # 输出位7*7*64的矩阵。\n\t    with tf.name_scope(\"layer4-pool2\"):\n\t        pool2 = tf.nn.max_pool(relu2,\n\t                               ksize=[1, 2, 2, 1],\n\t                               strides=[1, 2, 2, 1],\n\t                               padding=\"SAME\")\n\t\n\t    # 将第四层池化层的输出转化为第五层全连接层的输入格式。\n\t    # 第四层的输出为7*7*64的矩阵，然后第五层全连接层需要的输入格式为向量\n\t    # 所以在这里需要将这个7*7*64的矩阵拉直成一个向量。pool2.get_shape函数可以得到\n\t    # 第四层输出矩阵的维度而不需要手工计算。注意因为每一层神经网络的输入输出都为一个batch的矩阵，\n\t    # 所以这里得到的维度也包含了一个batch的数据的个数。\n\t    pool_shape = pool2.get_shape().as_list()\n\t\n\t    # 计算将矩阵拉直成向量之后的长度，这个长度就是矩阵长宽及深度的乘积。\n\t    # 这里pool_shape[0]为一个batch中数据的个数\n\t    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n\t\n\t    # 通过tf.reshape函数将第四层的输出变成一个batch的向量\n\t    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n\t\n\t    # 声明第五层全连接层的变量并实现前向传播过程，这一层的输入是拉直之后的一组向量，\n\t    # 向量长度为3136，输出是一组长度为512的向量。这一层和之前在重构MNIST数据集的代码基本一致，\n\t    # 唯一的区别就是引入了dropout的概念。dropout在训练时会随机将部分结点的输出改为0。\n\t    # dropout可以避免过拟合问题，从而使得模型在测试数据上的效果更好。\n\t    # dropout一般只在全连接层而不是卷积层或者池化层使用。\n\t    with tf.variable_scope(\"layer5-fc1\"):\n\t        fc1_weights = tf.get_variable(\"weight\",\n\t                                      [nodes, FC_SIZE],\n\t                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        # 只要全连接层的权重需要加入正则化\n\t        if regularizer is not None:\n\t            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t            tf.add_to_collection(\"losses\", regularizer(fc1_weights))\n\t        fc1_biases = tf.get_variable(\"bias\",\n\t                                     [FC_SIZE],\n\t                                     initializer=tf.constant_initializer(0.1))\n\t        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n\t        if train:\n\t            fc1 = tf.nn.dropout(fc1, dropout)\n\t\n\t    # 声明第六层全连接层的变量并实现前向传播过程。\n\t    # 这一层的输入为一组长度为512的向量，输出为一组长度为10的向量。\n\t    # 这一层的输出通过Softmax之后就得到了最后的分类结果。\n\t    with tf.variable_scope(\"layer6-fc2\"):\n\t        fc2_weights = tf.get_variable(\"weight\",\n\t                                      [FC_SIZE, NUM_LABELS],\n\t                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n\t        if regularizer is not None:\n\t            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n\t            tf.add_to_collection(\"losses\", regularizer(fc2_weights))\n\t        fc2_biases = tf.get_variable(\"bias\",\n\t                                     [NUM_LABELS],\n\t                                     initializer=tf.constant_initializer(0.1))\n\t        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n\t\n\t    # 返回第六层的输出\n\t    return logit\n\n\n- lenet_eval.py\n\n测试代码\n\n\t#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# @Time    : 2018年4月19日21:38:34\n\t# @Author  : Jasontang\n\t# @Site    : \n\t# @File    : mnist_eval.py\n\t# @ToDo    : 测试过程\n\t\n\t\n\timport time\n\timport tensorflow as tf\n\timport numpy as np\n\tfrom tensorflow.examples.tutorials.mnist import input_data\n\t\n\timport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\n\timport neural_network_learning.cnn.lenet.mnist_train as mnist_train\n\t\n\t# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\n\tEVAL_INTERVAL_SECS = 10\n\t\n\t\n\tdef evaluate(mnist):\n\t    x = tf.placeholder(tf.float32,\n\t                       [mnist.validation.images.shape[0],\n\t                        mnist_inference.IMAGE_SIZE,\n\t                        mnist_inference.IMAGE_SIZE,\n\t                        mnist_inference.NUM_CHANNELS], name=\"input-x\")\n\t    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=\"input-y\")\n\t\n\t    y = mnist_inference.inference(x, False, None)\n\t\n\t    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n\t    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\t\n\t    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n\t    variables_to_restore = variable_averages.variables_to_restore()\n\t    saver = tf.train.Saver(variables_to_restore)\n\t\n\t    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n\t    stop_count = 0\n\t    while True:\n\t        with tf.Session() as sess:\n\t            # tf.train.get_checkpoint_state函数会通过checkpoint文件自动找刀目录中最新模型的文件名\n\t            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n\t            # 停止条件 #\n\t            stop_count += 1\n\t            if stop_count > 5:\n\t                return\n\t            # 停止条件 #\n\t            if ckpt and ckpt.model_checkpoint_path:\n\t                # 加载模型\n\t                saver.restore(sess, ckpt.model_checkpoint_path)\n\t                # 通过文件名得到模型保存时迭代的轮数\n\t                # 输出./model/model.ckpt-29001\n\t                print(ckpt.model_checkpoint_path)\n\t                global_step = ckpt.model_checkpoint_path.split(\"/\")[-1].split(\"-\")[-1]\n\t                validate_feed = {x: mnist.validation.images,\n\t                                 y_: mnist.validation.labels}\n\t\n\t                # print(validate_feed[x])\n\t                reshaped_x = np.reshape(validate_feed[x],\n\t                                        [validate_feed[x].shape[0],\n\t                                         mnist_inference.IMAGE_SIZE,\n\t                                         mnist_inference.IMAGE_SIZE,\n\t                                         mnist_inference.NUM_CHANNELS])\n\t                validate_feed[x] = reshaped_x\n\t                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n\t                print(\"After %s training step(s), validation accuracy is %g\" % (global_step, accuracy_score))\n\t            else:\n\t                print(\"No checkpoint file found\")\n\t                return\n\t        time.sleep(EVAL_INTERVAL_SECS)\n\t\n\t\n\tdef main(argv=None):\n\t\t# 存放目录为当前工程目录下的MNIST_data目录\n\t    mnist = input_data.read_data_sets(\"./MNIST_data\", one_hot=True)\n\t    evaluate(mnist)\n\t\n\t\n\tif __name__ == '__main__':\n\t    tf.app.run()\n\nMNIST数据集\n\n![AI Live](https://mic-jasontang.github.io/imgs/mnist_data.png)\n\n测试结果\n\n![AI Live](https://mic-jasontang.github.io/imgs/mnist_result.png)\n\n最终正确率可以达到99%\n\n# Bibliography #\n\n1. A guide to convolution arithmetic for deep learning[https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf \"A guide to convolution arithmetic for deep learning\")\n2. TensforFlow 实战Google深度学习框架\n\n我参与举办了一个小团体，主要是技术分享，这篇是第三期的分享内容。下面是我们的公众号:\n\n![AI Live](https://mic-jasontang.github.io/imgs/AI_Live.jpg)\n\n![Join Us](https://mic-jasontang.github.io/imgs/join_us.jpg)\n\n","slug":"深度学习中的卷积和池化","published":1,"updated":"2019-05-08T10:50:25.483Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjvf4dqs2004glnvv3phyw3c3","content":"<h1 id=\"1-Convolution\"><a href=\"#1-Convolution\" class=\"headerlink\" title=\"1. Convolution\"></a>1. Convolution</h1><blockquote>\n<p>卷积是什么？</p>\n<p>卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。</p>\n</blockquote>\n<a id=\"more\"></a>\n<ul>\n<li>卷积动画演示<ul>\n<li>卷积核<br><img src=\"https://mic-jasontang.github.io/imgs/conv-kernel.png\" alt=\"卷积动画演示\"></li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_no_padding.gif\" alt=\"卷积动画演示\"></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_padding.gif\" alt=\"卷积动画演示\"></p>\n<blockquote>\n<p>在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。</p>\n<ul>\n<li>卷积的目的</li>\n</ul>\n<p>卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。</p>\n</blockquote>\n<hr>\n<p><strong>卷积分为许多种，下面将会一一介绍。</strong></p>\n<ul>\n<li>符号约定</li>\n</ul>\n<blockquote>\n<p>i: 输入大小表示为i x i</p>\n<p>k: 卷积核大小表示为k x k</p>\n<p>s: 步长</p>\n<p>p: 填充</p>\n<p>o: 输出表示为o*o</p>\n</blockquote>\n<h2 id=\"1-1-unit-strides\"><a href=\"#1-1-unit-strides\" class=\"headerlink\" title=\"1.1 unit strides\"></a>1.1 unit strides</h2><p>卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。</p>\n<h3 id=\"1-1-1-No-zero-padding-unit-strides\"><a href=\"#1-1-1-No-zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.1  No zero padding, unit strides\"></a>1.1.1  No zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.png\" alt=\"figure2.1\"></p>\n<p>无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1_2.png\" alt=\"figure2.1\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.gif\" alt=\"figure2.1\"></p>\n<h3 id=\"1-1-2-Zero-padding-unit-strides\"><a href=\"#1-1-2-Zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.2 Zero padding, unit strides\"></a>1.1.2 Zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.png\" alt=\"figure2.2\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2_2.png\" alt=\"figure2.2\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.gif\" alt=\"figure2.2\"></p>\n<h4 id=\"1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\"><a href=\"#1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\" class=\"headerlink\" title=\"1.1.2.1 Zero padding, unit strides - Half(Same) padding\"></a>1.1.2.1 Zero padding, unit strides - Half(Same) padding</h4><p>这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.png\" alt=\"figure2.3\"></p>\n<p>有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3_2.png\" alt=\"figure2.3\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.gif\" alt=\"figure2.3\"></p>\n<h4 id=\"1-1-2-2-Zero-padding-unit-strides-Full-padding\"><a href=\"#1-1-2-2-Zero-padding-unit-strides-Full-padding\" class=\"headerlink\" title=\"1.1.2.2 Zero padding, unit strides - Full padding\"></a>1.1.2.2 Zero padding, unit strides - Full padding</h4><p>卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.png\" alt=\"figure2.4\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4_2.png\" alt=\"figure2.4\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.gif\" alt=\"figure2.4\"></p>\n<h2 id=\"1-2-Non-unit-strides\"><a href=\"#1-2-Non-unit-strides\" class=\"headerlink\" title=\"1.2 Non-unit strides\"></a>1.2 Non-unit strides</h2><p>接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。</p>\n<h3 id=\"1-2-1-No-zero-padding-non-unit-strides\"><a href=\"#1-2-1-No-zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.1 No zero padding, non-unit strides\"></a>1.2.1 No zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.png\" alt=\"figure2.5\"></p>\n<p>无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure2.5_5.png\" alt=\"figure2.5\"></p>\n<p>其中向下取整是为了避免(i-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.gif\" alt=\"figure2.5\"></p>\n<h3 id=\"1-2-2-Zero-padding-non-unit-strides\"><a href=\"#1-2-2-Zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.2 Zero padding, non-unit strides\"></a>1.2.2 Zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.png\" alt=\"figure2.6\"></p>\n<p>有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6_2.png\" alt=\"figure2.6\"></p>\n<p>其中向下取整是为了避免(i+2p-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.gif\" alt=\"figure2.6\"></p>\n<h2 id=\"1-3-Convolution-as-a-matrix-operation\"><a href=\"#1-3-Convolution-as-a-matrix-operation\" class=\"headerlink\" title=\"1.3 Convolution as a matrix operation\"></a>1.3 Convolution as a matrix operation</h2><p>卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：</p>\n<p>1.1.1中的图被表示为如下形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png\" alt=\"figure2.6\"></p>\n<p>矩阵表示的形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix.png\" alt=\"figure2.6\"></p>\n<p>我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。</p>\n<h1 id=\"2-Pooling\"><a href=\"#2-Pooling\" class=\"headerlink\" title=\"2. Pooling\"></a>2. Pooling</h1><blockquote>\n<p>池化操作是什么？</p>\n<p>池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。</p>\n</blockquote>\n<blockquote>\n<p>池化操作是什么？</p>\n<p>池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。</p>\n</blockquote>\n<p>池化分为平均值池化和最大值池化，下面会一一介绍。</p>\n<h2 id=\"2-1-Average-Pooling\"><a href=\"#2-1-Average-Pooling\" class=\"headerlink\" title=\"2.1 Average Pooling\"></a>2.1 Average Pooling</h2><ul>\n<li>平均值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.png\" alt=\"figure1.5\"></p>\n<ul>\n<li>平均值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h2 id=\"2-2-Max-Pooling\"><a href=\"#2-2-Max-Pooling\" class=\"headerlink\" title=\"2.2 Max Pooling\"></a>2.2 Max Pooling</h2><ul>\n<li>最大值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.png\" alt=\"figure1.6\"></p>\n<ul>\n<li>最大值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h1 id=\"3-3D-Conv\"><a href=\"#3-3D-Conv\" class=\"headerlink\" title=\"3. 3D-Conv\"></a>3. 3D-Conv</h1><p>3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_3d.jpg\" alt=\"figure1.6\"></p>\n<p>输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。</p>\n<p>为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。<a href=\"http://cs231n.github.io/convolutional-networks/\" title=\"博客原地址\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/convolutional-networks/</a></p>\n&lt;iframe<br>    width=”100%”<br>    height=”100%”<br>    src=”<a href=\"http://cs231n.github.io/assets/conv-demo/index.html&quot;\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/assets/conv-demo/index.html&quot;</a><br>点击查看动画<a href=\"http://cs231n.github.io/assets/conv-demo/index.html\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/assets/conv-demo/index.html</a><br><br>    width=”750”<br>    height=”720”<br>    src=”<a href=\"https://cs231n.github.io/assets/conv-demo/index.html&quot;\" target=\"_blank\" rel=\"noopener\">https://cs231n.github.io/assets/conv-demo/index.html&quot;</a><br>    frameborder=”0”<br>    allowfullscreen&gt;<br>\n\n<h1 id=\"4-LeNet-5\"><a href=\"#4-LeNet-5\" class=\"headerlink\" title=\"4. LeNet-5\"></a>4. LeNet-5</h1><p>这里介绍下LeNet-5模型，为了理解前面讲述的各种模型</p>\n<h1 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h1><p>这里介绍下LeNet-5模型，为了理解前面讲述的各种卷积和2种池化，下面具体介绍LeNet-5的每个层。</p>\n<ul>\n<li>C1:<ul>\n<li>input: 32x32x1</li>\n<li>conv: 5x5x1</li>\n<li>padding: No Zero</li>\n<li>strides: 1</li>\n<li>output: 28x28x6</li>\n<li>parmas: 5x5x1x6+6 = 156</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第一层，卷积层</p>\n<p>这一层的输入就是原始的图像像素32<em>32</em>1。第一个卷积层过滤器尺寸为5<em>5，深度为6，不使用全0填充，步长为1。所以这一层的输出：28</em>28<em>6，卷积层共有5</em>5<em>1</em>6+6=156个参数。</p>\n</blockquote>\n<ul>\n<li>S2:<ul>\n<li>input: 28x28x6</li>\n<li>pool: 2x2</li>\n<li>padding: No Zero</li>\n<li>strides: 2</li>\n<li>output: 14x14x6</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第二层，池化层</p>\n<p>这一层的输入为第一层的输出，是一个28<em>28</em>6的节点矩阵。本层采用的过滤器大小为2<em>2，长和宽的步长均为2，所以本层的输出矩阵大小为14</em>14*6。</p>\n</blockquote>\n<ul>\n<li>C3:<ul>\n<li>input: 14x14x6</li>\n<li>conv: 5x5x16</li>\n<li>padding: No Zero</li>\n<li>strides: 1</li>\n<li>output: 10x10x16</li>\n<li>params: 5x5x6x16+16=2416</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第三层，卷积层</p>\n<p>本层的输入矩阵大小为14<em>14</em>6，使用的过滤器大小为5<em>5，深度为16.本层不使用全0填充，步长为1。本层的输出矩阵大小为10</em>10<em>16。本层有5</em>5<em>6</em>16+16=2416个参数。</p>\n</blockquote>\n<ul>\n<li>S4:<ul>\n<li>input: 10x10x16</li>\n<li>pool: 2x2</li>\n<li>padding: No Zero</li>\n<li>strides: 2</li>\n<li>output: 5x5x16</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第四层，池化层</p>\n<p>本层的输入矩阵大小10<em>10</em>16。本层采用的过滤器大小为2<em>2，长和宽的步长均为2，所以本层的输出矩阵大小为5</em>5*16。</p>\n</blockquote>\n<ul>\n<li>C5:<ul>\n<li>input: 5x5x16</li>\n<li>conv: 5x5</li>\n<li>padding: No Zero</li>\n<li>strides: 1</li>\n<li>output: 120</li>\n<li>params: 5x5x16x120 + 120=48120</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第五层，全连接层(卷积层)</p>\n<p>本层的输入矩阵大小为5<em>5</em>16，在LeNet-5论文中将这一层成为<strong>卷积层</strong>，但是因为过滤器的大小就是5<em>5，所以和全连接层没有区别。如果将5</em>5<em>16矩阵中的节点拉成一个向量，那么这一层和全连接层就一样了。本层的输出节点个数为120，总共有5</em>5<em>16</em>120+120=48120个参数。</p>\n</blockquote>\n<ul>\n<li>F6:<ul>\n<li>input: 120</li>\n<li>output: 84</li>\n<li>params: 120x84+84 = 10164</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第六层，全连接层</p>\n<p>本层的输入节点个数为120个，输出节点个数为84个，总共参数为120*84+84=10164个。</p>\n</blockquote>\n<ul>\n<li>Output:<ul>\n<li>input: 84</li>\n<li>output: 10</li>\n<li>parmas: 84x10 + 10 = 850</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第七层，全连接层</p>\n<p>本层的输入节点个数为84个，输出节点个数为10个，总共参数为84*10+10=850</p>\n</blockquote>\n<h1 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h1><ul>\n<li>lenet_train.py</li>\n</ul>\n<p>训练代码</p>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:08\n# @update  : 2018年4月19日21:38:34\n# @Author  : Jasontang\n# @Site    : \n# @File    : lenet_train.py\n# @ToDo    : 使用LeNet-5模型。定义了神经网络的训练过程\n\nimport os\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\n\n# 配置神经网络的参数\nBATCH_SIZE = 100\n# LEARNING_REATE_BASE = 0.8  # 0.8的学习率导致准确率不高。明显看出不收敛，准确率跟瞎猜差不多。\nLEARNING_REATE_BASE = 0.01  # 降低学习率\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAING_STEPS = 30000\nMOVING_AVERAGE_DECAY = 0.99\n# 模型保存的路径和文件名\nMODEL_SAVE_PATH = &quot;./model/&quot;\nMODEL_NAME = &quot;model.ckpt&quot;\n\n\ndef train(mnist):\n# 定义输入输出placeholder\nx = tf.placeholder(tf.float32,\n                   [BATCH_SIZE,\n                    mnist_inference.IMAGE_SIZE,\n                    mnist_inference.IMAGE_SIZE,\n                    mnist_inference.NUM_CHANNELS], name=&quot;input-x&quot;)\ny_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\nregularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\ny = mnist_inference.inference(x, True, regularizer)\nglobal_step = tf.Variable(0, trainable=False)\n\nvariable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\nvariables_average_op = variable_averages.apply(tf.trainable_variables())\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n# loss = tf.get_collection(&quot;losses&quot;) 返回一个列表\n# loss_add = tf.add_n(loss) 将列表元素进行相加\nloss = cross_entropy_mean + tf.add_n(tf.get_collection(&quot;losses&quot;))\n\nlearing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                          global_step,\n                                          mnist.train.num_examples / BATCH_SIZE,\n                                          LEARNING_RATE_DECAY)\ntrain_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\nwith tf.control_dependencies([train_step, variables_average_op]):\n    train_op = tf.no_op(name=&quot;train&quot;)\n\n# 初始化持久化类\nsaver = tf.train.Saver()\nwith tf.device(&quot;/gpu:0&quot;):\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Session(config=session_conf) as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            reshaped_xs = np.reshape(xs, [BATCH_SIZE,\n                                          mnist_inference.IMAGE_SIZE,\n                                          mnist_inference.IMAGE_SIZE,\n                                          mnist_inference.NUM_CHANNELS])\n            # print(type(xs))\n            # print(type(reshaped_xs))\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n\n            if i % 1000 == 0:\n                print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (step, loss_value))\n\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    # 存放目录为当前工程目录下的MNIST_data目录\n    mnist = input_data.read_data_sets(&quot;./MNIST_data&quot;, one_hot=True)\n    train(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><ul>\n<li>lenet_inference</li>\n</ul>\n<p>计算代码</p>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/20 19:43\n# @update  : 2018年4月19日21:38:34\n# @Author  : Jasontang\n# @Site    : \n# @File    : lent_inference.py\n# @ToDo    : 使用LeNet-5模型。定义了前向传播的过程及神经网络的参数\n\n\nimport tensorflow as tf\n\n# 定义神经网络结构相关的参数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\n\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nNUM_LABELS = 10\n\n# 第一层卷积层的尺寸和深度\nCONV1_DEEP = 32\nCONV1_SIZE = 5\n# 第二层卷积层的尺寸和深度\nCONV2_DEEP = 64\nCONV2_SIZE = 5\n# 全连接层的结点个数\nFC_SIZE = 512\n\n# 卷积神经网络的前向传播过程\n# 添加一个新的参数train，用于区分训练过程和测试过程。\n# 在这个程序中将用到dropout方法，dropout可以进一步提升模型可靠性并防止\n# 过拟合。dropout过程只在训练时使用。\ndef inference(input_tensor, train, regularizer, dropout=0.5):\n    # 声明第一层神经网络的变量并完成前向传播过程\n    # 和标准的LeNet-5模型不大一样，这里定义的卷积层输入为28*28*1的原始MNIST图片像素。\n    # 因为卷积层使用了全0填充，所以输出为28*28*32的矩阵。\n    with tf.variable_scope(&quot;layer1-conv1&quot;):\n        conv1_weights = tf.get_variable(&quot;weights&quot;, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv1_biases = tf.get_variable(&quot;biases&quot;, [CONV1_DEEP], initializer=tf.constant_initializer(0.1))\n        # 使用变长为5，深度为32的过滤器，过滤器移动的步长为1，且使用全0填充。\n        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;)\n        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n\n    # 实现第二层池化层的前向传播过程，这里选择最大池化层，池化层过滤器的变长为2\n    # 使用全0填充且移动的步长为2，这一层的输入是上一层的输出，也就是28*28*32的矩阵。\n    # 输出为14*14*32的矩阵\n    with tf.variable_scope(&quot;layer2-pool1&quot;):\n        pool1 = tf.nn.max_pool(relu1,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=&quot;SAME&quot;)\n\n    # 声明第三层卷积层的变量并实现前向传播过程，这一层的输入为14*14*32的矩阵。\n    # 输出为14*14*64的矩阵。\n    with tf.variable_scope(&quot;layer3-conv2&quot;):\n        conv2_weights = tf.get_variable(&quot;weight&quot;,\n                                        [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv2_biases = tf.get_variable(&quot;bias&quot;, [CONV2_DEEP], initializer=tf.constant_initializer(0.1))\n\n        # 使用边长为5，深度为64的过滤器，过滤器移动的步长为1，且使用全0填充。\n        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;)\n        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n\n    # 实现第四层池化层的前向传播过程。这一层和第二层的记过是一样的，这一层的输入为14*14*64的矩阵，\n    # 输出位7*7*64的矩阵。\n    with tf.name_scope(&quot;layer4-pool2&quot;):\n        pool2 = tf.nn.max_pool(relu2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=&quot;SAME&quot;)\n\n    # 将第四层池化层的输出转化为第五层全连接层的输入格式。\n    # 第四层的输出为7*7*64的矩阵，然后第五层全连接层需要的输入格式为向量\n    # 所以在这里需要将这个7*7*64的矩阵拉直成一个向量。pool2.get_shape函数可以得到\n    # 第四层输出矩阵的维度而不需要手工计算。注意因为每一层神经网络的输入输出都为一个batch的矩阵，\n    # 所以这里得到的维度也包含了一个batch的数据的个数。\n    pool_shape = pool2.get_shape().as_list()\n\n    # 计算将矩阵拉直成向量之后的长度，这个长度就是矩阵长宽及深度的乘积。\n    # 这里pool_shape[0]为一个batch中数据的个数\n    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n\n    # 通过tf.reshape函数将第四层的输出变成一个batch的向量\n    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n\n    # 声明第五层全连接层的变量并实现前向传播过程，这一层的输入是拉直之后的一组向量，\n    # 向量长度为3136，输出是一组长度为512的向量。这一层和之前在重构MNIST数据集的代码基本一致，\n    # 唯一的区别就是引入了dropout的概念。dropout在训练时会随机将部分结点的输出改为0。\n    # dropout可以避免过拟合问题，从而使得模型在测试数据上的效果更好。\n    # dropout一般只在全连接层而不是卷积层或者池化层使用。\n    with tf.variable_scope(&quot;layer5-fc1&quot;):\n        fc1_weights = tf.get_variable(&quot;weight&quot;,\n                                      [nodes, FC_SIZE],\n                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # 只要全连接层的权重需要加入正则化\n        if regularizer is not None:\n            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n            tf.add_to_collection(&quot;losses&quot;, regularizer(fc1_weights))\n        fc1_biases = tf.get_variable(&quot;bias&quot;,\n                                     [FC_SIZE],\n                                     initializer=tf.constant_initializer(0.1))\n        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n        if train:\n            fc1 = tf.nn.dropout(fc1, dropout)\n\n    # 声明第六层全连接层的变量并实现前向传播过程。\n    # 这一层的输入为一组长度为512的向量，输出为一组长度为10的向量。\n    # 这一层的输出通过Softmax之后就得到了最后的分类结果。\n    with tf.variable_scope(&quot;layer6-fc2&quot;):\n        fc2_weights = tf.get_variable(&quot;weight&quot;,\n                                      [FC_SIZE, NUM_LABELS],\n                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n        if regularizer is not None:\n            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n            tf.add_to_collection(&quot;losses&quot;, regularizer(fc2_weights))\n        fc2_biases = tf.get_variable(&quot;bias&quot;,\n                                     [NUM_LABELS],\n                                     initializer=tf.constant_initializer(0.1))\n        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n\n    # 返回第六层的输出\n    return logit\n</code></pre><ul>\n<li>lenet_eval.py</li>\n</ul>\n<p>测试代码</p>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018年4月19日21:38:34\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_eval.py\n# @ToDo    : 测试过程\n\n\nimport time\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\nimport neural_network_learning.cnn.lenet.mnist_train as mnist_train\n\n# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\nEVAL_INTERVAL_SECS = 10\n\n\ndef evaluate(mnist):\n    x = tf.placeholder(tf.float32,\n                       [mnist.validation.images.shape[0],\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.NUM_CHANNELS], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    y = mnist_inference.inference(x, False, None)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n    stop_count = 0\n    while True:\n        with tf.Session() as sess:\n            # tf.train.get_checkpoint_state函数会通过checkpoint文件自动找刀目录中最新模型的文件名\n            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n            # 停止条件 #\n            stop_count += 1\n            if stop_count &gt; 5:\n                return\n            # 停止条件 #\n            if ckpt and ckpt.model_checkpoint_path:\n                # 加载模型\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                # 通过文件名得到模型保存时迭代的轮数\n                # 输出./model/model.ckpt-29001\n                print(ckpt.model_checkpoint_path)\n                global_step = ckpt.model_checkpoint_path.split(&quot;/&quot;)[-1].split(&quot;-&quot;)[-1]\n                validate_feed = {x: mnist.validation.images,\n                                 y_: mnist.validation.labels}\n\n                # print(validate_feed[x])\n                reshaped_x = np.reshape(validate_feed[x],\n                                        [validate_feed[x].shape[0],\n                                         mnist_inference.IMAGE_SIZE,\n                                         mnist_inference.IMAGE_SIZE,\n                                         mnist_inference.NUM_CHANNELS])\n                validate_feed[x] = reshaped_x\n                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n                print(&quot;After %s training step(s), validation accuracy is %g&quot; % (global_step, accuracy_score))\n            else:\n                print(&quot;No checkpoint file found&quot;)\n                return\n        time.sleep(EVAL_INTERVAL_SECS)\n\n\ndef main(argv=None):\n    # 存放目录为当前工程目录下的MNIST_data目录\n    mnist = input_data.read_data_sets(&quot;./MNIST_data&quot;, one_hot=True)\n    evaluate(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><p>MNIST数据集</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/mnist_data.png\" alt=\"AI Live\"></p>\n<p>测试结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/mnist_result.png\" alt=\"AI Live\"></p>\n<p>最终正确率可以达到99%</p>\n<h1 id=\"Bibliography-1\"><a href=\"#Bibliography-1\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h1><ol>\n<li>A guide to convolution arithmetic for deep learning<a href=\"https://arxiv.org/pdf/1603.07285.pdf\" title=\"A guide to convolution arithmetic for deep learning\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1603.07285.pdf</a></li>\n<li>TensforFlow 实战Google深度学习框架</li>\n</ol>\n<p>我参与举办了一个小团体，主要是技术分享，这篇是第三期的分享内容。下面是我们的公众号:</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/AI_Live.jpg\" alt=\"AI Live\"></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/join_us.jpg\" alt=\"Join Us\"></p>\n","site":{"data":{"about":{"avatar":"https://mic-jasontang.github.io/imgs/avatar.jpg","name":"tech.radish","tag":"python/Java/ML/CV","desc":"行走在边缘的coder","skills":{"ptyhon":5,"Java":6,"invisible-split-line-1":-1,"ML":4},"projects":[{"name":"合金战争(Java Swing + MySql + Socket)","image":"https://mic-jasontang.github.io/imgs/game.png","tags":["Java","游戏开发"],"description":"合金战争是一款使用JavaSwing作为界面，MySQL作为数据库服务器，使用Socket通信的网络版RPG游戏","link_text":"合金战争","link":null},{"name":"iclass智能课堂助手","image":"https://mic-jasontang.github.io/imgs/iclass.png","description":"iclass是一款实用SSM框架开发的轻量级课堂辅助教学系统，旨在加强和方便师生之间的交流合作，提高教学效率。拥有web端和安卓端（本人完成web端和安卓后端的开发）","tags":["毕业设计","SSM框架"],"link_text":"Github地址","link":"https://github.com/Mic-JasonTang/iclass"}],"reward":["https://mic-jasontang.github.io/imgs/alipay-rewardcode.jpg","https://mic-jasontang.github.io/imgs/wetchat-rewardcode.jpg"]},"hint":{"new":{"selector":[".menu-reading"]}},"link":{"social":{"github":"https://github.com/mic-jasontang"},"extern":{"Github地址":"https://github.com/mic-jasontang"}},"reading":{"define":{"readed":"已读","reading":"在读","wanted":"想读"},"contents":{"readed":[{"title":"嫌疑人X的献身","cover":"https://img3.doubanio.com/lpic/s3254244.jpg","review":"百年一遇的数学天才石神，每天唯一的乐趣，便是去固定的便当店买午餐，只为看一眼在便当店做事的邻居靖子。","score":"8.9","doubanLink":"https://book.douban.com/subject/3211779/"},{"title":"Python核心编程（第二版）","cover":"https://img3.doubanio.com/lpic/s3140466.jpg","review":"本书是Python开发者的完全指南——针对 Python 2.5全面升级","score":"7.7","doubanLink":"https://book.douban.com/subject/3112503/"},{"title":"程序员代码面试指南：IT名企算法与数据结构题目最优解","cover":"https://img3.doubanio.com/lpic/s28313721.jpg","review":"这是一本程序员面试宝典！书中对IT名企代码面试各类题目的最优解进行了总结，并提供了相关代码实现。","score":"8.8","doubanLink":"https://book.douban.com/subject/26638586/"},{"title":"机器学习实战","cover":"https://img3.doubanio.com/lpic/s26696371.jpg","review":"全书通过精心编排的实例，切入日常工作任务，摒弃学术化语言，利用高效的可复用Python代码来阐释如何处理统计数据，进行数据分析及可视化。通过各种实例，读者可从中学会机器学习的核心算法，并能将其运用于一些策略性任务中，如分类、预测、推荐。另外，还可用它们来实现一些更高级的功能，如汇总和简化等。","score":"8.2","doubanLink":"https://book.douban.com/subject/24703171/"},{"title":"TensorFlow实战","cover":"https://img3.doubanio.com/lpic/s29343414.jpg","review":"《TensorFlow实战》希望能帮读者快速入门TensorFlow和深度学习，在工业界或者研究中快速地将想法落地为可实践的模型。","score":"7.3","doubanLink":"https://book.douban.com/subject/26974266/"}],"reading":[{"title":"深度学习","cover":"https://img1.doubanio.com/lpic/s29518349.jpg","review":"《深度学习》适合各类读者阅读，包括相关专业的大学生或研究生，以及不具有机器学习或统计背景、但是想要快速补充深度学习知识，以便在实际产品或平台中应用的软件工程师。","score":"8.7","doubanLink":"https://book.douban.com/subject/27087503/"},{"title":"Tensorflow：实战Google深度学习框架","cover":"https://img3.doubanio.com/lpic/s29349250.jpg","review":"《Tensorflow实战》包含了深度学习的入门知识和大量实践经验，是走进这个最新、最火的人工智能领域的首选参考书。","score":"8.2","doubanLink":"https://book.douban.com/subject/26976457/"},{"title":"机器学习","cover":"https://img1.doubanio.com/lpic/s28735609.jpg","review":"本书作为该领域的入门教材，在内容上尽可能涵盖机器学习基础知识的各方面。 为了使尽可能多的读者通过本书对机器学习有所了解, 作者试图尽可能少地使用数学知识. 然而, 少量的概率、统计、代数、优化、逻辑知识似乎不可避免. 因此, 本书更适合大学三年级以上的理工科本科生和研究生, 以及具有类似背景的对机器学 习感兴趣的人士. 为方便读者, 本书附录给出了一些相关数学基础知识简介.","score":"8.7","doubanLink":"https://book.douban.com/subject/26708119/"}],"wanted":[{"title":"OpenCV 3计算机视觉：Python语言实现（原书第2版）","cover":"https://img3.doubanio.com/lpic/s28902360.jpg","review":"本书针对具有一定Python工作经验的程序员以及想要利用OpenCV库研究计算机视觉课题的读者。本书不要求读者具有计算机视觉或OpenCV经验，但要具有编程经验。","score":"7.8","doubanLink":"https://book.douban.com/subject/26816975/"},{"title":"Python计算机视觉编程","cover":"https://img3.doubanio.com/lpic/s27305520.jpg","review":"《python计算机视觉编程》适合的读者是：有一定编程与数学基础，想要了解计算机视觉的基本理论与算法的学生，以及计算机科学、信号处理、物理学、应用数学和统计学、神经生理学、认知科学等领域的研究人员和从业者。","score":"7.5","doubanLink":"https://book.douban.com/subject/25906843/"}]}},"slider":[{"image":"https://mic-jasontang.github.io/imgs/coloreggs.jpg","align":"center","title":"computer vision","subtitle":"whole world","link":"/"},{"image":"https://mic-jasontang.github.io/imgs/wall.png","align":"left","title":"import tensorflow as tf","subtitle":"sess.run(hello)","link":null},{"image":"https://mic-jasontang.github.io/imgs/pythoner.png","align":"right","title":"import  __helloworld__","subtitle":">>> Hello World","link":null}]}},"excerpt":"<h1 id=\"1-Convolution\"><a href=\"#1-Convolution\" class=\"headerlink\" title=\"1. Convolution\"></a>1. Convolution</h1><blockquote>\n<p>卷积是什么？</p>\n<p>卷积在数学上用通俗的话来说就是输入矩阵与卷积核（卷积核也是矩阵）进行对应元素相乘并求和，所以一次卷积的结果的输出是一个数，最后对整个输入输入矩阵进行遍历，最终得到一个结果矩阵，下面通过一个动画使其更直观。</p>\n</blockquote>","more":"<ul>\n<li>卷积动画演示<ul>\n<li>卷积核<br><img src=\"https://mic-jasontang.github.io/imgs/conv-kernel.png\" alt=\"卷积动画演示\"></li>\n</ul>\n</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_no_padding.gif\" alt=\"卷积动画演示\"></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_padding.gif\" alt=\"卷积动画演示\"></p>\n<blockquote>\n<p>在上面我们没有使用很专业的数学公式来表示，来解释卷积操作和相关操作，我结合我自己的理解，争取做到白话，及时没有数学基础，也能理解卷积核池化操作。</p>\n<ul>\n<li>卷积的目的</li>\n</ul>\n<p>卷积在图像中的目的就是为了提取特征，我认为这就是深度学习的核心，因为有了卷积层，才避免了我们来手动提取图像的特征，让卷积层自动提取图像的高维度且有效的特征，虽然这没有手动提取特征比如Canny边缘，SIFT，HOG等的强大数学理论基础的支撑，但是卷积层提取的特征让最终的分类、识别结果往往非常的好。比如LeNet-5模型能在MNIST数据集上达到99%的识别率，一般来说网络结构越复杂，越深，往往最终的精确率会越高。</p>\n</blockquote>\n<hr>\n<p><strong>卷积分为许多种，下面将会一一介绍。</strong></p>\n<ul>\n<li>符号约定</li>\n</ul>\n<blockquote>\n<p>i: 输入大小表示为i x i</p>\n<p>k: 卷积核大小表示为k x k</p>\n<p>s: 步长</p>\n<p>p: 填充</p>\n<p>o: 输出表示为o*o</p>\n</blockquote>\n<h2 id=\"1-1-unit-strides\"><a href=\"#1-1-unit-strides\" class=\"headerlink\" title=\"1.1 unit strides\"></a>1.1 unit strides</h2><p>卷积从大体上可以分为单位步长（unit strides)和非单位步长（Non-unit strides），还可以细分为有0填充和无0填充。</p>\n<h3 id=\"1-1-1-No-zero-padding-unit-strides\"><a href=\"#1-1-1-No-zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.1  No zero padding, unit strides\"></a>1.1.1  No zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.png\" alt=\"figure2.1\"></p>\n<p>无零填充 单位步长的卷积，蓝色矩阵是输入（4x4）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1_2.png\" alt=\"figure2.1\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.1.gif\" alt=\"figure2.1\"></p>\n<h3 id=\"1-1-2-Zero-padding-unit-strides\"><a href=\"#1-1-2-Zero-padding-unit-strides\" class=\"headerlink\" title=\"1.1.2 Zero padding, unit strides\"></a>1.1.2 Zero padding, unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.png\" alt=\"figure2.2\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（6x6）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2_2.png\" alt=\"figure2.2\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.2.gif\" alt=\"figure2.2\"></p>\n<h4 id=\"1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\"><a href=\"#1-1-2-1-Zero-padding-unit-strides-Half-Same-padding\" class=\"headerlink\" title=\"1.1.2.1 Zero padding, unit strides - Half(Same) padding\"></a>1.1.2.1 Zero padding, unit strides - Half(Same) padding</h4><p>这种情况叫Half Padding 也叫 Same Padding，因为它能保证输入和输出的尺寸是一致的<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.png\" alt=\"figure2.3\"></p>\n<p>有零填充（p=1） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（5x5）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3_2.png\" alt=\"figure2.3\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.3.gif\" alt=\"figure2.3\"></p>\n<h4 id=\"1-1-2-2-Zero-padding-unit-strides-Full-padding\"><a href=\"#1-1-2-2-Zero-padding-unit-strides-Full-padding\" class=\"headerlink\" title=\"1.1.2.2 Zero padding, unit strides - Full padding\"></a>1.1.2.2 Zero padding, unit strides - Full padding</h4><p>卷积操作产生的输出一般都会减少输入图片的尺寸，但有时候我们需要放大输入图片的尺寸，这个时候就需要使用到Full Padding。<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.png\" alt=\"figure2.4\"></p>\n<p>有零填充（p=2） 单位步长的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（7x7）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4_2.png\" alt=\"figure2.4\"></p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.4.gif\" alt=\"figure2.4\"></p>\n<h2 id=\"1-2-Non-unit-strides\"><a href=\"#1-2-Non-unit-strides\" class=\"headerlink\" title=\"1.2 Non-unit strides\"></a>1.2 Non-unit strides</h2><p>接下来介绍非单位步长（Non-unit stride)的卷积操作，分为有零填充和无零填充。</p>\n<h3 id=\"1-2-1-No-zero-padding-non-unit-strides\"><a href=\"#1-2-1-No-zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.1 No zero padding, non-unit strides\"></a>1.2.1 No zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.png\" alt=\"figure2.5\"></p>\n<p>无零填充 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（2x2）.输出矩阵大小的计算公式为：</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure2.5_5.png\" alt=\"figure2.5\"></p>\n<p>其中向下取整是为了避免(i-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.5.gif\" alt=\"figure2.5\"></p>\n<h3 id=\"1-2-2-Zero-padding-non-unit-strides\"><a href=\"#1-2-2-Zero-padding-non-unit-strides\" class=\"headerlink\" title=\"1.2.2 Zero padding, non-unit strides\"></a>1.2.2 Zero padding, non-unit strides</h3><p><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.png\" alt=\"figure2.6\"></p>\n<p>有零填充（p=1） 非单位步长（s=2）的卷积，蓝色矩阵是输入（5x5）,深蓝色是卷积核（3x3）,上方绿色是输出（3x3）.输出矩阵大小的计算公式为：<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6_2.png\" alt=\"figure2.6\"></p>\n<p>其中向下取整是为了避免(i+2p-k)/s是小数的情况。</p>\n<p>动画演示<br><img src=\"https://mic-jasontang.github.io/imgs/figure2.6.gif\" alt=\"figure2.6\"></p>\n<h2 id=\"1-3-Convolution-as-a-matrix-operation\"><a href=\"#1-3-Convolution-as-a-matrix-operation\" class=\"headerlink\" title=\"1.3 Convolution as a matrix operation\"></a>1.3 Convolution as a matrix operation</h2><p>卷积操作也可以被表示为矩阵的形式，比如将1.1.1中的图转化为矩阵，如下图所示：</p>\n<p>1.1.1中的图被表示为如下形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix_2.png\" alt=\"figure2.6\"></p>\n<p>矩阵表示的形式</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_as_matrix.png\" alt=\"figure2.6\"></p>\n<p>我将上面的矩阵划分为了4行，每一行划分为了4列，表示此卷积操作需要进行16次，W0,0 W0,1 …… W2,2我在图中标注了出来。这个矩阵可以这样来看，按行来看，第一行对应于矩阵表示图的第一个图，第二行对应于矩阵表示图的第二个图，一次类推。</p>\n<h1 id=\"2-Pooling\"><a href=\"#2-Pooling\" class=\"headerlink\" title=\"2. Pooling\"></a>2. Pooling</h1><blockquote>\n<p>池化操作是什么？</p>\n<p>池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。</p>\n</blockquote>\n<blockquote>\n<p>池化操作是什么？</p>\n<p>池化操作的过程和卷积很类似，但是卷积是用来提取特征的，池化层是用来减少卷积层提取的特征的个数的，可以理解为是为了增加特征的鲁棒性或者是降维。</p>\n</blockquote>\n<p>池化分为平均值池化和最大值池化，下面会一一介绍。</p>\n<h2 id=\"2-1-Average-Pooling\"><a href=\"#2-1-Average-Pooling\" class=\"headerlink\" title=\"2.1 Average Pooling\"></a>2.1 Average Pooling</h2><ul>\n<li>平均值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.png\" alt=\"figure1.5\"></p>\n<ul>\n<li>平均值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.5.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h2 id=\"2-2-Max-Pooling\"><a href=\"#2-2-Max-Pooling\" class=\"headerlink\" title=\"2.2 Max Pooling\"></a>2.2 Max Pooling</h2><ul>\n<li>最大值池化可以被表示为</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.png\" alt=\"figure1.6\"></p>\n<ul>\n<li>最大值池化的动画演示</li>\n</ul>\n<p><img src=\"https://mic-jasontang.github.io/imgs/figure1.6.gif\" alt=\"figure1.6\"></p>\n<p>可以看到池化操作也有一个类似于卷积的核，但是这个核不需要提供值，只是表示一个能作用于输入图片的窗口大小。</p>\n<h1 id=\"3-3D-Conv\"><a href=\"#3-3D-Conv\" class=\"headerlink\" title=\"3. 3D-Conv\"></a>3. 3D-Conv</h1><p>3维的卷积，我个人的简单理解，就是在2维卷积的基础上加了一个深度的概念，如图。</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/conv_3d.jpg\" alt=\"figure1.6\"></p>\n<p>输入是一个32x32x3的矩阵，卷积核假定是5x5x3，可以看到一次的卷积操作的结果就是一个带有深度的单位矩阵（2维的一次卷积操作的结果是深度为1的单位矩阵）。这里的深度可以自己指定。</p>\n<p>为了更好的理解3维的卷积，这里引用斯坦福写的一篇博客里面的动画。<a href=\"http://cs231n.github.io/convolutional-networks/\" title=\"博客原地址\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/convolutional-networks/</a></p>\n&lt;iframe<br>    width=”100%”<br>    height=”100%”<br>    src=”<a href=\"http://cs231n.github.io/assets/conv-demo/index.html&quot;\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/assets/conv-demo/index.html&quot;</a><br>点击查看动画<a href=\"http://cs231n.github.io/assets/conv-demo/index.html\" target=\"_blank\" rel=\"noopener\">http://cs231n.github.io/assets/conv-demo/index.html</a><br><br>    width=”750”<br>    height=”720”<br>    src=”<a href=\"https://cs231n.github.io/assets/conv-demo/index.html&quot;\" target=\"_blank\" rel=\"noopener\">https://cs231n.github.io/assets/conv-demo/index.html&quot;</a><br>    frameborder=”0”<br>    allowfullscreen&gt;<br>\n\n<h1 id=\"4-LeNet-5\"><a href=\"#4-LeNet-5\" class=\"headerlink\" title=\"4. LeNet-5\"></a>4. LeNet-5</h1><p>这里介绍下LeNet-5模型，为了理解前面讲述的各种模型</p>\n<h1 id=\"Bibliography\"><a href=\"#Bibliography\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h1><p>这里介绍下LeNet-5模型，为了理解前面讲述的各种卷积和2种池化，下面具体介绍LeNet-5的每个层。</p>\n<ul>\n<li>C1:<ul>\n<li>input: 32x32x1</li>\n<li>conv: 5x5x1</li>\n<li>padding: No Zero</li>\n<li>strides: 1</li>\n<li>output: 28x28x6</li>\n<li>parmas: 5x5x1x6+6 = 156</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第一层，卷积层</p>\n<p>这一层的输入就是原始的图像像素32<em>32</em>1。第一个卷积层过滤器尺寸为5<em>5，深度为6，不使用全0填充，步长为1。所以这一层的输出：28</em>28<em>6，卷积层共有5</em>5<em>1</em>6+6=156个参数。</p>\n</blockquote>\n<ul>\n<li>S2:<ul>\n<li>input: 28x28x6</li>\n<li>pool: 2x2</li>\n<li>padding: No Zero</li>\n<li>strides: 2</li>\n<li>output: 14x14x6</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第二层，池化层</p>\n<p>这一层的输入为第一层的输出，是一个28<em>28</em>6的节点矩阵。本层采用的过滤器大小为2<em>2，长和宽的步长均为2，所以本层的输出矩阵大小为14</em>14*6。</p>\n</blockquote>\n<ul>\n<li>C3:<ul>\n<li>input: 14x14x6</li>\n<li>conv: 5x5x16</li>\n<li>padding: No Zero</li>\n<li>strides: 1</li>\n<li>output: 10x10x16</li>\n<li>params: 5x5x6x16+16=2416</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第三层，卷积层</p>\n<p>本层的输入矩阵大小为14<em>14</em>6，使用的过滤器大小为5<em>5，深度为16.本层不使用全0填充，步长为1。本层的输出矩阵大小为10</em>10<em>16。本层有5</em>5<em>6</em>16+16=2416个参数。</p>\n</blockquote>\n<ul>\n<li>S4:<ul>\n<li>input: 10x10x16</li>\n<li>pool: 2x2</li>\n<li>padding: No Zero</li>\n<li>strides: 2</li>\n<li>output: 5x5x16</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第四层，池化层</p>\n<p>本层的输入矩阵大小10<em>10</em>16。本层采用的过滤器大小为2<em>2，长和宽的步长均为2，所以本层的输出矩阵大小为5</em>5*16。</p>\n</blockquote>\n<ul>\n<li>C5:<ul>\n<li>input: 5x5x16</li>\n<li>conv: 5x5</li>\n<li>padding: No Zero</li>\n<li>strides: 1</li>\n<li>output: 120</li>\n<li>params: 5x5x16x120 + 120=48120</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第五层，全连接层(卷积层)</p>\n<p>本层的输入矩阵大小为5<em>5</em>16，在LeNet-5论文中将这一层成为<strong>卷积层</strong>，但是因为过滤器的大小就是5<em>5，所以和全连接层没有区别。如果将5</em>5<em>16矩阵中的节点拉成一个向量，那么这一层和全连接层就一样了。本层的输出节点个数为120，总共有5</em>5<em>16</em>120+120=48120个参数。</p>\n</blockquote>\n<ul>\n<li>F6:<ul>\n<li>input: 120</li>\n<li>output: 84</li>\n<li>params: 120x84+84 = 10164</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第六层，全连接层</p>\n<p>本层的输入节点个数为120个，输出节点个数为84个，总共参数为120*84+84=10164个。</p>\n</blockquote>\n<ul>\n<li>Output:<ul>\n<li>input: 84</li>\n<li>output: 10</li>\n<li>parmas: 84x10 + 10 = 850</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>第七层，全连接层</p>\n<p>本层的输入节点个数为84个，输出节点个数为10个，总共参数为84*10+10=850</p>\n</blockquote>\n<h1 id=\"Code\"><a href=\"#Code\" class=\"headerlink\" title=\"Code\"></a>Code</h1><ul>\n<li>lenet_train.py</li>\n</ul>\n<p>训练代码</p>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/21 16:08\n# @update  : 2018年4月19日21:38:34\n# @Author  : Jasontang\n# @Site    : \n# @File    : lenet_train.py\n# @ToDo    : 使用LeNet-5模型。定义了神经网络的训练过程\n\nimport os\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\n\n# 配置神经网络的参数\nBATCH_SIZE = 100\n# LEARNING_REATE_BASE = 0.8  # 0.8的学习率导致准确率不高。明显看出不收敛，准确率跟瞎猜差不多。\nLEARNING_REATE_BASE = 0.01  # 降低学习率\nLEARNING_RATE_DECAY = 0.99\nREGULARAZTION_RATE = 0.0001\nTRAING_STEPS = 30000\nMOVING_AVERAGE_DECAY = 0.99\n# 模型保存的路径和文件名\nMODEL_SAVE_PATH = &quot;./model/&quot;\nMODEL_NAME = &quot;model.ckpt&quot;\n\n\ndef train(mnist):\n# 定义输入输出placeholder\nx = tf.placeholder(tf.float32,\n                   [BATCH_SIZE,\n                    mnist_inference.IMAGE_SIZE,\n                    mnist_inference.IMAGE_SIZE,\n                    mnist_inference.NUM_CHANNELS], name=&quot;input-x&quot;)\ny_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\nregularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)\ny = mnist_inference.inference(x, True, regularizer)\nglobal_step = tf.Variable(0, trainable=False)\n\nvariable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\nvariables_average_op = variable_averages.apply(tf.trainable_variables())\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(y_, 1), logits=y)\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n# loss = tf.get_collection(&quot;losses&quot;) 返回一个列表\n# loss_add = tf.add_n(loss) 将列表元素进行相加\nloss = cross_entropy_mean + tf.add_n(tf.get_collection(&quot;losses&quot;))\n\nlearing_rate = tf.train.exponential_decay(LEARNING_REATE_BASE,\n                                          global_step,\n                                          mnist.train.num_examples / BATCH_SIZE,\n                                          LEARNING_RATE_DECAY)\ntrain_step = tf.train.GradientDescentOptimizer(learing_rate).minimize(loss, global_step)\n\nwith tf.control_dependencies([train_step, variables_average_op]):\n    train_op = tf.no_op(name=&quot;train&quot;)\n\n# 初始化持久化类\nsaver = tf.train.Saver()\nwith tf.device(&quot;/gpu:0&quot;):\n    session_conf = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Session(config=session_conf) as sess:\n        tf.global_variables_initializer().run()\n\n        for i in range(TRAING_STEPS):\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            reshaped_xs = np.reshape(xs, [BATCH_SIZE,\n                                          mnist_inference.IMAGE_SIZE,\n                                          mnist_inference.IMAGE_SIZE,\n                                          mnist_inference.NUM_CHANNELS])\n            # print(type(xs))\n            # print(type(reshaped_xs))\n            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: reshaped_xs, y_: ys})\n\n            if i % 1000 == 0:\n                print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (step, loss_value))\n\n                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n\n\ndef main(argv=None):\n    # 存放目录为当前工程目录下的MNIST_data目录\n    mnist = input_data.read_data_sets(&quot;./MNIST_data&quot;, one_hot=True)\n    train(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><ul>\n<li>lenet_inference</li>\n</ul>\n<p>计算代码</p>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018/2/20 19:43\n# @update  : 2018年4月19日21:38:34\n# @Author  : Jasontang\n# @Site    : \n# @File    : lent_inference.py\n# @ToDo    : 使用LeNet-5模型。定义了前向传播的过程及神经网络的参数\n\n\nimport tensorflow as tf\n\n# 定义神经网络结构相关的参数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\n\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nNUM_LABELS = 10\n\n# 第一层卷积层的尺寸和深度\nCONV1_DEEP = 32\nCONV1_SIZE = 5\n# 第二层卷积层的尺寸和深度\nCONV2_DEEP = 64\nCONV2_SIZE = 5\n# 全连接层的结点个数\nFC_SIZE = 512\n\n# 卷积神经网络的前向传播过程\n# 添加一个新的参数train，用于区分训练过程和测试过程。\n# 在这个程序中将用到dropout方法，dropout可以进一步提升模型可靠性并防止\n# 过拟合。dropout过程只在训练时使用。\ndef inference(input_tensor, train, regularizer, dropout=0.5):\n    # 声明第一层神经网络的变量并完成前向传播过程\n    # 和标准的LeNet-5模型不大一样，这里定义的卷积层输入为28*28*1的原始MNIST图片像素。\n    # 因为卷积层使用了全0填充，所以输出为28*28*32的矩阵。\n    with tf.variable_scope(&quot;layer1-conv1&quot;):\n        conv1_weights = tf.get_variable(&quot;weights&quot;, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],\n                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv1_biases = tf.get_variable(&quot;biases&quot;, [CONV1_DEEP], initializer=tf.constant_initializer(0.1))\n        # 使用变长为5，深度为32的过滤器，过滤器移动的步长为1，且使用全0填充。\n        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;)\n        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n\n    # 实现第二层池化层的前向传播过程，这里选择最大池化层，池化层过滤器的变长为2\n    # 使用全0填充且移动的步长为2，这一层的输入是上一层的输出，也就是28*28*32的矩阵。\n    # 输出为14*14*32的矩阵\n    with tf.variable_scope(&quot;layer2-pool1&quot;):\n        pool1 = tf.nn.max_pool(relu1,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=&quot;SAME&quot;)\n\n    # 声明第三层卷积层的变量并实现前向传播过程，这一层的输入为14*14*32的矩阵。\n    # 输出为14*14*64的矩阵。\n    with tf.variable_scope(&quot;layer3-conv2&quot;):\n        conv2_weights = tf.get_variable(&quot;weight&quot;,\n                                        [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],\n                                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n        conv2_biases = tf.get_variable(&quot;bias&quot;, [CONV2_DEEP], initializer=tf.constant_initializer(0.1))\n\n        # 使用边长为5，深度为64的过滤器，过滤器移动的步长为1，且使用全0填充。\n        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;)\n        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n\n    # 实现第四层池化层的前向传播过程。这一层和第二层的记过是一样的，这一层的输入为14*14*64的矩阵，\n    # 输出位7*7*64的矩阵。\n    with tf.name_scope(&quot;layer4-pool2&quot;):\n        pool2 = tf.nn.max_pool(relu2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=&quot;SAME&quot;)\n\n    # 将第四层池化层的输出转化为第五层全连接层的输入格式。\n    # 第四层的输出为7*7*64的矩阵，然后第五层全连接层需要的输入格式为向量\n    # 所以在这里需要将这个7*7*64的矩阵拉直成一个向量。pool2.get_shape函数可以得到\n    # 第四层输出矩阵的维度而不需要手工计算。注意因为每一层神经网络的输入输出都为一个batch的矩阵，\n    # 所以这里得到的维度也包含了一个batch的数据的个数。\n    pool_shape = pool2.get_shape().as_list()\n\n    # 计算将矩阵拉直成向量之后的长度，这个长度就是矩阵长宽及深度的乘积。\n    # 这里pool_shape[0]为一个batch中数据的个数\n    nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n\n    # 通过tf.reshape函数将第四层的输出变成一个batch的向量\n    reshaped = tf.reshape(pool2, [pool_shape[0], nodes])\n\n    # 声明第五层全连接层的变量并实现前向传播过程，这一层的输入是拉直之后的一组向量，\n    # 向量长度为3136，输出是一组长度为512的向量。这一层和之前在重构MNIST数据集的代码基本一致，\n    # 唯一的区别就是引入了dropout的概念。dropout在训练时会随机将部分结点的输出改为0。\n    # dropout可以避免过拟合问题，从而使得模型在测试数据上的效果更好。\n    # dropout一般只在全连接层而不是卷积层或者池化层使用。\n    with tf.variable_scope(&quot;layer5-fc1&quot;):\n        fc1_weights = tf.get_variable(&quot;weight&quot;,\n                                      [nodes, FC_SIZE],\n                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # 只要全连接层的权重需要加入正则化\n        if regularizer is not None:\n            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n            tf.add_to_collection(&quot;losses&quot;, regularizer(fc1_weights))\n        fc1_biases = tf.get_variable(&quot;bias&quot;,\n                                     [FC_SIZE],\n                                     initializer=tf.constant_initializer(0.1))\n        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n        if train:\n            fc1 = tf.nn.dropout(fc1, dropout)\n\n    # 声明第六层全连接层的变量并实现前向传播过程。\n    # 这一层的输入为一组长度为512的向量，输出为一组长度为10的向量。\n    # 这一层的输出通过Softmax之后就得到了最后的分类结果。\n    with tf.variable_scope(&quot;layer6-fc2&quot;):\n        fc2_weights = tf.get_variable(&quot;weight&quot;,\n                                      [FC_SIZE, NUM_LABELS],\n                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n        if regularizer is not None:\n            # 当使用正则化生成函数时,当前变量的正则化损失加入名字为losses的集合.\n            tf.add_to_collection(&quot;losses&quot;, regularizer(fc2_weights))\n        fc2_biases = tf.get_variable(&quot;bias&quot;,\n                                     [NUM_LABELS],\n                                     initializer=tf.constant_initializer(0.1))\n        logit = tf.matmul(fc1, fc2_weights) + fc2_biases\n\n    # 返回第六层的输出\n    return logit\n</code></pre><ul>\n<li>lenet_eval.py</li>\n</ul>\n<p>测试代码</p>\n<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 2018年4月19日21:38:34\n# @Author  : Jasontang\n# @Site    : \n# @File    : mnist_eval.py\n# @ToDo    : 测试过程\n\n\nimport time\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport neural_network_learning.cnn.lenet.mnist_inference as mnist_inference\nimport neural_network_learning.cnn.lenet.mnist_train as mnist_train\n\n# 每10s加载一次最新模型，并在测试数据上测试最新模型的正确率\nEVAL_INTERVAL_SECS = 10\n\n\ndef evaluate(mnist):\n    x = tf.placeholder(tf.float32,\n                       [mnist.validation.images.shape[0],\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.IMAGE_SIZE,\n                        mnist_inference.NUM_CHANNELS], name=&quot;input-x&quot;)\n    y_ = tf.placeholder(tf.float32, [None, mnist_inference.OUTPUT_NODE], name=&quot;input-y&quot;)\n\n    y = mnist_inference.inference(x, False, None)\n\n    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # 每隔EVAL_INTERVAL_SECS秒调用一次计算正确率的过程以检测训练过程中正确率的变化\n    stop_count = 0\n    while True:\n        with tf.Session() as sess:\n            # tf.train.get_checkpoint_state函数会通过checkpoint文件自动找刀目录中最新模型的文件名\n            ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)\n            # 停止条件 #\n            stop_count += 1\n            if stop_count &gt; 5:\n                return\n            # 停止条件 #\n            if ckpt and ckpt.model_checkpoint_path:\n                # 加载模型\n                saver.restore(sess, ckpt.model_checkpoint_path)\n                # 通过文件名得到模型保存时迭代的轮数\n                # 输出./model/model.ckpt-29001\n                print(ckpt.model_checkpoint_path)\n                global_step = ckpt.model_checkpoint_path.split(&quot;/&quot;)[-1].split(&quot;-&quot;)[-1]\n                validate_feed = {x: mnist.validation.images,\n                                 y_: mnist.validation.labels}\n\n                # print(validate_feed[x])\n                reshaped_x = np.reshape(validate_feed[x],\n                                        [validate_feed[x].shape[0],\n                                         mnist_inference.IMAGE_SIZE,\n                                         mnist_inference.IMAGE_SIZE,\n                                         mnist_inference.NUM_CHANNELS])\n                validate_feed[x] = reshaped_x\n                accuracy_score = sess.run(accuracy, feed_dict=validate_feed)\n                print(&quot;After %s training step(s), validation accuracy is %g&quot; % (global_step, accuracy_score))\n            else:\n                print(&quot;No checkpoint file found&quot;)\n                return\n        time.sleep(EVAL_INTERVAL_SECS)\n\n\ndef main(argv=None):\n    # 存放目录为当前工程目录下的MNIST_data目录\n    mnist = input_data.read_data_sets(&quot;./MNIST_data&quot;, one_hot=True)\n    evaluate(mnist)\n\n\nif __name__ == &apos;__main__&apos;:\n    tf.app.run()\n</code></pre><p>MNIST数据集</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/mnist_data.png\" alt=\"AI Live\"></p>\n<p>测试结果</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/mnist_result.png\" alt=\"AI Live\"></p>\n<p>最终正确率可以达到99%</p>\n<h1 id=\"Bibliography-1\"><a href=\"#Bibliography-1\" class=\"headerlink\" title=\"Bibliography\"></a>Bibliography</h1><ol>\n<li>A guide to convolution arithmetic for deep learning<a href=\"https://arxiv.org/pdf/1603.07285.pdf\" title=\"A guide to convolution arithmetic for deep learning\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/pdf/1603.07285.pdf</a></li>\n<li>TensforFlow 实战Google深度学习框架</li>\n</ol>\n<p>我参与举办了一个小团体，主要是技术分享，这篇是第三期的分享内容。下面是我们的公众号:</p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/AI_Live.jpg\" alt=\"AI Live\"></p>\n<p><img src=\"https://mic-jasontang.github.io/imgs/join_us.jpg\" alt=\"Join Us\"></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cjvf4dqpo0000lnvvkd0ws8cv","category_id":"cjvf4dqpu0003lnvvqxuqbq4s","_id":"cjvf4dqqa000tlnvvymdc8geh"},{"post_id":"cjvf4dqpo0000lnvvkd0ws8cv","category_id":"cjvf4dqq7000mlnvvafa7bpkz","_id":"cjvf4dqqb000vlnvvag8lyj9o"},{"post_id":"cjvf4dqpt0002lnvv28dpdcny","category_id":"cjvf4dqpz0008lnvvd749vsg9","_id":"cjvf4dqqc0014lnvvje06iy0d"},{"post_id":"cjvf4dqpt0002lnvv28dpdcny","category_id":"cjvf4dqqb000xlnvvc6efwab4","_id":"cjvf4dqqd0017lnvvsu9bx4pq"},{"post_id":"cjvf4dqq1000blnvvqctdzlhk","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqqd0019lnvvqqgrxmgz"},{"post_id":"cjvf4dqq3000elnvvyrp3m38o","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqqe001clnvvpk887lnx"},{"post_id":"cjvf4dqpw0005lnvv0lvpncee","category_id":"cjvf4dqq3000clnvv10w9pbon","_id":"cjvf4dqqf001ilnvvofgiy3pt"},{"post_id":"cjvf4dqpw0005lnvv0lvpncee","category_id":"cjvf4dqqd0018lnvvqcczxa40","_id":"cjvf4dqqf001klnvv6mn39jdi"},{"post_id":"cjvf4dqq5000glnvvb8hbqsrw","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqqf001nlnvv5a9pgsyf"},{"post_id":"cjvf4dqq6000klnvvnkekv1pd","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqqg001qlnvv7zvcmabc"},{"post_id":"cjvf4dqq7000llnvvwcqv1poz","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqqg001slnvvzn7tl8ov"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","category_id":"cjvf4dqq6000ilnvvdvnf6my1","_id":"cjvf4dqqh001vlnvvj0qrdo4p"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","category_id":"cjvf4dqqg001plnvvu94qaz3c","_id":"cjvf4dqqh001xlnvvw85ls0gl"},{"post_id":"cjvf4dqq8000plnvvg04xjvzz","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqqi0023lnvvieewf3il"},{"post_id":"cjvf4dqpy0007lnvvgeyrrh27","category_id":"cjvf4dqpz0008lnvvd749vsg9","_id":"cjvf4dqqj0029lnvvfa2jithy"},{"post_id":"cjvf4dqpy0007lnvvgeyrrh27","category_id":"cjvf4dqqh001zlnvvlwd2w2ms","_id":"cjvf4dqqj002blnvvlhli8cdq"},{"post_id":"cjvf4dqq0000alnvvptqmpgyq","category_id":"cjvf4dqq6000ilnvvdvnf6my1","_id":"cjvf4dqqj002elnvv3zr1sbgv"},{"post_id":"cjvf4dqq0000alnvvptqmpgyq","category_id":"cjvf4dqqg001plnvvu94qaz3c","_id":"cjvf4dqqk002flnvv57fwfldx"},{"post_id":"cjvf4dqq8000olnvv3lbk6r4h","category_id":"cjvf4dqqg001tlnvvgy0o05q8","_id":"cjvf4dqqk002hlnvvj0ihbnm8"},{"post_id":"cjvf4dqq8000olnvv3lbk6r4h","category_id":"cjvf4dqqi0028lnvvhunxoxvx","_id":"cjvf4dqqk002jlnvvxhojonf0"},{"post_id":"cjvf4dqr90040lnvvx9p6qppg","category_id":"cjvf4dqqc0012lnvvu3getomw","_id":"cjvf4dqrc0043lnvvx88cm4hb"},{"post_id":"cjvf4dqr8003ylnvvonpwbtz5","category_id":"cjvf4dqra0041lnvvv03zzox1","_id":"cjvf4dqrf004alnvvfvvz54in"},{"post_id":"cjvf4dqr8003ylnvvonpwbtz5","category_id":"cjvf4dqre0045lnvv822mcibm","_id":"cjvf4dqrg004blnvv1ferz6wv"},{"post_id":"cjvf4dqs2004glnvv3phyw3c3","category_id":"cjvf4dqs3004hlnvvvvoipfhs","_id":"cjvf4dqs6004mlnvvd4hl7b2k"},{"post_id":"cjvf4dqs2004glnvv3phyw3c3","category_id":"cjvf4dqs4004klnvv6laregtv","_id":"cjvf4dqs6004olnvv8ch842vj"}],"PostTag":[{"post_id":"cjvf4dqpo0000lnvvkd0ws8cv","tag_id":"cjvf4dqpv0004lnvvwpqi28kl","_id":"cjvf4dqq4000flnvvcck7ll6j"},{"post_id":"cjvf4dqpo0000lnvvkd0ws8cv","tag_id":"cjvf4dqpz0009lnvvj4qb2q5g","_id":"cjvf4dqq5000hlnvv2sdc0hpc"},{"post_id":"cjvf4dqpt0002lnvv28dpdcny","tag_id":"cjvf4dqq3000dlnvvxeoohok6","_id":"cjvf4dqqb000wlnvv83rn17iq"},{"post_id":"cjvf4dqpt0002lnvv28dpdcny","tag_id":"cjvf4dqq6000jlnvvecnh6z70","_id":"cjvf4dqqb000ylnvvutg4wf6m"},{"post_id":"cjvf4dqpt0002lnvv28dpdcny","tag_id":"cjvf4dqq8000nlnvvpe1819qf","_id":"cjvf4dqqc0010lnvv8sqy3faj"},{"post_id":"cjvf4dqpt0002lnvv28dpdcny","tag_id":"cjvf4dqq9000rlnvvlyvh03qd","_id":"cjvf4dqqc0011lnvvw9j07baa"},{"post_id":"cjvf4dqpw0005lnvv0lvpncee","tag_id":"cjvf4dqqb000ulnvvo3vfnx1h","_id":"cjvf4dqqe001blnvvfhruguyy"},{"post_id":"cjvf4dqpw0005lnvv0lvpncee","tag_id":"cjvf4dqqc000zlnvvgejgu2vz","_id":"cjvf4dqqe001elnvv5653qpwn"},{"post_id":"cjvf4dqpw0005lnvv0lvpncee","tag_id":"cjvf4dqqc0013lnvvnbgo7qix","_id":"cjvf4dqqe001glnvv614njofd"},{"post_id":"cjvf4dqpw0005lnvv0lvpncee","tag_id":"cjvf4dqqd0016lnvvww1cpvpo","_id":"cjvf4dqqf001jlnvvnvfxzq7b"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","tag_id":"cjvf4dqqe001alnvvg7unuago","_id":"cjvf4dqqh0020lnvvydav2jqn"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","tag_id":"cjvf4dqqe001flnvvsslbad2h","_id":"cjvf4dqqi0021lnvveeo9jdu0"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","tag_id":"cjvf4dqqf001llnvvdmbu89d8","_id":"cjvf4dqqi0025lnvv8gjdmwbi"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","tag_id":"cjvf4dqqg001olnvvuucw31qy","_id":"cjvf4dqqi0026lnvvqye6g1ou"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","tag_id":"cjvf4dqqg001rlnvvehcfephb","_id":"cjvf4dqqj002alnvvmiu8uq2o"},{"post_id":"cjvf4dqpx0006lnvvs3uog4r7","tag_id":"cjvf4dqqg001ulnvvc4o6plim","_id":"cjvf4dqqj002clnvv4jfk3fz5"},{"post_id":"cjvf4dqpy0007lnvvgeyrrh27","tag_id":"cjvf4dqqh001ylnvvc0pt4hpy","_id":"cjvf4dqqk002ilnvvhlu7kurr"},{"post_id":"cjvf4dqpy0007lnvvgeyrrh27","tag_id":"cjvf4dqqi0022lnvv8yc3rrmz","_id":"cjvf4dqqk002klnvvi87zehb8"},{"post_id":"cjvf4dqpy0007lnvvgeyrrh27","tag_id":"cjvf4dqq8000nlnvvpe1819qf","_id":"cjvf4dqqk002mlnvv40w57str"},{"post_id":"cjvf4dqpy0007lnvvgeyrrh27","tag_id":"cjvf4dqq9000rlnvvlyvh03qd","_id":"cjvf4dqql002nlnvvagzbly4p"},{"post_id":"cjvf4dqq0000alnvvptqmpgyq","tag_id":"cjvf4dqqk002glnvvfp46ruo5","_id":"cjvf4dqql002qlnvvti7nv6sq"},{"post_id":"cjvf4dqq0000alnvvptqmpgyq","tag_id":"cjvf4dqqe001alnvvg7unuago","_id":"cjvf4dqql002rlnvvo9tt0ayy"},{"post_id":"cjvf4dqq0000alnvvptqmpgyq","tag_id":"cjvf4dqql002olnvv8e2rda06","_id":"cjvf4dqql002tlnvvgsq6u160"},{"post_id":"cjvf4dqq1000blnvvqctdzlhk","tag_id":"cjvf4dqql002plnvv03czsmun","_id":"cjvf4dqqm002wlnvvmuw9bxnh"},{"post_id":"cjvf4dqq1000blnvvqctdzlhk","tag_id":"cjvf4dqql002slnvvblgl8rc8","_id":"cjvf4dqqm002xlnvvgooi2ykw"},{"post_id":"cjvf4dqq1000blnvvqctdzlhk","tag_id":"cjvf4dqql002ulnvvfrglqa60","_id":"cjvf4dqqm002zlnvvfr40cn93"},{"post_id":"cjvf4dqq3000elnvvyrp3m38o","tag_id":"cjvf4dqqm002vlnvvyovg7gug","_id":"cjvf4dqqn0032lnvvi988o73b"},{"post_id":"cjvf4dqq3000elnvvyrp3m38o","tag_id":"cjvf4dqqm002ylnvv20zye2pm","_id":"cjvf4dqqn0033lnvvo6fd0m37"},{"post_id":"cjvf4dqq3000elnvvyrp3m38o","tag_id":"cjvf4dqqm0030lnvvghkvshus","_id":"cjvf4dqqn0035lnvv5qxd4tc8"},{"post_id":"cjvf4dqq5000glnvvb8hbqsrw","tag_id":"cjvf4dqqn0031lnvvznqgko0m","_id":"cjvf4dqqo0038lnvv3go5pa35"},{"post_id":"cjvf4dqq5000glnvvb8hbqsrw","tag_id":"cjvf4dqqn0034lnvv8m2sigm7","_id":"cjvf4dqqo0039lnvv5n9g1hyy"},{"post_id":"cjvf4dqq5000glnvvb8hbqsrw","tag_id":"cjvf4dqql002ulnvvfrglqa60","_id":"cjvf4dqqo003blnvv7pawfyuv"},{"post_id":"cjvf4dqq6000klnvvnkekv1pd","tag_id":"cjvf4dqqo0037lnvvfnluv25w","_id":"cjvf4dqqp003dlnvvyukh1m6z"},{"post_id":"cjvf4dqq6000klnvvnkekv1pd","tag_id":"cjvf4dqql002ulnvvfrglqa60","_id":"cjvf4dqqp003elnvvjd6kn89i"},{"post_id":"cjvf4dqq7000llnvvwcqv1poz","tag_id":"cjvf4dqqo003clnvvehgvmmu2","_id":"cjvf4dqqr003klnvvizllvr1u"},{"post_id":"cjvf4dqq7000llnvvwcqv1poz","tag_id":"cjvf4dqqp003flnvvnwesaxwu","_id":"cjvf4dqqr003llnvvmucgf89r"},{"post_id":"cjvf4dqq7000llnvvwcqv1poz","tag_id":"cjvf4dqqn0031lnvvznqgko0m","_id":"cjvf4dqqr003nlnvvvyygdfwe"},{"post_id":"cjvf4dqq7000llnvvwcqv1poz","tag_id":"cjvf4dqqq003hlnvvjvhlhbea","_id":"cjvf4dqqr003olnvvj20yqysd"},{"post_id":"cjvf4dqq7000llnvvwcqv1poz","tag_id":"cjvf4dqql002ulnvvfrglqa60","_id":"cjvf4dqqs003qlnvv6aoy473f"},{"post_id":"cjvf4dqq8000olnvv3lbk6r4h","tag_id":"cjvf4dqqq003jlnvvd57rjqbg","_id":"cjvf4dqqs003slnvv8bgqtflv"},{"post_id":"cjvf4dqq8000olnvv3lbk6r4h","tag_id":"cjvf4dqqr003mlnvvgvohqoui","_id":"cjvf4dqqs003tlnvv58eo3jug"},{"post_id":"cjvf4dqq8000olnvv3lbk6r4h","tag_id":"cjvf4dqqr003plnvvbyep4qy8","_id":"cjvf4dqqt003vlnvvi1rf5fpl"},{"post_id":"cjvf4dqq8000plnvvg04xjvzz","tag_id":"cjvf4dqqs003rlnvvsbn05mjh","_id":"cjvf4dqqt003wlnvvl86382zb"},{"post_id":"cjvf4dqq8000plnvvg04xjvzz","tag_id":"cjvf4dqql002ulnvvfrglqa60","_id":"cjvf4dqqu003xlnvv58284lpx"},{"post_id":"cjvf4dqr8003ylnvvonpwbtz5","tag_id":"cjvf4dqrb0042lnvvnhee4b4h","_id":"cjvf4dqrf0047lnvv3mbtrqcn"},{"post_id":"cjvf4dqr8003ylnvvonpwbtz5","tag_id":"cjvf4dqrd0044lnvvfnybnoq6","_id":"cjvf4dqrf0048lnvv792nuone"},{"post_id":"cjvf4dqr90040lnvvx9p6qppg","tag_id":"cjvf4dqre0046lnvv2otdjr62","_id":"cjvf4dqrg004clnvv35mkbgpr"},{"post_id":"cjvf4dqr90040lnvvx9p6qppg","tag_id":"cjvf4dqrf0049lnvv8i2aiwup","_id":"cjvf4dqrg004dlnvviow1jwz0"},{"post_id":"cjvf4dqr90040lnvvx9p6qppg","tag_id":"cjvf4dqqo0037lnvvfnluv25w","_id":"cjvf4dqrg004elnvvmi78rcjz"},{"post_id":"cjvf4dqr90040lnvvx9p6qppg","tag_id":"cjvf4dqql002ulnvvfrglqa60","_id":"cjvf4dqrg004flnvvw5a9rw4k"},{"post_id":"cjvf4dqs2004glnvv3phyw3c3","tag_id":"cjvf4dqs3004ilnvvwcficidj","_id":"cjvf4dqs6004nlnvv5pxwqqty"},{"post_id":"cjvf4dqs2004glnvv3phyw3c3","tag_id":"cjvf4dqs4004jlnvv4juwm1yp","_id":"cjvf4dqs6004plnvvu8etbj29"},{"post_id":"cjvf4dqs2004glnvv3phyw3c3","tag_id":"cjvf4dqrd0044lnvvfnybnoq6","_id":"cjvf4dqs7004qlnvvifx5zy0w"},{"post_id":"cjvf4dqs2004glnvv3phyw3c3","tag_id":"cjvf4dqs4004llnvvcwaadj18","_id":"cjvf4dqs7004rlnvvz3xsgopi"}],"Tag":[{"name":"hexo","_id":"cjvf4dqpv0004lnvvwpqi28kl"},{"name":"多电脑同步","_id":"cjvf4dqpz0009lnvvj4qb2q5g"},{"name":"二叉树","_id":"cjvf4dqq3000dlnvvxeoohok6"},{"name":"层次遍历","_id":"cjvf4dqq6000jlnvvecnh6z70"},{"name":"java实现","_id":"cjvf4dqq8000nlnvvpe1819qf"},{"name":"Python实现","_id":"cjvf4dqq9000rlnvvlyvh03qd"},{"name":"k阶矩","_id":"cjvf4dqqb000ulnvvo3vfnx1h"},{"name":"随机过程","_id":"cjvf4dqqc000zlnvvgejgu2vz"},{"name":"偏度","_id":"cjvf4dqqc0013lnvvnbgo7qix"},{"name":"峰度","_id":"cjvf4dqqd0016lnvvww1cpvpo"},{"name":"图像处理","_id":"cjvf4dqqe001alnvvg7unuago"},{"name":"卷积运算","_id":"cjvf4dqqe001flnvvsslbad2h"},{"name":"guass","_id":"cjvf4dqqf001llnvvdmbu89d8"},{"name":"soble","_id":"cjvf4dqqg001olnvvuucw31qy"},{"name":"prewitt","_id":"cjvf4dqqg001rlnvvehcfephb"},{"name":"laplacian","_id":"cjvf4dqqg001ulnvvc4o6plim"},{"name":"字符串","_id":"cjvf4dqqh001ylnvvc0pt4hpy"},{"name":"旋转词","_id":"cjvf4dqqi0022lnvv8yc3rrmz"},{"name":"python","_id":"cjvf4dqqk002glnvvfp46ruo5"},{"name":"直方图均衡化","_id":"cjvf4dqql002olnvv8e2rda06"},{"name":"风格迁移","_id":"cjvf4dqql002plnvv03czsmun"},{"name":"序列模型","_id":"cjvf4dqql002slnvvblgl8rc8"},{"name":"deep learning","_id":"cjvf4dqql002ulnvvfrglqa60"},{"name":"NLP基础","_id":"cjvf4dqqm002vlnvvyovg7gug"},{"name":"CV基础","_id":"cjvf4dqqm002ylnvv20zye2pm"},{"name":"相机模型","_id":"cjvf4dqqm0030lnvvghkvshus"},{"name":"YOLO","_id":"cjvf4dqqn0031lnvvznqgko0m"},{"name":"Attention","_id":"cjvf4dqqn0034lnvv8m2sigm7"},{"name":"GN","_id":"cjvf4dqqo0037lnvvfnluv25w"},{"name":"目标检测","_id":"cjvf4dqqo003clnvvehgvmmu2"},{"name":"R-CNN系列","_id":"cjvf4dqqp003flnvvnwesaxwu"},{"name":"SSD","_id":"cjvf4dqqq003hlnvvjvhlhbea"},{"name":"表面粗糙度","_id":"cjvf4dqqq003jlnvvd57rjqbg"},{"name":"matlab模拟粗糙度","_id":"cjvf4dqqr003mlnvvgvohqoui"},{"name":"3dsMax仿真","_id":"cjvf4dqqr003plnvvbyep4qy8"},{"name":"FCN","_id":"cjvf4dqqs003rlnvvsbn05mjh"},{"name":"Mnist","_id":"cjvf4dqrb0042lnvvnhee4b4h"},{"name":"tensorflow","_id":"cjvf4dqrd0044lnvvfnybnoq6"},{"name":"Normalization","_id":"cjvf4dqre0046lnvv2otdjr62"},{"name":"BN","_id":"cjvf4dqrf0049lnvv8i2aiwup"},{"name":"卷积","_id":"cjvf4dqs3004ilnvvwcficidj"},{"name":"池化","_id":"cjvf4dqs4004jlnvv4juwm1yp"},{"name":"LeNet-5","_id":"cjvf4dqs4004llnvvcwaadj18"}]}}
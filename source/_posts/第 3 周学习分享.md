---
title: YOLO & Attention
date: 2019-04-13 11:11:00
categories:
- 周总结
tags: 
- YOLO
- Attention 
- deep learning
---
这周准备了3场笔试，所以学习新知识的时间不多。主要将Andrew Ng的深度学习课程的最后一节序列模型和注意力机制学完了，同时回顾了一下前面学习的YOLO算法，下面做一个总结。。

<!-- more -->
1. YOLO (v1）：

- YOLO算法，首先将图像网格化，然后将目标按中心所在位置分配给所在格子，然后利用卷积网络的参数共享特性，只需要一次卷积就可以得出结果，加速计算，能做到实时。
- 对每个格子都预测B个bounding boxes，每个bounding box都包含5个预测值：x,y,w,h和confidence，在原文中作者取S=7，B=2.
- 为了解决有很多检测框的问题，h和confidence，保留最大的，其他的删除。
- 为了解决有多个目标出现在同一个grid里的情况，通过预先设置Anchor box模板，来解决，假设会有2个目标同时出现在同一个grid中，行人和汽车，就设置两个Anchor box1和Anchor box2，同时将输出y，这里y里面的元素就会有10个了。再对于每个类别，单独运行非极大值抑制，就可以得到最后的结果。
- 如何选择anchor box：
1、可以人工的指定Anchor box的形状，使其包含训练集中的大多数样本，个数一般5-10个左右。
2、还有更好的做法，k-means算法，对形状进行聚类。

2. Attention：
Attention说白了是做一个向量加权，在NLP、语音和图像描述等方面上得到了应用。

    2.1. Attention的定义：

    - 给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。

    - attention的重点就是这个集合values中的每个value的“权值”的计算方法。

    - 有时候也把这种attention的机制叫做query的输出关注了（或者说叫考虑到了）原文的不同部分
   
   通过计算Attention Scores，然后利用softmax函数将其概率化。
   
    2.2. Attention的变体
   
    - soft attention、global attention、动态attention
    - Hard attention
    - local attention
    - 静态attention
    - 强制前向attention
    - self attention
    - key-value attention
    - multi-head attention
    


















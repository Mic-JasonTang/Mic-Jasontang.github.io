---
title: 风格迁移与序列模型研究
date: 2019-03-30 11:11:00
categories:
- 周总结
tags: 
- 风格迁移
- 序列模型 
- deep learning
---

<!-- more -->

1. 风格迁移：将一副图片的风格迁移到另一幅新的图片中。
	- a. 使用两张图片作为输入（如一张毕加索的图S，一张普通照片C）生成一张新的图片G；
	- b. 风格迁移需要定义它的损失函数，内容损失L_content和风格损失L_style，整体损失为L=αL_content(C, G) + βL_style(S, G)； 
	- c. 有了输入和损失函数，就可以来训练网络了，S和C都是已知的，那G怎么初始化呢？一般采用随机数来填充整幅图像；
	- d. 那么内容损失函数L_content具体是怎么定义呢？最简单的，取两幅图像像素差值的二范数。；
	- e. L_style如何定义呢？这里公式有点复杂，用文字描述就是单独计算S的通道之间像素相关性（像素值相乘）、G的通道之间像素相关性（像素值相乘），之后作差，求Frobenius范数。；有了输入、初始化、损失，就可以使用优化方法对损失函数进行优化了。
2. 序列模型：学习RNN及其变种。
	- a. 序列模型偏向于NLP、语音等方面；
	- b. 首先是RNN网络，相比于标准的层级神经网路，RNN可以处理输入数据长度和输出数据长度不同的情况；不能在文本的不同位置共享已学得的特征。但是RNN不能利用后续的信息，在文本处理方面会有一定的局限性，比如Name Entity Recognition。而且RNN不能处理梯度消失的问题，当RNN层数很深的时候，反向传播时，后面的层，不能去影响前面的层的参数。；
	- c. GRU网络，可以说它解决了RNN梯度消失的问题，它引入了C(memory cell)来记忆和更新门(gamma_u)来控制是否更新C或者使用old_C。；
	- d. LSTM网络，GRU网络是LSTM的简化版本，后者多了遗忘门(gamma_f)和输出门(gamma_o)，使用gamma_u和gamma_f来控制是否更新C，所以GRU更适合搭建复杂网络，相比之下GRU搭建的网络参数会比较少。；
	- e. BRNN(Bidirectional RNN)，在RNN的基础上增加了一个反向的RNN，正因为有了双向，所以使得RNN可以使用整段文本的信息，不过这一般需要等待文字/语音输入完毕，不适合于实时系统。；
	- f. Deep RNNs，使用RNN、GRU、LSTM作为基本单元，横向、纵向地去搭建更深的网络以完成更复杂的任务。


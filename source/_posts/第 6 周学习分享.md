---
title: FCN 论文总结
date: 2019-05-04 11:00:46
categories:
- 周总结
tags: 
- FCN
- deep learning
---
FCN，于2015年被Jonathan提出，是应用在Semantic segmentation中，下面总结与CNN有何不同，以及它的优缺点。
<!-- more -->

2015年Jonathan发表了[《Fully Convolutional Networks for Semantic Segmentation》](https://arxiv.org/pdf/1411.4038.pdf)将FCN应用在了语义分割中。

##### 1. CNN与FCN有何不同？
##### 1.1 网络结构上的不同
CNN通常在卷积层之后会接上若干个全连接层，将卷积层产生的特征映射为固定长度的特征向量。如经典的AlexNet，最后输出1000维的特征向量，再用Softmax做分类。CNN的输入尺寸一般是固定的。

FCN，所有层都是卷积层，可以接受任意尺寸的输入图像，采用反卷积对FCN中最后一个卷积层的特征图**上采样**，使它恢复到输入图像相同的尺寸，最后逐个像素计算softmax分类的损失，相当于每一个像素对应一个训练样本，如图1所示。
![FCN](https://images2015.cnblogs.com/blog/829125/201701/829125-20170104183245769-878631707.png)

##### 1.2 在分割任务上的不同
基于CNN的分割方法，为了对一个像素进行分类，需要使用该像素周围的像素区域作为CNN的输入。这样做有几个缺点：a.存储开销大，存储空间依赖卷积核的大小和卷积的次数。b.计算效率低，因为卷积会有很多重复计算。c.CNN提取局部特征，受限于卷积核的大小限制。

而在FCN中，是进行的像素级任务，可以对每个像素进行分类。（**我觉得主要解决的还是上面的缺点c**）

##### 2. 全连接层与卷积层
全连接层与卷积层是可以相互转换的。利用1*1卷积即可。

#### 3. 如何保持输出与输入尺寸相同？
保留每个池化层的feature map,将最后一个池化层之后的结果（heatMap）进行上采样（双线性插值），然后依次使用上层的卷积核对上采样结果进行反卷积，最终就得到了与输入尺寸相同的图像。

#### 4.缺点
缺点显而易见，进行的是像素级任务，那自然就抛弃了像素之间的关系——空间一致性。

上采样的结果比较模糊，不能注意到图像的细节。


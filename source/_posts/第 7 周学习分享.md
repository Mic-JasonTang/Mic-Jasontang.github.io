---
title: 学习基于深度学习的目标检测框架
date: 2019-5-8 17:15:00
categories:
- 周总结
tags: 
- 目标检测
- R-CNN系列
- YOLO
- SSD
- deep learning
---
总结R-CNN、SPP-NET、Fast R-CNN、Faster R-CNN、YOLO(v1 v2)、SSD [参考](https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fast-r-cnn)
<!-- more -->

### 提前的总结
#### 方案1.候选区域 + 深度学习回归（two stage）
[R-CNN](http://arxiv.org/abs/1311.2524)，2014（Selective Search + CNN + SVM），首先预选2000个框

[SPP-Net](http://arxiv.org/abs/1406.4729)，2015（ROI Pooling），一次卷积，不限制输入尺寸

[Fast R-CNN](http://arxiv.org/abs/1504.08083)，2015（Selective Search + CNN + ROI），结合R-CNN和SPP-Net的思路。将分类和回归都加入到网络中进行训练，做成了端到端。

[Faster R-CNN](http://arxiv.org/abs/1506.01497)，2015（RPN + CNN + ROI），用RPN来求候选框，同时引入anchor机制来应对目标形变问题。

#### 方案2. 深度学习回归（one stage）
[YOLO v1](http://arxiv.org/abs/1506.02640)，2016（回归任务）

[YOLO v2](https://arxiv.org/abs/1612.08242)，2017，引入Faster R-CNN的anchor机制

[SSD](http://arxiv.org/abs/1512.02325)，2016，结合YOLO的回归和Faster R-CNN的anchor

---

### 细节讨论

#### 1.R-CNN
1. 候选区域：利用颜色、纹理、边缘信息，使用selective search提取一定（2000）的框（在保持较高的recall情况下）。对于每个框的区域，需要修正区域大小，以适合于CNN的输入，并提取特征。
2. SVM分类：训练一个二分类SVM，来判断当前候选框里物体的类别。
3. 回归：使用回归来修正候选框的位置。
4. **问题1**：对2000个框都要进行CNN，框与框有重叠，时间复杂度高，在重叠区域的计算是不必要的，因此可以提速。
5. **问题2**：框的尺寸不一，需要resize，但是resize就会出现失真的问题。肯定会影响精度。

#### 2.SPP-Net
针对R-CNN的两个问题，SPP-Net就被提出来了。

1. ROI Pooling：在普通的CNN结构中加入了ROI Pooling，其中的pooling filter可根据输入调整大小，使得输出是一个固定维度的向量，输入可以是任意的尺寸。然后给到全连接FC层。
2. 对原图只做一次卷积，首先得到整张图的feature map，再找到每个候选框在feature map上所对应的区域，将这个区域的特征输入到ROI Pooling，完成特征提取的工作。

#### 3.Fast R-CNN
在R-CNN的基础上结合了SPP-Net的思路，相比原来R-CNN，使用一次卷积；在最后一次卷积后添加了ROI Pooling；将边框回归直接加入到了CNN网络中，因此做成了端到端网络。

R-CNN分为三个阶段，Fast R-CNN使用softmax来代替SVM，边框回归也加入到网络中，因此成为了端到端的网络。**这可以为后面改进阶段性的算法作为一个重要的提示**

R-CNN的方法：2000个候选框,resize -> CNN提取特征 -> 分类+回归

Fast R-CNN方法:原始图片 -> CNN -> ROI Pooling -> 分类+回归

相比R-CNN训练速度提升了8.8x， 测试速度提升了146x

**问题** Selective Search寻找候选框非常耗时，

#### 4.Faster R-CNN
引入RPN网络代替selective search，使用RPN来搜索候选框，同时引入anchor box应对目标形状的变化问题。

1. RPN：将RPN放在最后一个卷积层后面，所以RPN是作用在feature map上的，RPN包含分类和边框回归两个任务，分类器判断框属于的类别，边框回归进一步的优化/精细候选框。

---

#### 5.YOLO v1
*two stage*的方案不能满足实时性要求，所以one stage的出来了。

1. 首先将图像划分为7x7的网格
2. 对于每个网格，都预测2个边框（包括边框是目标的置信度和类别概率），最终可以得到7x7x2个边框，再经过NMS，即可得到最终的回归框。
3. **问题1**：只是用7x7的网格，使得回归的精度不够高。而且当每个格子中包含多个物体（>2）时，不能被检测出来。
4. **问题2**：输入图像输入resize到224*224

#### 6.YOLO v2
引入了Faster R-CNN的anchor box机制，并使用卷积层代替YOLO v1的全连接层，所以对输入图像不需要resize了。

1. 改进anchor box：Faster R-CNN中需要首选anchor box，而v2中采用k-means在许多框中进行聚类，以产生合适的框，同时使用过IOU作为k-means的距离计算。
2. 引入skip layer：借鉴ResNet的思想，使得网络更深。

#### 7.SSD
SSD结合了YOLO回归的思想和Faster R-CNN的anchor box机制，YOLO中预测某个位置使用的全图特征，SSD预测某个位置使用的这个位置周围的特征。

同时SSD的anchor是在多个feature map上进行的，这样就可以做的多尺度。
